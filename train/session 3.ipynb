{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap Framework - Computer Vision\n",
    "\n",
    "In this session, we will introduce you to preprocessing image data for computer vision. Preprocessing, storage, retrieval and batch management are all handled by two classes, the <b style='color:saddlebrown'>Image</b> and <b style='color:saddlebrown'>Images</b> class.\n",
    "\n",
    "    Image - represents a single preprocessed image\n",
    "    Images - represents a collection (or batch) of preprocessed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go the directory of the Gap Framework\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "!cd\n",
    "#ls #on linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's start by importing the Gap <b style='color:saddlebrown'>vision</b> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Gap Vision module\n",
    "from gapml.vision import Image, Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to a respository of images for sign language. We will use this repository for image preprocessing for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../Training/AITraining/Intermediate/Machine Learning/sign-lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sign language characters (a-z) are labeled 1 .. 26, and 0 is for not a character.\n",
    "# Each of the training images are under a subdirectory of the corresponding label.\n",
    "labels = os.listdir(\"gestures\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Class\n",
    "\n",
    "The <b style='color:saddlebrown'>Image</b> class supports the preprocessing of a single image into machine learning ready data. It can process JPG, PNG, TIF and GIF files. We will start by instantiating an <b style='color:saddlebrown'>Image</b> object for an image in the sign-lang image collection. For parameters, we will give the path to the image, and the label value (1). Labels must be mapped into integer values. For the sign-lang dataset, 1-26 is mapped to the 26 letters of the alphabet, and 0 is for a non-letter. \n",
    "\n",
    "When we instantiate the image, by default the following will happen:\n",
    "\n",
    "    1. The image is read in and decompressed, as a numpy array.\n",
    "    2. The image is processed according to the configuration parameters or defaults (e.g., resize, normalized, flatten,   \n",
    "       channel conversion).\n",
    "    3. The raw image data, processed image data, thumbnail, and metadata are stored to a HDF5 file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image('gestures/1/1.jpg', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Properties\n",
    "\n",
    "Let's look at some properties of the <b style='color:saddlebrown'>Image</b> class.\n",
    "\n",
    "Note how the shape of the ML ready data is (50, 50, 3). We will change that in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( image.name )   # The root name of the image (w/o suffix)\n",
    "print( image.type )   # Type of image (e.g. jpeg)\n",
    "print( image.dir )    # The directory where the ML ready data will be stored\n",
    "print( image.size )   # The original size of the image\n",
    "print( image.shape )  # The shape of the preprocessed image (ML ready data)\n",
    "print( image.label )  # The label\n",
    "print( image.time )   # The amount of time (secs) to preprocess the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data\n",
    "\n",
    "Let's look at both the raw and ML ready data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw Data\", image.raw)\n",
    "print(\"ML ready data\", image.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the number of channels is preserved (e.g., 1 for grayscale, 3 for RGB, and 4 for RGBA), and the data is normalized. On the later, the 0..255 pixel values are rescaled between 0 and 1.\n",
    "\n",
    "Let's now change the preprocessing of the image to a grayscale image and resize it to 32x32. When we print the shape, you can see the 3rd dimension (channels) is gone - indicating a grayscale image, and the size is now 32 by 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image('gestures/1/1.jpg', 1, config=['grayscale', 'resize=(32,32)'])\n",
    "\n",
    "print( image.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now say that the image data will be feed into a ANN (not CNN) or to a CNN with a 1D input vector. In this case, we need to feed the ML ready data as a flatten 1D vector. We can do that to. Now when we print the shape you can see its 1024 (32 x 32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image('gestures/1/1.jpg', 1, config=['grayscale', 'resize=(32,32)', 'flatten', 'raw'])\n",
    "\n",
    "print( image.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Loading\n",
    "\n",
    "When an image is preprocessed, the ML ready data, raw data and attributes are stored in an HDF5 file. We can subsequently recall (load) the image information from the HDF5 file into an <b style='color:saddlebrown'>Image</b> object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = Image()   # Create an empty image object\n",
    "image.load('1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get the same properties again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( image.name )   # The root name of the image (w/o suffix)\n",
    "print( image.type )   # Type of image (e.g. jpeg)\n",
    "print( image.dir )    # The directory where the ML ready data will be stored\n",
    "print( image.size )   # The original size of the image\n",
    "print( image.shape )  # The shape of the preprocessed image (ML ready data)\n",
    "print( image.label )  # The label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we get the raw and ML ready data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw Data\", image.raw)\n",
    "print(\"ML ready data\", image.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thumbnails\n",
    "\n",
    "We can also generate a store a thumbnail of the original image with the *config* parameter thumb. In the example below, we create a thumbnail with size 16x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image('gestures/1/1.jpg', 1, config=['grayscale', 'thumb=(16,16)'])\n",
    "print(image.thumb)\n",
    "print(\"Thumb Shape\", image.thumb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aysnchronous Preprocess\n",
    "\n",
    "The image data can also be preprocessed asynchronously. In this mode, the parameter *ehandler* is set to an event handler (function) that will be called when the image is done being preprocessed. The image object is passed as a parameter to the event handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(image):\n",
    "    print(\"DONE\", image.name)\n",
    "\n",
    "image = Image('gestures/1/1.jpg', 1, ehandler=func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cleanup and remove the HDF5 file\n",
    "os.remove(\"1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Image (Url)\n",
    "\n",
    "The <b style='color:saddlebrown'>Image</b> class (and correspondly the <b style='color:saddlebrown'>Images</b> class), paths to the image file may be specified as an URL; providing the ability to preprocess images stored at remote locations. In this case, an HTTP request is made to retrieve the image data over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load an image from the CNN news website\n",
    "image = Image('https://cdn.cnn.com/cnnnext/dam/assets/180727161452-trump-speech-economy-072718-exlarge-tease.jpg', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display some properties of the image that was fetched from a remote location and then preprocessed in ML ready data.\n",
    "print(image.name)\n",
    "print(image.size)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Image (Pixel)\n",
    "\n",
    "The <b style='color:saddlebrown'>Image</b> class (and correspondly the <b style='color:saddlebrown'>Images</b> class), paths to the image file may alternatively be the raw pixel input; providing the ability to preprocess images without retreiving from storage, when they are otherwise already in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the openCV module\n",
    "import cv2\n",
    "\n",
    "# Read the pixel data into memory for an image using openCV\n",
    "raw = cv2.imread('gestures/1/1.jpg')\n",
    "\n",
    "# Let's load the image from directly the raw pixel data in memory\n",
    "image = Image(raw, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some properties. \n",
    "\n",
    "Note, since this is raw pixel data, the image has no name, and the size is the decompressed (raw) size in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display some properties of the image that was directly loaded from raw pixel data and then preprocessed in ML ready data.\n",
    "print(image.name)\n",
    "print(image.size)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "\n",
    "Image Augmentation is the process of generating (synthesizing) new images from existing images, which can then be used to augment the training process. Augmentation can include, rotation, skew, sharpending and blur of existing images. These new images are then feed into the neural network during training to augment the training set. Rotating and skew aid in recognizing images from different angles, and sharpening and blur help generalize recognition (combat overfitting), as well as recognition under different lightening and time of day conditions.\n",
    "\n",
    "The <b style='color:saddlebrown'>Image</b> class supports generating new images by rotation. Any degree of rotation can be specified from 0 to 360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now rotate it 90 degress\n",
    "rotated = image.rotate(90)\n",
    "\n",
    "# Let's now look at the rotated image\n",
    "cv2.imshow('image',rotated)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images Class\n",
    "\n",
    "The <b style='color:saddlebrown'>Images</b> class supports the preprocessing of a collection of images into machine learning ready data. For required parameters, the <b style='color:saddlebrown'>Images</b> class takes a list of images and either a list of corresponding labels, or a single value, where all the images share the same label.\n",
    "\n",
    "Let's start by creating an <b style='color:saddlebrown'>Images</b> object for all the images under the subfolder 1 (letter A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a list of all the images in the subfolder for the label 1 (letter A)\n",
    "imgdir = \"gestures/1/\"\n",
    "imglst = [imgdir + x for x in os.listdir(imgdir)]\n",
    "\n",
    "# There should be 1200 images\n",
    "len(imglst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create an <b style='color:saddlebrown'>Images</b> object and preprocess all the above images.\n",
    "    1. Process all 1200 images in the subfolder 1\n",
    "    2. Set the label to 1\n",
    "    3. Convert them to grayscale.\n",
    "    \n",
    "By default, the image data will be stored in an HDF5 file with the name 'collection.1.h5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the set of images\n",
    "images = Images(imglst, 1, config=['grayscale'])\n",
    "\n",
    "# Check that the image data is stored in HDF5 file.\n",
    "os.path.exists(\"collection.1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images Properties\n",
    "\n",
    "Next, we will show some properties of the <b style='color:saddlebrown'>Images</b> class.\n",
    "\n",
    "Note, how fast it was to preprocess the set of 1200 images into machine ready data and store them in an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( images.name )   # The name of the collection of images\n",
    "print( image.dir )     # where the ML ready data is stored\n",
    "print( images.time )   # The length of time to preprocess the collection of images\n",
    "print( len(images) )     # The len() operator is overridden to return the number of images in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the vector of labels\n",
    "print(\"LABELS\", images.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b style='color:saddlebrown'>Image</b> objects for each corresponding image can be accessed using the [] index operator. Let's get the 33rd one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third Image object\n",
    "image = images[32]\n",
    "print(type(image))\n",
    "print(\"Name\", image.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories (Subfolders) of Images\n",
    "\n",
    "The <b style='color:saddlebrown'>Images</b> class can alternately take a list of subfolders (vs. list of images); in which case, all the images under each subfolder are preprocessed into ML ready data. This is useful if your images are separated into subfolders, where each subfolder is a separate class (label) of images. This is a fairly common practice.\n",
    "\n",
    "In this case, the corresponding label in the same index of the labels parameter will be assigned to each image in the subfolder.\n",
    "\n",
    "In the example below, we also use the *name* parameter to specify a name (vs. default) for the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process a list of subfolders of images, and name the collection 'foobar'\n",
    "images = Images(['gestures/1', 'gestures/2'], [1,2], name='foobar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two subfolders of 1200 images each, so we should expect 2400 images\n",
    "print(len(images))\n",
    "\n",
    "# cleanup\n",
    "os.remove('foobar.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aysnchronous Preprocess\n",
    "\n",
    "A collection of images can also be preprocessed asynchronously. In this mode, the parameter *ehandler* is set to an event handler (function) that will be called when the collection of images is done being preprocessed. The <b style='color:saddlebrown'>Images</b> object is passed as a parameter to the event handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(images):\n",
    "    print(\"DONE\", images.name, \"TIME\", images.time)\n",
    "    \n",
    "images = Images(imglst, 1, config=['grayscale'], ehandler=func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemblying a Collection\n",
    "\n",
    "For performance purposes, one may decide to process subparts of a collection asynchronosly, and then assemble them together into a single collection. The Images class provides support for this assembling subcollections into a single collection using the overridden *+=* operator, to merge other preprocessed collections into a single collection.\n",
    "\n",
    "In the example below, we first create a collection for the letter 'A' images (label 1) and then separately create a second collection for the letter 'B' images (label 2), and use the *+=* operator to merge the second collection into the first collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for the letter 'A' images\n",
    "images = Images(['gestures/1'], 1)\n",
    "\n",
    "# Create collection for the letter 'B' images and merge it with the letter 'A' image collection\n",
    "images += Images(['gestures/2'], 2)\n",
    "\n",
    "# The merged collection should have 2400 images (1200 for 'A', and 1200 for 'B')\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a Collection into Training and Test Data\n",
    "\n",
    "The *split* property will split the <b style='color:saddlebrown'>Image</b> objects into training and test data. The list of training image objects is then randomized. When used as a setter, the property takes either 1 or 2 arguments. The first argument is the percentage that is test data, and the optional second argument is the seed for the random shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the image objects into 80% training and 20% test\n",
    "images.split = 0.20\n",
    "\n",
    "# Let's verify that the training set is 80% (960 of 1200) by printing the internal variable _train\n",
    "print(len(images._train))\n",
    "\n",
    "# Let's now print the randomized list of image object indices\n",
    "print(\"TRAIN INDICES\", images._train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add the optional parameter for a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the image objects into 80% training and 20% test\n",
    "images.split = 0.20, 42\n",
    "\n",
    "# Let's now print the randomized list of image object indices\n",
    "print(\"TRAIN INDICES\", images._train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Feeding (Batch Gradient Descent)\n",
    "\n",
    "There are three ways to use the <b style='color:saddlebrown'>Images</b> object to feed a neural network. In batch mode, the entire training set can be ran through the neural network as a single pass, prior to backward probagation and updating the weights using gradient descent. This is known as 'batch gradient descent'.\n",
    "\n",
    "When the *split* property is used as a getter, it returns the image data and corresponding labels for the training and test set similar to using sci-learn's train_test_split() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the percentage and seed, and split the data\n",
    "images.split = 0.20, 42\n",
    "\n",
    "# Get the training, test sets and corresponding labels\n",
    "x_train, x_test, y_train, y_test = images.split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify and print the len of the train, test and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train\", len(x_train))\n",
    "print(\"y_train\", len(y_train))\n",
    "print(\"x_test\", len(x_test))\n",
    "print(\"y_test\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the contents that the elements are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in x_train list should be a numpy array\n",
    "print(type(x_train[0]))\n",
    "# Each element should be in the shape 50 x 50 pixels\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each elment in y_train should be the label (integer)\n",
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Iterating (Stochastic Gradient Descent)\n",
    "\n",
    "Another way of feeding a neural network is to feed one image at a time and do backward probagation, using gradient descent. This is known as stochastic gradient descent.\n",
    "\n",
    "The *next()* operator supports iterating through the training list one image object at a time. Once all of the entire training set has been iterated through, it is reset and the training set is randomly re-shuffled for the next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's iterate through the ML ready data and label for each image in the training set\n",
    "while True:\n",
    "    data, label = next(images)\n",
    "    if data is None: break\n",
    "    print(type(data), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch generation\n",
    "\n",
    "Another way of feeding a neural network is through mini-batches. A mini-batch is a subset of the training set, that is greater than one. After each mini-batch is feed, then backward probagation, using gradient descent, is done. \n",
    "\n",
    "Typically, minibatches are set to sizes like 30, 50, 100, or 200. We will use the *minibatch* property as a setter to set the mini-batch size to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minibatch size to 100 images\n",
    "images.minibatch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the *minibatch* property as a getter, it will create a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of batches\n",
    "nbatches = len(images) // 100\n",
    "\n",
    "# process each mini-batch\n",
    "for _ in range(nbatches):\n",
    "    # Create a generator for the next minibatch\n",
    "    g = images.minibatch\n",
    "    # Get the data, labels for each item in the minibatch\n",
    "    for data, label in g:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset of Images\n",
    "\n",
    "Let's load the entire dataset - that's 27 collections of 1200 images each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare each set of labeled data into machine learning ready data\n",
    "# The images are 50x50, bitdepth=8, 1 channel (grayscale)\n",
    "total = 0\n",
    "collections=[]\n",
    "for label in labels:\n",
    "    # Get a list of all images in the subdirectory for this label (should be 1200 images)\n",
    "    imgdir = \"gestures/\" + label + \"/\"\n",
    "    imglst = [imgdir + x for x in os.listdir(imgdir)]\n",
    "    images = Images(imglst, int(label), name='tmp' + label, config=['flatten', 'grayscale', 'raw'])\n",
    "    collections.append(images)\n",
    "    print(\"Procesed: \" + label, \"Number of images:\", len(images), \"Time: \", images.time)\n",
    "    total += images.time\n",
    "    \n",
    "print(\"average:\", total / len(labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the preprocessing of our image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many batches (collections) we have (hint: should be 27)\n",
    "print(len(collections))\n",
    "\n",
    "# Let's verify that the items in the collections are an Images object\n",
    "collection = collections[3]\n",
    "print(type(collection))\n",
    "\n",
    "# For a collection, let's see how many image objects we have (hint: should be 1200)\n",
    "print(len(collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first Image object in this collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the first Image item and verify it is an Image object\n",
    "image = collection[0]\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's name view some of the properties and verify that images got processed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some basic information about the image\n",
    "print(image.name)  # the root name of the image\n",
    "print(image.type)  # the image file suffix\n",
    "print(image.size)  # the size of the image on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now check the raw (uncompressed) unprocessed image\n",
    "print(image.raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at how the image got processed.\n",
    "print(image.shape)  # Note, that the preprocessed image was flattened into a 1D vector. It was 50x50, and now is 2500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the image. Remember to hit any key to exit the viewer (i.e., cv2.waitKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the raw image\n",
    "import cv2\n",
    "cv2.imshow('image',image.raw)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleanup\n",
    "os.remove('collection.1.h5')\n",
    "for _ in range(27):\n",
    "    os.remove('tmp' + str(_) + '.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
