{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap Framework - Computer Vision\n",
    "\n",
    "In this session, we will introduce you to preprocessing image data for computer vision. Preprocessing, storage, retrieval and batch management are all handled by two classes, the Image and Images class.\n",
    "\n",
    "    Image - represents a single preprocessed image\n",
    "    Images - represents a collection (or batch) of preprocessed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the Gap <b style='color:saddlebrown'>vision</b> module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Gap Vision module\n",
    "from scripts.vision import Image, Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\'\\Desktop\\Gap-ml\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to a respository of images for sign language. We will use this repository for image preprocessing for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../Training/AITraining/Intermediate/Machine Learning/sign-lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sign language characters (a-z) are labeled 1 .. 26, and 0 is for not a character.\n",
    "# Each of the training images are under a subdirectory of the corresponding label.\n",
    "labels = os.listdir(\"gestures\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesed: 0 Number of images: 1200 Time:  2.1700029373168945\n",
      "Procesed: 1 Number of images: 1200 Time:  2.1200029850006104\n",
      "Procesed: 10 Number of images: 1200 Time:  2.342005729675293\n",
      "Procesed: 11 Number of images: 1200 Time:  2.130002975463867\n",
      "Procesed: 12 Number of images: 1200 Time:  2.130002975463867\n",
      "Procesed: 13 Number of images: 1200 Time:  1.9700028896331787\n",
      "Procesed: 14 Number of images: 1200 Time:  1.9920039176940918\n",
      "Procesed: 15 Number of images: 1200 Time:  2.020002841949463\n",
      "Procesed: 16 Number of images: 1200 Time:  1.9700026512145996\n",
      "Procesed: 17 Number of images: 1200 Time:  1.9800026416778564\n",
      "Procesed: 18 Number of images: 1200 Time:  2.0120041370391846\n",
      "Procesed: 19 Number of images: 1200 Time:  2.650003671646118\n",
      "Procesed: 2 Number of images: 1200 Time:  5.540007829666138\n",
      "Procesed: 20 Number of images: 1200 Time:  4.58000636100769\n",
      "Procesed: 21 Number of images: 1200 Time:  4.480006456375122\n",
      "Procesed: 22 Number of images: 1200 Time:  5.210007190704346\n",
      "Procesed: 23 Number of images: 1200 Time:  5.490007638931274\n",
      "Procesed: 24 Number of images: 1200 Time:  4.878014326095581\n",
      "Procesed: 25 Number of images: 1200 Time:  4.850006580352783\n",
      "Procesed: 26 Number of images: 1200 Time:  5.840011358261108\n",
      "Procesed: 3 Number of images: 1200 Time:  5.5500078201293945\n",
      "Procesed: 4 Number of images: 1200 Time:  4.750006675720215\n",
      "Procesed: 5 Number of images: 1200 Time:  4.450006008148193\n",
      "Procesed: 6 Number of images: 1200 Time:  5.240007162094116\n",
      "Procesed: 7 Number of images: 1200 Time:  4.4500062465667725\n",
      "Procesed: 8 Number of images: 1200 Time:  4.724011659622192\n",
      "Procesed: 9 Number of images: 1200 Time:  5.110007286071777\n",
      "average: 3.7269689242045083\n"
     ]
    }
   ],
   "source": [
    "# Prepare each set of labeled data into machine learning ready data\n",
    "# The images are 50x50, bitdepth=8, 1 channel (grayscale)\n",
    "total = 0\n",
    "collections=[]\n",
    "for label in labels:\n",
    "    # Get a list of all images in the subdirectory for this label (should be 1200 images)\n",
    "    imgdir = \"gestures/\" + label + \"/\"\n",
    "    imglst = [imgdir + x for x in os.listdir(imgdir)]\n",
    "    images = Images(imglst, int(label), dir='tmp' + label, config=['flatten', 'grayscale'])\n",
    "    collections.append(images)\n",
    "    print(\"Procesed: \" + label, \"Number of images:\", len(images), \"Time: \", images.time)\n",
    "    total += images.time\n",
    "    \n",
    "print(\"average:\", total / len(labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the preprocessing of our image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "<class 'image.Images'>\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many batches (collections) we have (hint: should be 27)\n",
    "print(len(collections))\n",
    "\n",
    "# Let's verify that the items in the collections are an Images object\n",
    "collection = collections[3]\n",
    "print(type(collection))\n",
    "\n",
    "# For a collection, let's see how many image objects we have (hint: should be 1200)\n",
    "print(len(collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first Image object in this collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'image.Image'>\n"
     ]
    }
   ],
   "source": [
    "# Let's get the first Image item and verify it is an Image object\n",
    "image = collection[0]\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "jpg\n",
      "1459\n"
     ]
    }
   ],
   "source": [
    "# Let's get some basic information about the image\n",
    "print(image.name)  # the root name of the image\n",
    "print(image.type)  # the image file suffix\n",
    "print(image.size)  # the size of the image on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "# Let's now check the raw (uncompressed) unprocessed image\n",
    "print(image.raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "# Let's look at how the image got processed.\n",
    "print(image.shape)  # Note, that the preprocessed image was flattened into a 1D vector. It was 50x50, and now is 2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view the raw image\n",
    "import cv2\n",
    "cv2.imshow('image',image.raw)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's presume that it is some later date, and we want to recall the collections from disk.\n",
    "\n",
    "We start by creating an empty Images() object. We then identify the name of the collection and execute the load() method with the collection name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load up one collection\n",
    "images = Images(dir=\"tmp0/\")\n",
    "images.load(\"batch.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we just recalled the whole collection from disk. Let's now look at our machine learning ready data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1013\n",
      "jpg\n",
      "369\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "image = images[17]\n",
    "print(image.name)\n",
    "print(image.type)\n",
    "print(image.size)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "print(image.raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
