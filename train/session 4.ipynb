{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap Framework - Data Preparation for Sentiment Analysis\n",
    "\n",
    "## PDSG: Boardgame rating and comment\n",
    "\n",
    "The Boardgame rating/comment dataset is in a CSV format. To start you will need to read in the raw data from the CSV file using a csv reader. Fortunately, Python has a CSV reader. Let's start by importing the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python's CSV parser module\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python csv reader creates a generator. For convenience, since you may want to re-read the file more than once as you practice preparing the data, let's make a function to create the generator.\n",
    "\n",
    "Note: The boardgame rating/comment dataset has unicode characters in it. So we need to open the file with the encoding 'utf-8'. If we don't the reader will throw an exception when it encounters a unicode character sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator for reading each row as a list in the CSV file.\n",
    "def load():\n",
    "    # make sure to open with Unicode encoding (there are unicode chars in the dataset)\n",
    "    f = open('boardgames.csv', encoding='utf-8')\n",
    "    r = csv.reader(f, delimiter=',')\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only the data I am interested in.\n",
    "\n",
    "For my purposes, I am only interested in the rating and the comment. The entry and game ID will not contribute to my model, so I want to toss them out. I will simply create a new dataset where I will copy only the rating and comment over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataset of just the rating and comment\n",
    "dataset = []\n",
    "\n",
    "# Let's read each row to assemble a new dataset\n",
    "header = True\n",
    "for row in load():\n",
    "    # First row is a header, so let's skip it.\n",
    "    if header:\n",
    "        header = False\n",
    "        continue\n",
    "        \n",
    "    # some of the ratings are floating point values, like 8.5 or 6.2. We will make them all ints for convinence. \n",
    "    rating = int(float(row[2]))\n",
    "\n",
    "    dataset.append( { 'rating': rating, 'comment': row[3] } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we really create the new dataset as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 847\n",
      "Row 0 {'rating': 8, 'comment': \"++++ Thematic +++ Bluff - Many randomness   I really like that one. Maybe it's more fun to play as a cylon than as a human, but when you do, you really feel like an undercover agent who awaits the best moment to strike and hurt bad...  It is a long poker time when everyone tries to guess who is who.  Having seen the series is really a must, to enjoy this game, otherwise the thematic won't be appreciated.\"}\n",
      "Row 2 {'rating': 8, 'comment': 'LOVE this game!  If only the GF would play it with me.  Tired at end of day with basic math = bad idea.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows\", len(dataset))\n",
    "print(\"Row 0\", dataset[0])\n",
    "print(\"Row 2\", dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first row shows some of the issues you will need to consider:\n",
    "\n",
    "    - Punctuation sequences like +++ and *** may indicate the user is emphasizing the word.\n",
    "    - All caps (e.g., BAD) may indicate the user is emphasizing the word.\n",
    "    - ! may mean the user is elevating their statement.\n",
    "    \n",
    "So why is this an issue? The general rule of thumb in NLP preprocessing is to remove punctuation and lowercase words. Hum, that might cause us to lose valuable information. We will explore this, along with other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing with Gap\n",
    "\n",
    "Okay, there are a zillion ways one could code preprocessing text for sentiment analysis. There are no shortage of blogs.\n",
    "\n",
    "I will show you how to use **Gap** as an alternative to writing each line of preprocessing code by hand, and do it in a few simple steps. \n",
    "\n",
    "We start by first importing the <b style='color: saddlebrown'>SYNTAX</b> module from **Gap**. This module does syntactically analysis for Text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\'\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\'\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import Words from Gap's Syntax module\n",
    "from gapml.syntax import Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the <b style='color:saddlebrown'>Words</b> class to demonstrate methods to preprocess (i.e., prepare) the text for machine learning. Like many applications of NLP, sentiment analysis has its own unique challenges:\n",
    "\n",
    "    - Reviews contain slang\n",
    "    - Repetitive punctuation and all CAPS are used as emphasis.\n",
    "    - Short-hand, incomplete sentences, higher frequency of spelling errors (not a concern to someone entering a review).\n",
    "    \n",
    "I will start with a simple example of a single 'positive' phrase; but in the phrase, the reviewer has used punctuation and all caps to emphasis words.\n",
    "\n",
    "First, we preprocess the traditional way: remove punctuation, lowercase, stem and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our helper function to make it easier to display preprocessed text w/o tagging\n",
    "def towords(words):\n",
    "    for word in words:\n",
    "        print(word['word'], ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Method\n",
    "\n",
    "Let's process and print the NLP tokenized string according to the 'standard rule of thumb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game  \n",
      "time  \n"
     ]
    }
   ],
   "source": [
    "w = Words('+++ My favourite game of all TIME!!')\n",
    "towords(w.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMG. What's wrong with this. Well, we lost stuff that might be important, like:\n",
    "\n",
    "    - TIME was in all CAPS\n",
    "    - +++ punctuation was used as emphasis\n",
    "    - favorite was spelled according to UK spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bare Mode\n",
    "\n",
    "Let's do the opposite and just keep everything. In **Gap**, that's the parameter *bare=True*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+  \n",
      "+  \n",
      "+  \n",
      "My  \n",
      "favourite  \n",
      "game  \n",
      "of  \n",
      "all  \n",
      "TIME  \n",
      "!  \n",
      "!  \n"
     ]
    }
   ],
   "source": [
    "w = Words('+++ My favourite game of all TIME!!', bare=True)\n",
    "towords(w.words)\n",
    "\n",
    "# Note that tag 14 is an acronymn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that might be better. But favourite won't match other occurrences of the American version favorite, and TIME won't match time and Time, etc. Perhaps your neural network will learn the relationship. But why burden the neural network? It has plenty of other important things to learn, like predicting the rating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping Punctuation\n",
    "\n",
    "Okay, let's go back to step 0. We start by doing the standard rule of thumb, but this time we keep punctuation by setting the keyword parameter *punct* to True, since it might indicate an emphasis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+  \n",
      "+  \n",
      "+  \n",
      "game  \n",
      "time  \n",
      "!  \n",
      "!  \n"
     ]
    }
   ],
   "source": [
    "w = Words('+++ My favourite game of all TIME!!', punct=True)\n",
    "towords(w.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's better. But we are still missing important things. Note how time is lowercased. But if we look at it's tag, it will tell us that it was uppercased (value 14). Also the punctuation all have tags as punctuation (value 23) or symbol (value 24). \n",
    "\n",
    "Interesting, we don't need to write code to tell if something was uppercased or if we have sequence of punctuation. We can just look at the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 24, 'word': '+'}, {'tag': 24, 'word': '+'}, {'tag': 24, 'word': '+'}, {'tag': 0, 'word': 'game'}, {'tag': 14, 'word': 'time'}, {'tag': 23, 'word': '!'}, {'tag': 23, 'word': '!'}]\n"
     ]
    }
   ],
   "source": [
    "print(w.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Words\n",
    "\n",
    "We are still missing something. It's the word 'favourite' that indicates the sentiment! And it's in UK spelling. **Gap** has a word dictionary of US and UK spellings, slang and misspellings of words that (or may) indicate a sentiment. Let's tell **Gap** to keep these words by setting the keyword parameter *sentiment* to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 24, 'word': '+'}, {'tag': 24, 'word': '+'}, {'tag': 24, 'word': '+'}, {'tag': 18, 'word': 'favorite'}, {'tag': 0, 'word': 'game'}, {'tag': 14, 'word': 'time'}, {'tag': 23, 'word': '!'}, {'tag': 23, 'word': '!'}]\n"
     ]
    }
   ],
   "source": [
    "w = Words('+++ My favourite game of all TIME!!', punct=True, sentiment=True)\n",
    "print(w.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a close look at the NLP sequence. Aah, eventhough time is lowercase, the tag (14) indicates it was all CAPS. We kept the punctuation. And look, we retain the word favorite, and it's in US spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negation\n",
    "\n",
    "Okay, sometimes people use a negation. That is, they use a positive word like 'great' or 'good', but proceed it was a negation, like 'not' or 'never'. **Gap** recognizes this as well. Let's try one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 19, 'word': 'not'}, {'tag': 0, 'word': 'game'}, {'tag': 23, 'word': '.'}]\n"
     ]
    }
   ],
   "source": [
    "w = Words(\"Did not like the game.\", sentiment=True, punct=True)\n",
    "print(w.words)\n",
    "\n",
    "# Note the positive word Like is negated by not, so it was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh, **Gap** dropped the otherwise positive word 'like' and simply kept the negative word 'not'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contractions\n",
    "\n",
    "**Gap** handles contractions as well. For example, the not might be part of a contraction like: don't, can't, won't, isn't, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 19, 'word': 'not'}, {'tag': 0, 'word': 'game'}, {'tag': 23, 'word': '.'}]\n"
     ]
    }
   ],
   "source": [
    "w = Words(\"Didn't like the game.\", sentiment=True, punct=True)\n",
    "print(w.words)\n",
    "\n",
    "# Handles contractions too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So many ways to write a word\n",
    "\n",
    "**Gap** has builtin and 3rd party stemmers and lemmatizers to match the same word with different word endings. For example, the words 'best' and 'better' are recognized as the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 18, 'word': 'great'}, {'tag': 0, 'word': 'game'}]\n",
      "[{'tag': 18, 'word': 'great'}, {'tag': 0, 'word': 'game'}]\n"
     ]
    }
   ],
   "source": [
    "w = Words('it is a great game', sentiment=True, punct=True)\n",
    "print(w.words)\n",
    "w = Words('it is the greatest game', sentiment=True, punct=True)\n",
    "print(w.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling Errors\n",
    "\n",
    "Reviews are notorious for spelling errors! **Gap** an implementation of the (Peter) Norvig speller builtin for English (and French, Spanish, German and Italian) language spell checker. Using the spell parameter, **Gap** will lookup each word in the pyaspeller dictionary. If the word is not found and the *Norvig* speller finds a replacement, the misspelled word is replaced.\n",
    "\n",
    "Note in the example, the misspelled word 'grat' is replaced with 'great'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 0, 'word': 'game'}, {'tag': 0, 'word': 'graat'}, {'tag': 23, 'word': '!'}]\n",
      "[{'tag': 0, 'word': 'game'}, {'tag': 18, 'word': 'great'}, {'tag': 23, 'word': '!'}]\n"
     ]
    }
   ],
   "source": [
    "w = Words('The game was graat!', sentiment=True, punct=True)\n",
    "print(w.words)\n",
    "w = Words('The game was graat!', sentiment=True, punct=True, spell='en')\n",
    "print(w.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Neural Network\n",
    "\n",
    "First, **Gap** wasn't focused for sentiment analysis, but for extracting and processing text from PDF documents. So we need to do some last steps by hand that in the future will be incorporated into **Gap**, these include:\n",
    "\n",
    "\n",
    "1. Add special markers.\n",
    "2. Reduce token sequence to fixed length.\n",
    "3. Convert words to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Markers\n",
    "\n",
    "Let's start defining special markers, some of which I will explain their purpose further into the code-along.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad      = '<PAD>'\n",
    "emphasis = '<EMP>'\n",
    "positive = '<POS>'\n",
    "negative = '<NEG>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know create a function that will insert some of our special markers, as follows:\n",
    "    1. If a word has a negative sentiment tag, insert <NEG>\n",
    "    2. If a word has a positive sentiment tag, insert <POS>\n",
    "    3. If a word is all caps, insert a <EMP>\n",
    "    4. If + or * occurs two or more times in sequence, replace with a <EMP>\n",
    "    5. If a ! mark appears, replace with a <EMP>\n",
    "    6. Otherwise, remove all remaining punctuation and symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gapml.syntax import Vocabulary\n",
    "\n",
    "def prepare(comment):\n",
    "    \"\"\" Convert text to NLP sequence \"\"\"\n",
    "    ret = []\n",
    "    \n",
    "    # Create words object from text: keep punctuation and sentiment words\n",
    "    try:\n",
    "        words = Words(comment, punct=True, sentiment=True)\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    # Reconstruct word list, adding special markers\n",
    "    last = None\n",
    "    for word in words.words:\n",
    "        # Add special <EMP> marker for all caps words\n",
    "        if word['tag'] == Vocabulary.ACRONYM:\n",
    "            ret.append(emphasis)\n",
    "        # Add special <POS> marker for positive words\n",
    "        if word['tag'] == Vocabulary.POSITIVE:\n",
    "            ret.append(positive)\n",
    "        # Add special <NEG> marker for negative words\n",
    "        elif word['tag'] == Vocabulary.NEGATIVE:\n",
    "            ret.append(negative)\n",
    "        # Drop punctuation, unless its an exclamation mark\n",
    "        elif word['tag'] == Vocabulary.PUNCT:\n",
    "            # Add <EMP> if exclamation mark\n",
    "            if word['word'] == '!':\n",
    "                ret.append(emphasis)\n",
    "            continue\n",
    "        elif word['tag'] == Vocabulary.SYMBOL:\n",
    "            # Drop symbols, unless + or * as multiple sequence\n",
    "            if word['word'] in ['+', '*']:\n",
    "                # Add <EMP> if multiple sequence\n",
    "                if last == word['word']:\n",
    "                    ret.append(emphasis)\n",
    "            continue\n",
    "        ret.append(word['word'])\n",
    "        # remember the last word\n",
    "        last = word['word']\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now update each comment in the dataset with our special markers using the prepare function.\n",
    "\n",
    "Note, since the contents of lists are mutable in Python, we do not need to create a new list. We can replace the previous element in the list with a new element in place (no copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate each row with special markers\n",
    "for row in dataset:\n",
    "    comment = prepare(row['comment'])\n",
    "    row['comment'] = comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': ['well',\n",
       "  '<NEG>',\n",
       "  'ugly',\n",
       "  'artwork',\n",
       "  'help',\n",
       "  'immerse',\n",
       "  'egyptian',\n",
       "  'theme',\n",
       "  '<POS>',\n",
       "  'cool',\n",
       "  'auction',\n",
       "  '<EMP>',\n",
       "  'ra',\n",
       "  '<EMP>',\n",
       "  '<EMP>',\n",
       "  '<EMP>'],\n",
       " 'rating': 8}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncating Size\n",
    "\n",
    "So we plan to send our sequences into a Recurrent Neural Network (RNN), like a LSTM. Best practice is that each sequence of tokens we input is the same length. So we need to pick a length, and anything above that length we truncate and below that length we add the special marker <PAD> - alas, the purpose of the <PAD> special marker.\n",
    "    \n",
    "Let's set the sequence length to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 15\n",
    "\n",
    "# Resize each comment to be exactly 15 tokens\n",
    "for row in dataset:\n",
    "    try:\n",
    "        rlen = len(row['comment'])\n",
    "    except:\n",
    "        continue\n",
    "    # If comment is > 15 tokens, then  truncate\n",
    "    if rlen > 15:\n",
    "        row['comment'] = row['comment'][0:15]\n",
    "    # If less than 15 tokens, then add paddimg\n",
    "    elif rlen < 15:\n",
    "        for _ in range(rlen, 15+1):\n",
    "            row['comment'].append('<PAD>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at a couple of rows and see what changed.\n",
    "    - Row 1: Was 16 elements, we dropped the last element <EMP>.\n",
    "    - Row 2: Was 13 elements, we added two <PAD> elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': ['well',\n",
       "  '<NEG>',\n",
       "  'ugly',\n",
       "  'artwork',\n",
       "  'help',\n",
       "  'immerse',\n",
       "  'egyptian',\n",
       "  'theme',\n",
       "  '<POS>',\n",
       "  'cool',\n",
       "  'auction',\n",
       "  '<EMP>',\n",
       "  'ra',\n",
       "  '<EMP>',\n",
       "  '<EMP>'],\n",
       " 'rating': 8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': ['<EMP>',\n",
       "  'love',\n",
       "  'game',\n",
       "  '<EMP>',\n",
       "  '<EMP>',\n",
       "  'gf',\n",
       "  'play',\n",
       "  'tir',\n",
       "  'basic',\n",
       "  'math',\n",
       "  '<NEG>',\n",
       "  'bad',\n",
       "  'idea',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " 'rating': 8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Words to Integers\n",
    "\n",
    "Well, inputs to neural networks are numbers not words! Yeaks, what next.\n",
    "\n",
    "Well, we need to make a dictionary that maps each unique word to a unique integer and the replace the words in our inputs with the corresponding integer value.\n",
    "\n",
    "In Python, we do that with a dictionary object. Let's start by initializing our dictionary with the special markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {}\n",
    "word2int['<PAD>'] = 0\n",
    "word2int['<EMP>'] = 1\n",
    "word2int['<POS>'] = 2\n",
    "word2int['<NEG>'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a dictionary of all the words in our dataset, mapping each word to a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4 # next integer value of word to add to dictionary\n",
    "\n",
    "# Walk thru each row in the dataset\n",
    "for row in dataset:\n",
    "    # Get the list of preprocessed words for this row's comment\n",
    "    words = row['comment']\n",
    "    # For each word, we will see if we need to add it to the dictionary\n",
    "    for word in words:\n",
    "        # looks like this word is not in the dictionary!\n",
    "        if word not in word2int:\n",
    "            # Add the word and set its value to the next index sequence\n",
    "            word2int[word] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some verification that this step work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words: 1774\n",
      "Mapping of the word fun: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Unique Words:\", len(word2int))\n",
    "print(\"Mapping of the word fun:\", word2int['fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update each row in the dataset, replacing words with their unique integer value\n",
    "for row in dataset:\n",
    "    comment = []\n",
    "    # for each word in the comment, lookup its integer value\n",
    "    for word in row['comment']:\n",
    "        # replace the word with its unique integer value\n",
    "        comment.append(word2int[word])\n",
    "    # replace the comment with the list of integer values\n",
    "    row['comment'] = comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at a couple of rows and see what changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': [15, 3, 16, 17, 18, 19, 20, 21, 2, 22, 23, 1, 24, 1, 1],\n",
       " 'rating': 8}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': [1, 25, 26, 1, 1, 27, 8, 28, 29, 30, 3, 31, 32, 0, 0, 0],\n",
       " 'rating': 8}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF SESSION 4\n",
    "\n",
    "You are now ready to input your dataset into a Recurrent Neural Network (RNN) like a LSTM or GRU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
