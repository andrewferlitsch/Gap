{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap Framework - Natural Language Processing\n",
    "## Splitter Module\n",
    "\n",
    "<b>[Github] (https://github.com/andrewferlitsch/gap)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated PDF, Fax, Image Capture Text Extraction with Gap (Session 1)\n",
    "\n",
    "Let's start with the basics. We will be using the <b style='color: saddlebrown'>SPLITTER</b> module in the **Gap** Framework.\n",
    "\n",
    "Steps:\n",
    "1. Import the <b style='color: saddlebrown'>Document</b> and <b style='color: saddlebrown'>Page</b> class from the <b style='color: saddlebrown'>splitter</b> module.\n",
    "2. Create a <b style='color: saddlebrown'>Document</b> object.\n",
    "3. Pass a PDF (text or scanned), Facsimile (TIFF) or image captured document to the <b style='color: saddlebrown'>Document</b> object.\n",
    "4. Wait for the results :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go to the directory where Gap Framework is installed\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Document and Page from the document module\n",
    "from gapml.splitter import Document, Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: saddlebrown'>Document</span> Object\n",
    "\n",
    "The initializer (constructor) takes the following arguments:<br/>\n",
    "\n",
    "        document - path to the document\n",
    "        dir      - directory where to store extracted pages and text\n",
    "        ehandler - function to invoke when processing is completed in asynchronous mode\n",
    "        config   - configuration settings for SYNTAX module\n",
    "        \n",
    "Let's start by preprocessing a 105 page PDF, which is a medical benefits plan. We should see:\n",
    "\n",
    "- Split into individual PDF pages\n",
    "- Text extracted from each page\n",
    "- Individual page PDF and text stored in specified directory.\n",
    "\n",
    "*Note, for brevity, we reduced the size of the PDF document to 10 pages for this code along.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"train/10nc.pdf\", \"train/nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we are done! Let's look at the last page (page 105 ~ page 10 in the shorten version).\n",
    "\n",
    "Wow, that's the foreign language translation page - see how it handles other (non-latin) character sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the name property to see the name of the document\n",
    "print( doc.name )\n",
    "\n",
    "# Use the len() operator to find out how many pages are in the document\n",
    "print( len(doc) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: saddlebrown'>Page</span> Object\n",
    "\n",
    "Let's now dive deeper. When the document was processed, each page was put into a <b style='color: saddlebrown'>Page</b> object. Here are some things we can do:\n",
    "\n",
    "1. Walk thru each page sequentially as an array index (list).<br/>\n",
    "2. See the original text from the page.<br/>\n",
    "3. See the \"default\" NLP preprocessing of the text on the page (which can be modified with *config* settings).<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at one of the pages\n",
    "pages = doc.pages\n",
    "\n",
    "# total number of pages\n",
    "print(len(pages))\n",
    "\n",
    "# Last page in the document\n",
    "pages[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the text for that page (page 10)\n",
    "page = pages[9]\n",
    "page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the default NLP preprocessing of the text (stemming, stopword removal, punct removal)\n",
    "page.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some words appear a lot, like preventive, health and protection. Let's get information on the distribution of words in the page. There are two properties we can use for this purpose:\n",
    "\n",
    "    freqDist - The count of the number of occurrences of each word.\n",
    "    termFreq - The percentage the word appears on the page (TF -> Term Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the frequency distribution (word counts) for the page\n",
    "page.freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see the term frequency (TF)\n",
    "page.termFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: saddlebrown'>Document</span> Object (Advanced)\n",
    "\n",
    "Let's look at more advanced features of the <b style='color:saddlebrown'>Document</b> object.\n",
    "\n",
    "1. Word Count and Term Frequency\n",
    "2. Save and Restore\n",
    "3. Asychronous Processing of Documents\n",
    "4. Scanned PDF / OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "\n",
    "Let's look at a frequency distribution (word count) for the whole document. Note that if we look at just the top 10 word counts (after removing stopwords), it is very clear what the document is about: service, benefit, cover, health, medical, care, coverage, ...\n",
    "\n",
    "If we look at the top 25 word counts, we can see secondary classification indicators, like: plan, medication, treatment, deductible, eligible, dependent, hospital, claim, authorization, prescription and limit.\n",
    "\n",
    "HINT: It's a Healthcare Benefit Plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.freqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re) Load\n",
    "\n",
    "When a <b style='color:saddlebrown'>Document</b> object is created, the individual PDF pages, text extraction and NLP analysis are stored. \n",
    "\n",
    "The document can then be subsequently reloaded from storage without reprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first delete the Document object from memory\n",
    "doc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reload the document from storage.\n",
    "doc = Document()\n",
    "doc.load(\"train/10nc.pdf\", \"train/nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some examples of how the document was reconstructed from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Name, Number of Pages\n",
    "print(doc.document)\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print text from the last page\n",
    "page = doc[9]\n",
    "page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the word (count) frequency distribution\n",
    "doc.freqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Execution\n",
    "\n",
    "Let's say you have PDF files arriving for processing in real-time from various sources. The *ehandler* option provides asynchronous processing of documents. When this option is specified, the document is processed on an independent process thread, and when complete the specified event handler is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def done(document):\n",
    "    print(\"EVENT HANDLER: done\")\n",
    "    \n",
    "doc = Document(\"train/crash_2015.pdf\", \"train/crash\", ehandler=done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a frequency distribution for this document. BTW, it is a 2015 State of Oregon table of crash statistics (single page) from a multi-page report. Note how the top ten words (after stopword removal) indicate what the document is about: serious, injury, fatal, crash, highway, death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.freqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scanned PDF / OCR\n",
    "\n",
    "Let's now process a scanned PDF. That's a PDF which is effective a scanned image of a text document, which is then wrapped inside a PDF.\n",
    "\n",
    "- Split into pages\n",
    "- Extract page image\n",
    "- OCR the image image into text\n",
    "- Extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document.SCANCHECK = 0\n",
    "\n",
    "# OCR the scanned PDF and extract text\n",
    "doc = Document(\"train/4scan.pdf\", \"train/4scan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a few properties of the preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scanned property indicates the document was a scanned PDF (true)\n",
    "print( doc.scanned )\n",
    "\n",
    "# Let's print the number of pages\n",
    "print( len(doc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first page\n",
    "page = doc[0]\n",
    "\n",
    "page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean dir\n",
    "import shutil\n",
    "shutil.rmtree('train/nc')\n",
    "shutil.rmtree('train/4scan')\n",
    "shutil.rmtree('train/crash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF SESSION 1"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
