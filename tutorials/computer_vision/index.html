<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Computer Vision - Gap-ML</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Computer Vision";
    var mkdocs_page_input_path = "tutorials\\computer_vision.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-124527631-1', 'https://andrewferlitsch.github.io/Gap/');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Gap-ML</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../about/">About</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../org-os/">Organization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../quick-start-guide/">Quick Start Guide</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Tutorials</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Computer Vision</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#computer-vision">Computer Vision</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#introduction">Introduction</a></li>
        
            <li><a class="toctree-l4" href="#the-first-steps-in-using-gap-for-computer-vision-cv">The First Steps in using Gap for Computer Vision (CV)</a></li>
        
            <li><a class="toctree-l4" href="#advanced-topics">ADVANCED TOPICS</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../natural_language_processing/">Natural Language Processing</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Specifications</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../specs/splitter_spec/">Splitter</a>
                </li>
                <li class="">
                    
    <a class="" href="../../specs/syntax_spec/">Syntax</a>
                </li>
                <li class="">
                    
    <a class="" href="../../specs/segmentation_spec/">Segmentation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../specs/vision_spec/">Vision</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Gap-ML</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Tutorials &raquo;</li>
        
      
    
    <li>Computer Vision</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/andrewferlitsch/Gap/edit/master/docs/tutorials/computer_vision.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="computer-vision">Computer Vision</h1>
<h2 id="introduction">Introduction</h2>
<p>Welcome to the labs.earth collaborative laboratory tutorials on machine learning. </p>
<p>The computer vision (CV) tutorials will start with the basics and progress to advanced real world applications. The tutorials go beyond explaining the code and steps, to include the answers to the anticipated what and why questions.</p>
<p>Before the advent of machine learning with computer vision and today's modern ML/CV frameworks, working with and building real world applications was once the exclusive domain of imaging scientists. The <strong>Gap</strong> framework extends modern computer vision to software developers, whom are familar with object oriented programming (OOP), object relational models (ORM), design patterns (e.g., MVC), asynchronous programming (AJAX), and microservice architectures.</p>
<p>For the data analyst and statisticians whom feel they don't have the necessary software development background, we encourage you to visit the collaborative lab's training site for fundamentials in modern software programming. Likewise, for those software developers whom feel they don't have the necessary background in statistics and machine learning, we encourage you to visit the collaborative lab's training site for fundamentials in <a href="https://github.com/andrewferlitsch/Training/tree/master/AITraining/Fundamentals/Machine%20Learning">modern statistics and machine learning</a>.</p>
<p>As far as our team and contributers, they keep a single phrase in mind when designing, coding and building tutorials. They like to say that <strong>Gap</strong> is:</p>
<pre><code>                                        Machine Learning for Humans
</code></pre>
<h2 id="the-first-steps-in-using-gap-for-computer-vision-cv">The First Steps in using Gap for Computer Vision (CV)</h2>
<p>The first step in using <strong>Gap</strong> for machine learning (ML) of computer vision (CV) is learning to classify a single object in an image. Is it a dog, a cat, what digit is it, what sign language digit is it, etc...</p>
<p>To do single object classification, depending on the images, one will use either a artificial neural network 
(<a href="https://github.com/andrewferlitsch/Training/blob/master/AITraining/Fundamentals/Machine%20Learning/ML%20Neural%20Networks.pptx">ANN</a>) 
or a convolutional neural network (<a href="https://github.com/andrewferlitsch/Training/blob/master/AITraining/Fundamentals/Machine%20Learning/ML%20Convolutional%20Neural%20Networks.pptx">CNN</a>).</p>
<p>In either case, the raw pixel data is not directly inputted into a neural network. Instead, it has to be prepared into machine learning (ML) ready data. How it is prepared/transformed is dependent on the image source, the type and configuration of the
neural network, and the target application.</p>
<p>Images can come from a variety of sources, such as by your cell phone, images found on the Internet, a <a href="https://en.wikipedia.org/wiki/TIFF">facsimile image (FAX)</a>, a <a href="https://en.wikipedia.org/wiki/Film_frame#Video_frames">video frame from a video stream</a>, a <a href="https://en.wikipedia.org/wiki/Digital_radiography">digitized medical/dental x-ray</a>, a <a href="https://en.wikipedia.org/wiki/Magnetic_resonance_imaging">magnetic resonance imaging (MRI)</a>, a <a href="https://en.wikipedia.org/wiki/Transmission_electron_microscopy">electron microscopy (TEM)</a>, etc. </p>
<p>Images go from very basic like <a href="https://en.wikipedia.org/wiki/Binary_image">1-bit BW single channel (color plane)</a>, to <a href="https://en.wikipedia.org/wiki/Grayscale">8-bit gray scale single channel</a>, to <a href="https://en.wikipedia.org/wiki/8-bit_color">8-bit color three channel (RGB)</a>, to <a href="https://en.wikipedia.org/wiki/Alpha_compositing">8-bit color four channel (+alpha channel)</a>, to <a href="https://en.wikipedia.org/wiki/CMYK_color_model">16-bit high tone (CMYK)</a>, to <a href="https://en.wikipedia.org/wiki/Infrared_photography">infrared</a>, to <a href="https://en.wikipedia.org/wiki/Stereoscopy">stereoscopic images (3D)</a>, <a href="https://en.wikipedia.org/wiki/Sonar">sound navigation and ranging (SONAR)</a>, to <a href="https://en.wikipedia.org/wiki/Radar">RADAR</a>, and more.</p>
<p><img alt="pixels" src="../../img/tutorials/pixels.png" /></p>
<p><img alt="color" src="../../img/tutorials/color.png" /></p>
<h3 id="fundamentals-in-preparing-an-image-for-machine-learning">Fundamentals in Preparing an Image for Machine Learning</h3>
<p>Neural networks take as input numbers, specifically numbers that are continous real numbers and have been <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalized</a>. For images, pixel values are proportionally squashed between 0 and 1. For ANN networks, the inputs need to be a 1D vector, in which case the input data needs to be flatten, while in a CNN, the input is a 2D vector. Neural networks for computer vision take input of fixed sizes, so there is a transformation step to transform the pixel data to the input size and shape of the neural network, and finally assigning a label to the image (e.g., it's a cat). Again, for labels, neural networks use integer numbers; for example a cat must be assigned to a unique integer value and a dog to a different unique integer value. </p>
<p>These are the basic steps for all computer vision based neural networks:</p>
<ul>
<li>Transformation</li>
<li>Normalization</li>
<li>Shaping (e.g., flattening)</li>
<li>Labeling</li>
</ul>
<h3 id="importing-vision-module">Importing Vision module</h3>
<p>The <strong>Vision</strong> module of the <strong>Gap</strong> framework implements the classes and methods for computer vision. Within the <a href="https://github.com/andrewferlitsch/Gap/blob/master/vision.py">Vision</a> module are two primary class objects for data management of images. The <code>Image</code> class manages individual images, while the <code>Images</code> class manages collections of images. As a first step, in your Python script or program you want to import from the <code>Vision</code> module the <code>Image</code> and <code>Images</code> class objects.</p>
<pre><code class="python">from gapml.vision import Image, Images
</code></pre>

<h3 id="preprocessing-preparing-an-image-with-gap">Preprocessing (Preparing) an image with Gap</h3>
<p>Relative to the location of this tutorial are a number of test images used in verifying releases of Gap. For the purpose of these tutorials, the images that are part of the Gap release verification will be used for examples. The test file <code>1_100.jpg</code> is a simple 100x100 96 dpi color image (RGB/8bit) from the Kaggle Fruit360 dataset. This dataset was part of a Kaggle contents to classify different types of fruits and their variety. It was a fairly simple dataset in that all the images were of the same size, type and number of channels. Further, each image contained only the object to classify (i.e., fruit) and was centered in the image.</p>
<p>The first step is to instantiate an <code>Image</code> class object and load the image into it, and its corresponding label. In the example below, an <code>Image</code> object is created where the first two positional parameters are the path to the image and the corresponding label (i.e., 1). </p>
<pre><code class="python">image = Image(&quot;../tests/files/1_100.jpg&quot;, 1)
</code></pre>

<p>While Python does not have OOP polymorphism builtin, the class objects in <strong>Gap</strong> have been constructed to emulate polymorphism in a variety of ways. The first positional parameter (image path) to the <code>Image</code> class can either be a local path or a remote path. In the latter case, a path starting with http or https is a remote path. In this case, a HTTP request to fetch the image from the remote location is made.</p>
<pre><code class="python">image = Image(&quot;https://en.wikipedia.org/wiki/File:Example.jpg&quot;, 1)
</code></pre>

<p>Alternately, raw pixel data can be specified as the first (image) positional parameter, as a numpy array.</p>
<pre><code class="python">raw = cv2.imread(&quot;../tests/files/1_100.jpg&quot;)
image = Image(raw, 1)
</code></pre>

<p>Preprocessing of the image in the above examples is synchronous. The initializer (i.e., constructor) returns an image object once the image file has been preprocessed. Alternately, preprocessing of an image can be done asynchronously, where the preprocessing is performed by a background thread. Asynchronous processing occurs if the keyword parameter <code>ehandler</code> is specified. The value of the parameter is set to a function or method, which is invoked with the image object as a parameter when preprocessing of the image is complete.</p>
<pre><code class="python">image = Image(&quot;../tests/files/1_100.jpg&quot;, 1, ehandler=myfunc)

def myfunc(image):
    print(&quot;done&quot;)
</code></pre>

<p>The <code>Image</code> class has a number of attributes which are accessed using OOP properties (i.e., getters and setters). The attributes below provide information on the source image:</p>
<pre><code class="python">print(image.name)   # the name of the image (e.g., 1_100)
print(image.type)   # the type of the image (e.g., jpg)
print(image.size)   # the size of the image in bytes (e.g., 3574)
print(image.label)  # the label assigned to the image (e.g., 1)
</code></pre>

<p>The raw pixel data of the source image is accessed with the <code>raw</code> property, where property returns the uncompressed pixel data of the source image as a numpy array.</p>
<pre><code class="python">raw = image.raw
print(type(raw))    # outputs &lt;class 'numpy.ndarry'&gt;
print(raw.shape)    # outputs the shape of the source image (e.g., (100, 100, 3))
</code></pre>

<p>The preprocessed machine learning ready data is accessed with the <code>data</code> property, where the property returns the data as a numpy array.</p>
<pre><code class="python">data = image.data
print(type(data))   # outputs &lt;class 'numpy.ndarry'&gt;
print(data.shape)   # outputs the shape of the machine learning data (e.g., (100, 100, 3))
</code></pre>

<p>By default, the shape and number of channels of the source image are maintained in the preprocessed machine learning ready data, and the pixel values are normalized to values between 0.0 and 1.0. </p>
<pre><code class="python">print(raw[0][80])   # outputs pixel values (e.g., [250, 255, 255])
print(data[0][80])  # outputs machine learning ready data values (e.g., [0.98039216, 1.0, 1.0])
</code></pre>

<p>When processing of the image is completed, the machine learning ready data, and attributes are stored in a HDF5 (Hierarchical Data Format) formatted file. By default, the file is stored in the current local directory, where the rootname of the file is the rootname of the image. Storage provides the means to latter retrieval the machine learning ready data for feeding into a neural network, and/or retransforming the machine learning ready data. In the above example, the file would be stored as:</p>
<pre><code>./1_100.hd5
</code></pre>
<p>The path location of the stored HDF5 can be specified with the keyword parameter <code>dir</code>.</p>
<pre><code class="python">image = Image(&quot;../tests/files/1_100.jpg&quot;, 1, dir=&quot;tmp&quot;)
</code></pre>

<p>In the above example, the HDF5 file will be stored under the subdirectory <code>tmp</code>. If the subdirectory path does not exist, the <code>Image</code> object will attempt to create the subdirectory.</p>
<p>The <code>Image</code> class optionally takes the keyword parameter <code>config</code>. This parameter takes a list of one or more settings, which alter how the image is preprocessed. For example, one can choose to disable storing to the HDF5 file using the keyword parameter <code>config</code> with the setting <code>nostore</code>.</p>
<pre><code class="python">image = Image(&quot;../tests/files/1_100.jpg&quot;, 1, config=['nostore'])
</code></pre>

<p>Alternately, one could choose to additionally store the raw pixel data to the HDF5 file using the keyword parameter <code>config</code> with the setting <code>raw</code>.</p>
<pre><code class="python">image = Image(&quot;../tests/files/1_100.jpg&quot;, 1, config=['raw'])
</code></pre>

<h3 id="example-cloud-based-image-processing-pipeline">Example: Cloud-based Image Processing Pipeline</h3>
<p>For a real-world example, let's assume one is developing a cloud based system that takes images uploaded from users, with the following requirements.</p>
<ul>
<li>Handles multiple users uploading at the same time.</li>
<li>Preprocessing of the images is concurrent.</li>
<li>The machine learning ready data is passed to another step in a data (e.g., workflow) pipeline.</li>
</ul>
<p>Below is a bare-bones implementation.</p>
<pre><code class="python">def first_step(uploaded_image, label):
    &quot;&quot;&quot; Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning 
        ready data to another step in a data pipeline.
    &quot;&quot;&quot;
    image = Image(uploaded_image, label, ehandler=next_step, config=['nostore'])

def next_step(image):
    &quot;&quot;&quot; Do something with the Image object as the next step in the data pipeline &quot;&quot;&quot;
    data = image.data
</code></pre>

<h3 id="preprocessing-transformations-resizing-reshaping-flattening">Preprocessing Transformations: Resizing, Reshaping, Flattening</h3>
<p>The keyword parameter <code>config</code> has a number of settings for specifying how the raw pixel data is preprocessed. The <strong>Gap</strong> framework is designed to eliminate the use of large numbers of keyword parameters, and instead uses a modern convention of passing in a configuration parameter. Here are some of the configuration settings:</p>
<pre><code>nostore                     # do not store in a HDF5 file
grayscale | gray            # convert to a grayscale image with a single channel (i.e., color plane)
flatten   | flat            # flatten the machine learning ready data into a 1D vector
resize=(height, width)      # resize the raw pixel data
thumb=(height, width)       # create (and store) a thumbnail of the raw pixel data
raw                         # store the raw pixel data
float16 | float32 | float64 # data type of normalized pixel data (e.g., float32 is default)
</code></pre>
<p>Let's look how you can use these settings for something like neural network's equivalent of the hello world example ~ <a href="https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners">training the MNIST dataset</a>. The MNIST dataset consists of 28x28 grayscale images. Do to its size, grayscale and simplicity, it can be trained with just a ANN (vs. CNN). Since ANN take as input a 1D vector, the machine learning ready data would need to be reshaped (i.e., flatten) into a 1D vector.</p>
<pre><code class="python"># An example of how one might use the Image object to preprocess an image from the MNIST dataset for a ANN
image = Image(&quot;mnist_example.jpg&quot;, digit, config=[&quot;gray&quot;, &quot;flatten&quot;])

print(image.shape)  # would output (784,)
</code></pre>

<p>In the above, the preprocessed machine learning ready data will be in a vector of size 784 (i.e., 28x28) with data type float. </p>
<p>Let's look at another publicly accessible training set, the Fruits360 Kaggle competition. In this training set, the images are 100x100 RGB images (i.e., 3 channels). If one used a CNN for this training set, one would preserve the number of channels. But the input vector may be unneccessarily large for training purposes (30000 ~ 100x100x3). Let's reduce the size using the resize setting by 1/4.</p>
<pre><code class="python">image = Image(&quot;../tests/files/1_100.jpg&quot;, config=['resize=(50,50)'])

print(image.shape)  # would output (50, 50, 3)
</code></pre>

<h3 id="example-image-processing-dashboard">Example: Image Processing Dashboard</h3>
<p>Let's expand on the real-word cloud example from earlier. In this case, let's assume that one wants to have a dashboard for a DevOps person to monitor the preprocessing of images from a user, with the requirements:</p>
<ul>
<li>Each time an image is preprocessed, the following is displayed on the dashboard:<ul>
<li>A thumbnail of the source image.</li>
<li>The amount of time to preprocess the image.</li>
<li>Progress count of number of images preprocessed and accumulated time.</li>
</ul>
</li>
</ul>
<p>Here's the updated code:</p>
<pre><code class="python">def first_step(uploaded_image, label):
    &quot;&quot;&quot; Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning 
        ready data to another step in a data pipeline.
    &quot;&quot;&quot;
    image = Image(uploaded_image, label, ehandler=second_step, config=['nostore', 'thumb=(16,16)'])

nimages = 0
nsecs   = 0

def second_step(image):
    &quot;&quot;&quot; Display progress in dashboard &quot;&quot;&quot;
    # Progress Accumulation
    nimages += 1
    nsecs += image.time

    # Construct message and pass thumbnail and msg to the dashboard
    msg = &quot;Time %d, Number: %d, Accumulated: %f&quot; % (time.time, nimages, nsecs)
    dashboard.display(img=image.thumb, text=msg)

    # The next processing step ...
    third_step(image)
</code></pre>

<p>Okay, there is still some problem with this example in that <code>nimages</code> and <code>nsecs</code> are global and would be trashed by concurrent processing of different users. The <code>ehandler</code> parameter can be passed a tuple instead of a single value. In this case, the <code>Image</code> object emulates polymorphism. When specified as a tuple, the first item in the tuple is the event handler and the remaining items are additional arguments to the event handler. Let's now solve the above problem by adding a new object <code>user</code> which is passed to the first function <code>first_step()</code>. The <code>user</code> object will have fields for accumulating the number of times an image was processed for the user and the accumulated time. The <code>ehandler</code> parameter is then modified to pass the <code>user</code> object to the event handler <code>second_step()</code>.</p>
<pre><code class="python">def first_step(uploaded_image, label, user):
    &quot;&quot;&quot; Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning 
        ready data to another step in a data pipeline.
    &quot;&quot;&quot;
    image = Image(uploaded_image, label, ehandler=(second_step, user), config=['nostore', 'thumb=(16,16)'])

def second_step(image, user):
    &quot;&quot;&quot; Display progress in dashboard &quot;&quot;&quot;
    # Progress Accumulation
    user.nimages += 1
    user.nsecs += image.time

    # Construct message and pass thumbnail and msg to the dashboard
    msg = &quot;Time %d, Number: %d, Accumulated: %f&quot; % (time.time, nimages, nsecs)
    dashboard.display(img=image.thumb, text=msg)

    # The next processing step ...
    third_step(image, user)
</code></pre>

<h3 id="image-retrieval">Image Retrieval</h3>
<p>By default, the <code>Image</code> class will store the generated HDF5 in the current working directory (i.e., ./). The keyword parameter <code>dir</code> tells the <code>Image</code> class where to store the generated HDF5 file.</p>
<pre><code class="python">image = Image(&quot;../tests/files/1_100.jpg&quot;, dir='tmp')  # stored as tmp/1_100.h5
</code></pre>

<p>Once stored, the <code>Image</code> object subsequently can be retrieved (i.e., reused) from the HDF5 file. In the example below, an empty <code>Image</code> object is first instantiated, and then the method <code>load()</code> is invoked passing it the name (rootname) of the image and the directory where the HDF5 file is stored, if not in the current working directory.</p>
<pre><code class="python">image = Image()
image.load('1_100', dir='tmp')

# retrieve the machine learning ready data from the loaded Image object
data = image.data
</code></pre>

<h3 id="image-reference">Image Reference</h3>
<p>For a complete reference on all methods and properties for the <code>Image</code> class, see <a href="../../specs/vision_spec/">reference</a>.</p>
<h3 id="image-collections">Image Collections</h3>
<p>The <code>Images</code> class provides preprocessing of a collections of images (vs. a single image). The parameters and emulated polymorphism are identical to the <code>Image</code> class, except the images and labels parameter refer to a plurality of images, which comprise the collection. The positional parameter <code>images</code> can be specified as:</p>
<ul>
<li>A list of local or remote images (e.g., [ '1_100.jpg', '2_100.jpg', '3_100.jpg'])</li>
<li>A single directory path of images  (e.g., 'apple')</li>
<li>A list of directory paths of images (e.g., ['apple', 'pear', 'banana'])</li>
<li>A numpy multi-dimensional array</li>
</ul>
<p>The corresponding positional parameter <code>labels</code> must match the number of images as follows:</p>
<ul>
<li>A single value, applies to all the images (e.g., 1)</li>
<li>A list of values which are the same length as the list of images or directory paths (e.g., [1, 2, 3]).</li>
</ul>
<p>The example below creates an <code>Images</code> objects consisting of three images with corresponding labels 1, 2 and 3.</p>
<pre><code class="python">images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3])
</code></pre>

<p>For each image specified, the <code>Images</code> class creates an <code>Image</code> object, which are maintained in the <code>images</code> objects as a list. The list of corresonding <code>Image</code> objects can be accessed from the property <code>images</code>. In the example below, a collection of three images is created, and then the <code>images</code> property is accessed as a list iterator in a for loop. On each loop, the next <code>Image</code> object is accessed and inside the loop the code prints the name and label of the corresponding <code>Image</code> object.</p>
<pre><code class="python">images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3])
for image in images.images:
    print(image.name, image.label)
</code></pre>

<p>will output:</p>
<pre><code>1_100 1
2_100 2
3_100 3
</code></pre>
<p>The builtin operators <code>len()</code> and <code>[]</code> are overridden in the <code>Images</code> class. The <code>len()</code> operator will return the number of images, and the list (array) index operator <code>[]</code> will return the <code>Image</code> object at the corresponding index. Using the builtin operators, the above example can be alternately coded as:</p>
<pre><code class="python">        for i in range(len(images)):
            print(images[i].name, images[i].label)
</code></pre>

<h3 id="collection-storage-retrieval">Collection Storage &amp; Retrieval</h3>
<p>The <code>Images</code> class, disables the <code>Image</code> objects from storing the machine learning ready data as individual HDF5 files per image, and insteads stores a single HDF5 for the entire collection. By default, the file name combines the prefix <code>collection.</code> with the root name of the first image in the collection, and is stored in the current working directory. In the above example, the machine learning ready data for the entire collection would be stored as:</p>
<pre><code>./collection.1_100.h5
</code></pre>
<p>The directory where the HDF5 file is stored can be changed with the keyword paramater <code>dir</code>, and the root name of the file can be set with the keyword parameter <code>name</code>.</p>
<pre><code class="python">images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3], dir='tmp', name='apples')
</code></pre>

<p>In the above example, the machine learning ready data is stored as:</p>
<pre><code>./tmp/apples.h5
</code></pre>
<p>A stored collection can the be subsequently retrieved from storage by instantiating an empty <code>Images</code> object and invoking the <code>load()</code> method with the corresponding collection name. For the above example, the apples collection would be retrieved and <code>Images</code> and corresponding <code>Image</code> objects reconstructed in memory:</p>
<pre><code class="python"># Instantiate an empty Images object
images = Images()

# Set the directory where the collection is
images.dir = 'tmp'

# Load the collection into memory
images.load('apples')
</code></pre>

<p>Alternately, the <code>load()</code> method can be passed the keyword parameter <code>dir</code> to specify the directory where the collection is stored. For the above, this can be specified as:</p>
<pre><code class="python">images.load('apples', dir='tmp')
</code></pre>

<h3 id="example-data-preparation-for-a-fruits-dataset-as-individual-collections">Example: Data Preparation for a Fruits Dataset: As Individual Collections</h3>
<p>In this example, a dataset of images of fruit are preprocessed into machine learning ready data, as follows:</p>
<ol>
<li>The images for each type of fruit are in separate directories (i.e., apple, pear, banana).</li>
<li>The labels for the fruit will are sequentially numbered (i.e., 1, 2, 3).</li>
<li>The images will be preserved as color images, but resized to (50,50).</li>
<li>The shape of the preprocessed machine learning data will be (50, 50, 3) for input to a CNN.</li>
</ol>
<p>In the example below, a separate collection is created for each type of fruit:</p>
<pre><code class="python">apples   = Images( 'apple',  1, name='apples',  config=['resize=(50,50)'] )
pears    = Images( 'pear' ,  2, name='pears',   config=['resize=(50,50)'] )
bananas  = Images( 'banana', 3, name='bananas', config=['resize=(50,50)'] )
</code></pre>

<p>In the above example, the machine learning ready data is stored as:</p>
<pre><code>./apples.h5
./pears.h5
./bananas.h5
</code></pre>

<p>In the above example, the preprocessing of each type of fruit was sequentially. Since conventional CPUs today are multi-core, we can take advantage of concurrency and speed up the processing in many computers by using the keyword parameter <code>ehandler</code> to process each collection asynchronously in parallel.</p>
<pre><code class="python">accumulated = 0

label = 1
for fruit in ['apple', 'pear', 'banana']:
    Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler )
    label += 1

def collectionHandler(images):
    accumulated += images.time
    print(&quot;Number of Images:&quot;, len(images), &quot;Time:&quot;, images.time)
</code></pre>

<p>Let's describe some of the aspects of the above example. For the directories, we created a list of the directory names and then iterated through it. For each iteration, we:</p>
<ul>
<li>Instantiate an <code>Images</code> object for the current fruit.</li>
<li>Set the collection name to the plural of the fruit name (i.e., fruit + 's').</li>
<li>Use an incrementer, starting at 1, for the label.</li>
<li>Use the <code>ehandler</code> parameter to process the collection asynchronously.</li>
</ul>
<p>When each collection is completed, the function collectionHandler is called. This function will print the number of images
processed in the collection, the time (in seconds) to process the collection, and the accumulated processing time for all the collections.</p>
<h3 id="example-data-preparation-for-a-fruits-dataset-as-a-combined-collection">Example: Data Preparation for a Fruits Dataset: As a Combined Collection</h3>
<p>Alternatively, we can process a group of collections as a single combined collection. The example below does the same as the aforementioned example, but produces a single (combined) dataset (vs. three individual datasets).</p>
<pre><code class="python">fruits = Images( ['apples', 'pears', 'bananas'], [1, 2, 3], name='fruits', config=['resize=(50,50)'] )
</code></pre>

<p>In the above example, the machine learning ready data is stored as:</p>
<pre><code>./fruits.h5
</code></pre>
<p>Can we improve on the above? We got the benefit of a combined collection, but lost the benefit of concurrently preprocessing each collection. That's not overlooked. The <code>+=</code> operator for the <code>Images</code> collection is overridden to combine collections. Let's update the earlier example to preprocess each collection asynchronously and combine them into a single collection.</p>
<pre><code class="python">dataset = None
accumulated = 0
lock = Lock()

label = 1
for fruit in ['apple', 'pear', 'banana']:
    Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler )
    label += 1

def collectionHandler(images):
    accumulated += images.time
    print(&quot;Number of Images:&quot;, len(images), &quot;Time:&quot;, images.time)

    # these steps need to be atomic
    lock.acquire()
    if dataset is None:
        dataset = images
    else:
        dataset += images
    lock.release()
</code></pre>

<p>In the above example, we used the variable dataset for the combined collection. After the first collection is preprocessed, we set the variable dataset to the first <code>Images</code> object, and afterwards we combine it with subsequent collections using the <code>+=</code> operator.</p>
<p>Because the processing and invoking the event handler happen concurrently, there are possible problems including a race condition (i.e., two threads access dataset at the same time), and trashing the internal data (i.e., two threads are combining data at the same time). We solve this by making this operation atomic using Python's thread lock mechanism.</p>
<h3 id="example-image-data-is-already-numpy-preprocessed">Example: Image Data is Already Numpy Preprocessed</h3>
<p><strong>Gap</strong> can handle datasets that have been prior preprocessed into numpy arrays, where the image data has been normalized and the label data has been one-hot encoded. For example, the Tensorflow MNIST example dataset, all the images have been flatten and normalized into a numpy array, and all the labels have been one-hot encoded into a 2D numpy matrix. Below is an example demonstrating importing the datasets into an <code>Images</code> collection.</p>
<pre><code class="python"># Import the MNIST input_data function from the tutorials.mnist package
from tensorflow.examples.tutorials.mnist import input_data

# Read in the data
# The paramter one_hot=True refers to the label which is categorical (1-10). 
# The paramter causes the label to be re-encoded as a 10 column vector.
mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)

# Create the images collection for the Training Set
train = Images(mnist.train.images, mnist.train.labels)

# Create the images collection for the Test Set
test = Images(mnist.test.images, mnist.test.labels)
</code></pre>

<h3 id="size-of-preprocessed-machine-learning-ready-data">Size of Preprocessed Machine Learning Ready Data</h3>
<p>When preprocessing image data into machine learning ready data, there can be a significant expansion in size. For example, the average size of an (compressed) JPEG flowers sample set (not shown) image is 30K bytes. The compression ratio on these image is as much as 90%. When read in by openCV and decompressed into a raw pixel image, the size typically is 250K bytes. In the raw pixel data, the byte size per pixel is 1 (i.e., 8bits per pixel). When the data is normalized (e.g., divided by 255.0), each pixel becomes represented by a floating point value. By default, the data type is np.float32, which is a 4 byte per pixel representation. Thus, a 250K byte raw pixel image will expand to 1Mb in memory.</p>
<p>The <code>config</code> parameter setting <code>float=</code> overrides the size of the floating point representation after normalization. We generally recommend maintaining the 32-bit representation (np.float32). A smaller representation, such as a half floating point (np.float16) may expose the model to a vanishing gradient during training, while half the memory size. Some hardware, such as newer NVIDIA GPUs doing 16bit matrix multiplicaiton, have specialized hardware using stochastic rounding to eliminate the vanishing gradient problem. If you have this type of hardware, you can utilize the float16 representation to decrease the memory footprint by 50% and reduce instruction execution by appropriametly 75%. The following are the <code>float</code> settings for the <code>config</code> parameter:</p>
<pre><code>float16
float32
float64
</code></pre>
<p>The following is an example usage:</p>
<pre><code class="python"># Use 16-bit floating point representation per pixel
images = Images(['flowers_photo/roses'], 1, config=['float16'])
</code></pre>

<h3 id="splitting-the-collection-into-training-and-test-data">Splitting the Collection into Training and Test Data</h3>
<p>The first step to training a neural network is to split the collection into training and test data. We will cover some basic cases here.</p>
<p>One uses the property <code>split</code> as a setter to split the collection into training and test data. This property will randomized the order of the <code>Image</code> objects in the collection, create a partition between the train and test data and create a corresponding internal index. The <code>split</code> property is implemented using emulated polymorphism, whereby the property can be given a single value or a tuple. The first value (parameter) is the percentage of the collection that will be set aside as testing data, and must be between 0 and 1. Below is an example:</p>
<pre><code class="python"># 20% of the collection is test, and 80% is training
images.split = 0.2
</code></pre>

<p>In another case, one might have separate collections for train and test. In this case, for both collections set the split to 0, which means use the entire collection, but otherwises randomizes the order of the <code>Image</code> objects.</p>
<pre><code class="python">train = Images( ['train/apple', 'train/pear', 'train/banana'], [1, 2, 3], config=['resize=(50,50)'])
test  = Images( ['test/apple',  'test/pear',  'test/banana'],  [1, 2, 3], config=['resize=(50,50)'])
train.split = 0
test.split  = 0
</code></pre>

<p>The random number generation by default will start at a different seed each time. If you need (desire) consistency between training on the results for comparison or demo'ing, then one specifies a seed value for the random number generation. The seed value is an integer value and is specified as a second parameter (i.e., tuple) to the <code>split</code> property. In the example below, the split is set to 20% test, and the random seed set to 42.</p>
<pre><code>images.split = 0.2, 42
</code></pre>
<p>One can see the index of the randomized distribution by displaying the internal member <code>_train</code>. This member is a list of integers which correspond to the index in the <code>images</code> list. While Python does not support the OOP concept of data encapsulation using private members, the <strong>Gap</strong> framework follows the convention that any member beginning with an underscore should be treated by developers as private. While not enforced by Python, members like <code>_train</code> should only be read and not written. The example below accesses (read) the randomized index for the training data and then prints it.</p>
<pre><code class="python">indexes = images._train
print(indexes)
</code></pre>

<h3 id="forward-feeding-a-neural-network">Forward Feeding a Neural Network</h3>
<p>The <code>Images</code> class provides methods for batch, stochastic and mini-batch feeding for training and evaluating a neural network. The feeders produce full batch samples, single samples and mini-batch samples as numpy matrixes, which are compatible for input with all Python machine learning frameworks that support numpy arrays, such as Tensorflow, Keras and Pytorch, for example. Forward feeding is randomized, and the entire collection(s) can be continuously re-feed (i.e., epoch), where each time they are re-randomized.</p>
<p>The <code>split</code>, <code>minibatch</code>, and overriden <code>next()</code> operator support forward feeding. </p>
<h4 id="batch-feeding">Batch Feeding</h4>
<p>In batch mode, the entire training set can be ran through the neural network as a single pass, prior to backward propagation and updating the weights using gradient descent. This is known as 'batch gradient descent'.</p>
<p>When the <code>split</code> property is used as a getter, it returns the image data and corresponding labels for the training and test set similar to using sci-learn's <code>train_test_split()</code> function, as numpy arrays, and the labels are one hot encoded. In the example below:</p>
<ul>
<li>The dataset is split into 20% test and 80% training.</li>
<li>The <code>X_train</code> and <code>X_test</code> is the list of machine learning ready data, as numpy arrays, of the corresponding training and test images.</li>
<li>The <code>Y_train</code> and <code>Y_test</code> is the list of the corresponding labels.</li>
<li>The variable epochs is the number of times the <code>X_train</code> dataset will be forward feed through the neural network.</li>
<li>The optimizer performs backward probagation to update the weights.</li>
<li>At the end of each epoch, The training data is re-randomized by calling the <code>split</code> method again as a getter.</li>
<li>When training is done, the <code>X_test</code> and corresponding <code>Y_test</code> are forward feed to evaluate the accuracy of the trained model.</li>
</ul>
<pre><code class="python"># Get the first randomized split of the dataset
images.split = 0.2, 42
X_train, X_test, Y_train, Y_test = images.split

nepochs = 200   # the number of times to feed the entire training set while training the neural network
for _ in range(nepochs):
    # Feed the entire training set per epoch (i.e., X_train, Y_train) and calculate the cost function
    pass

    # Run the optimizer (backward probagation) to update the weights
    pass

    # Re-randomize the training set
    X_train, _, Y_train, _ = images.split


# Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train)
pass

# Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test)
pass
</code></pre>

<h4 id="stochastic-feeding">Stochastic Feeding</h4>
<p>Another way of feeding a neural network is to feed one image at a time and do backward probagation, using gradient descent. This is known as 'stochastic gradient descent'.</p>
<p>The <code>next()</code> operator supports iterating through the training list one image object at a time. Once all of the entire training set has been iterated through, the <code>next()</code> operator returns None, and the training set is randomly re-shuffled for the next epoch.</p>
<pre><code class="python"># Split the data into test and training datasets
images.split = 0.2, 42

# Forward Feed the training set 200 times (epochs)
epochs = 200
for _ in range(epochs):

  # Now terate through the ML ready data and label for each image in the training set
  while True:
      data, label = next(images)
      if data is None: break

      # Forward feed the image data and label through the neural network and calculate the cost function
      pass

      # Run the optimizer (backward probagation) to update the weights
      pass

# Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train)
pass

# Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test)
pass
</code></pre>

<h4 id="mini-batch-feeding">Mini-Batch Feeding</h4>
<p>Another way of feeding a neural network is through mini-batches. A mini-batch is a subset of the training set, that is greater than one. After each mini-batch is feed, then backward probagation, using gradient descent, is done.</p>
<p>Typically, mini-batches are set to sizes like 30, 50, 100, or 200. The <code>minibatch</code> property when used as a setter, will set the size of the mini-batches. In the example below, the mini-batch size is set to 100.</p>
<pre><code class="python">images.minibatch = 100
</code></pre>

<p>When the <code>minibatch</code> property is used as a getter, it will produce a generator, which will generate a batch from the training set of the size specified when used as a setter. Each time the <code>minibatch</code> property is called as a getter, it will sequentially move through the randomized set of training data. Upon completion of an epoch, the training set is re-randomized, and the <code>minibatch</code> property will reset to the begining of the training set. In the example below:</p>
<ul>
<li>The minibatch size is set to 100.</li>
<li>The total number of batches for the training set is calculated.</li>
<li>The training set is forward feed through the neural network 200 times (epochs).</li>
<li>On each epoch, the training set is partitioned into mini-batches.</li>
<li>After each mini-batch is feed, run the optimizer to update the weights.</li>
</ul>
<pre><code class="python"># Set the minibatch size
images.minibatch = 100

# Calculate the number of batches
nbatches = len(images) // 100

# Forward Feed the training set 200 times (epochs)
epochs = 200
for _ in range(epochs):

  # Process each mini-batch
  for _ in range(nbatches):
      # Create a generator for the next minibatch
      g = images.minibatch

      # Get the data, labels for each item in the minibatch
      for data, label in g:
          # Forward Feed the image data and label
          pass

      # Run the optimizer (backward probagation) to update the weights after each mini-batch
      pass

# Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train)
pass

# Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test)
pass
</code></pre>

<h3 id="image-data-augmentation">Image (Data) Augmentation</h3>
<p>Image Augmentation is the process of generating (synthesizing) new images from existing images, which can then be used to augment the training process. Synthesis can include, rotation, skew, sharpending and blur of existing images. These new images are then feed into the neural network during training to augment the training set. Rotating and skew aid in recognizing images from different angles, and sharpening and blur help generalize recognition (offset overfitting), as well as recognition under different lightening and time of day conditions.</p>
<p>When the <code>augment</code> property is used as a setter, it will either enable or disable image augmentation when forward feeding the neural network when used in conjunction with the <code>split</code> property, <code>minibatch</code> property or <code>next()</code> operator. The <code>augment</code> property uses emulated polymorphism for the paramters. When the parameter is <code>True</code>, the feed forward process (e.g., <code>next()</code>) will generate an additional augmented image for each image in the training set, where the augmented image is a random rotation between -90 and 90 degress of the original image. The augmentation process adjusts the height and width of the image prior to rotation, as to prevent cropping, and then resizes back to the target size.</p>
<pre><code class="python"># Split the data into test and training datasets
images.split = 0.2, 42

# Enable image augmentation
images.augmentation = True

# Forward Feed the training set 200 times (epochs)
epochs = 200
for _ in range(epochs):

  # Now terate through the ML ready data and label for each image in the training set
  while True:
      # Twice as many images as size of the training set will be generated, where every other image
      # is a random rotation between -90 and 90 degrees of the last image.
      data, label = next(images)
      if data is None: break

      # Forward feed the image data and label through the neural network and calculate the cost function
      pass

      # Run the optimizer (backward probagation) to update the weights
      pass

# Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train)
pass

# Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test)
pass
</code></pre>

<p>The parameter to the <code>augment</code> property may also be a tuple. The tuple specifies the rotation range and optionally the number of agumented images to generate per image; otherwise defaults to one. In the example below:</p>
<ul>
<li>Augmented images will be a random rotation between -45 and 120.  </li>
<li>For each image, three augmented images will be generated.  </li>
<li>The mini-batch size is set to 100, so with the augmentation each mini-batch will produce 400 images.</li>
</ul>
<pre><code class="python">images.augment = -45, 120, 3
images.minibatch = 100
</code></pre>

<h3 id="transformation">Transformation</h3>
<p>The transformation methods provide the ability to transform the existing stored machine learning ready data into another shape without reprocessing the image data. This feature is particularly useful if the existing machine learning ready data is repurposed for another neural network whose input is a different shape.</p>
<p>The property <code>flatten</code> when used as a setter will flatten and unflatten the preprocessed machine learning ready data. When set to True, the preprocessed machine learning ready data will be transformed from 2D/3D matrixes to a 1D vector, such as repurposing the data from a CNN to an ANN. Below is an example:</p>
<pre><code class="python"># Process images as shape (60, 60, 3)
images =  Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)'])

# Retrieve the preprocess collection of images from storage
images = Images()
images.load('fruit')

# Display the existing shape: will output (60, 60, 3)
print(images[0].datas.shape)

# Convert to 1D vector
images.flatten = True
# Display the existing shape: will output (10800,)
print(images[0].datas.shape)
</code></pre>

<p>When set to False, the preprocessed machine learning ready data will be transformed from a 1D vector to a 3D matrix, such as repurposing from an ANN to a CNN. Below is an example:</p>
<pre><code class="python"># Process images as shape (10800,)
images =  Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)', 'flatten'])

# Retrieve the preprocess collection of images from storage
images = Images()
images.load('fruit')

# Display the existing shape: will output (10800,)
print(images[0].datas.shape)

# Convert to 3D vector
images.flatten = False
# Display the existing shape: will output (60, 60, 3)
print(images[0].datas.shape)
</code></pre>

<p>The property resize when used as a setter will resize the preprocessed machine learning ready data.</p>
<pre><code class="python"># Process images as shape (60, 60, 3)
images =  Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)']) 

# Retrieve the preprocess collection of images from storage
images = Images()
images.load('fruit')

# Display the existing shape: will output (60, 60, 3)
print(images[0].datas.shape)

# resize to (50, 50)
images.resize = (50, 50)

# Display the existing shape: will output (50, 50, 3)
print(images[0].datas.shape)
</code></pre>

<h3 id="images-reference">Images Reference</h3>
<p>For a complete reference on all methods and properties for the <code>Images</code> class, see <a href="../../specs/vision_spec/">reference</a>.</p>
<h2 id="advanced-topics">ADVANCED TOPICS</h2>
<p>This section discusses more advanced topics in uses the <strong>Gap</strong> computer vision module.</p>
<h3 id="processing-errors">Processing Errors</h3>
<p>The <code>Images</code> class tracks images that fail to be preprocessed. Examples for failure are: image does not exist, not an image, etc. The property <code>fail</code> will return the number of images that failed to preprocess and the property <code>errors</code> will return a list of tuples, where each tuple is the corresponding image argument that failed to preprocess, and the reason it failed.</p>
<pre><code class="python"># assume that nonexist.jpg does not exist
images = Images(['good_image.jpg', 'nonexist.jpg'], 1)

# The length of the collection will be only one image (i.e., output from print is 1)
print(len(images))

# Will output 1 for the one failed image (i.e., nonexist.jpg)
print(images.fail)

# Will output: [ ('nonexist.jpg', 'FileNotFoundError') ]
print(images.errors)
</code></pre>

<h3 id="image-dataset-as-numpy-multi-dimensional-array">Image Dataset as Numpy Multi-Dimensional Array</h3>
<p>Many of the machine learning frameworks come with prepared training sets for their tutorials, such as the MNIST, CIFAR, IRIS, etc. In some cases, the training set may already be in a numpy multi-dimensional format:</p>
<p><em>Color RGB</em><br />
  Dimension 1: Number of Images<br />
  Dimension 2: Image Height<br />
  Dimension 3: Image Width<br />
  Dimension 4: Number of Channels  </p>
<p><em>Grayscale</em><br />
  Dimension 1: Number of Images<br />
  Dimension 2: Image Height<br />
  Dimension 3: Image Width  </p>
<p><em>Flatten</em><br />
  Dimension 1: Number of Images<br />
  Dimension 2: Flatten Pixel Data  </p>
<p>This format of a training set can be passed into the Images class as the <code>images</code> parameter. If the data type of the pixel data is <code>uint8</code> or <code>uint16</code>, the pixel data will be normalized; otherwise, data type is float, the pixel data is assumed to be already normalized.</p>
<pre><code class="python"># Let's assume that the image data is already available in memory, such as being read in from file by openCV
import cv2
raw = []
raw.append(cv2.imread('image1.jpg'))  # assume shape is (100, 100, 3)
raw.append(cv2.imread('image2.jpg'))  # assume shape is (100, 100, 3)

# Let's assume now that the list of raw pixel images has been converted to a multi-dimensional numpy array
import numpy as np
dataset = np.asarray(raw)

print(dataset.shape)  # would print: (2, 100, 100, 3)

images = Images(dataset, labels)
print(len(images))      # will output 2
print(images[0].shape)  # will output (100, 100, 3)
</code></pre>

<h3 id="reducing-storage-by-deferring-normalization">Reducing Storage by Deferring Normalization</h3>
<p>In some cases, you may want to reduce your overall storage of the machine learning ready data. By default, each normalized pixel is stored as a float32, which consists of 4 bytes of storage. If the <code>config</code> setting <code>uint8</code> is specified, then normalization of the image is deferred. Instead, each pixel is kept unchanged (non-normalized) and stored as a uint8, which consists of a single byte of storage. For example, if a dataset of 200,000 images of shape (100,100,3) which has been normalized will require 24GB of storage. If stored unnormalized, the data will only require 1/4 the space, or 6GB of storage.</p>
<p>When the dataset is subsequently feed (i.e., properties <code>split</code>, <code>next()</code> and <code>minibatch</code>), the pixel data per image will be normalized in-place each time the image is feed.</p>
<pre><code class="python"># Create the machine learning ready data without normalizing the image data
images = Images(dataset, labels, config=['uint8'])

# set 20% of the dataset as test and 80% as training
images.split = 0.2

# Run 100 epochs of the entire training set through the model
epochs = 100
for _ in range(epochs):
  # get the next image - which will be normalized in-place
  x, y = next(images)

  # send image through the neural network ....
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../natural_language_processing/" class="btn btn-neutral float-right" title="Natural Language Processing">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../quick-start-guide/" class="btn btn-neutral" title="Quick Start Guide"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/andrewferlitsch/Gap/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../../quick-start-guide/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../natural_language_processing/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
