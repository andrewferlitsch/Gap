{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gap : NLP/CV Data Engineering Framework, v0.9.2 (Pre-launch: alpha) Natural Language Processing for PDF, TIFF, and Camera Captured Documents, and Computer Vision Processing for Images Framework The Gap NLP/CV data engineering framework provides an easy to get started into the world of machine learning for your unstructured data in PDF documents, scanned documents, TIFF facsimiles and camera captured documents, and your image data in image files and image repositories. NLP Automatic OCR of scanned PDF and camera captured images. Automatic Text Extraction from documents. Automatic Syntax Analysis. Optional Romanization of Latin-1 diacritic characters. Optional Spell Correction. Programmatic control for data extraction or redaction (de-identification). Names, Addresses, Proper Places Social Security Numbers, Data of Birth, Gender, Age Telephone Numbers Numerical Information (e.g., medical, financial, \u2026) and units of measurement. Unit conversion from US Standard to Metric, and vice-versa Unicode character recognition Machine Training of Document and Page Classification. Asynchronous processing of documents. Automatic generation of NLP machine learning ready data. CV Automatic storage and retrieval with high performance HDF5 files. Automatic handling of mixed channels (grayscale, RGB and RGBA) and pixel size. Programmatic control of resizing. Programmatic control of conversion into machine learning ready data format: decompression, normalize, flatten. Programmatic control of minibatch generation. Programmatic control of image augmentation. Asynchronous processing of images. Automatic generation of CV machine learning ready data. The framework consists of a sequence of Python modules which can be retrofitted into a variety of configurations. The framework is designed to fit seamlessly and scale with an accompanying infrastructure. To achieve this, the design incorporates: Problem and Modular Decomposition utilizing Object Oriented Programming Principles. Isolation of Operations and Parallel Execution utilizing Functional Programming Principles. High Performance utilizing Performance Optimized Python Structures and Libraries. High Reliability and Accuracy using Test Driven Development Methodology. Audience This framework is ideal for any organization planning to do: Data extraction from their repository of documents into an RDBMS system for CART analysis, linear/logistic regressions, or generating word vectors for natural language deep learning (DeepNLP). Generating machine learning ready datan from their repository of images for computer vision. License The source code is made available under the Apache 2.0 license: Apache 2.0 Prerequites The Gap framework extensively uses a number of open source applications/modules. The following applications and modules will be downloaded and installed on your computer/laptop, when the package is installed. Artifex's Ghostscript - extracting text from text PDF ImageMagic's Magick - extracting image from scanned PDF Google's Tesseract - OCR of scanned/image captured text NLTK (Natural Language Toolkit) - stemming/lemmatizer/parts of speech annotation unidecode - romanization of latin character codes numpy - high performance in-memory arrays (tensors) HDF5 - high performance of on-disk data (tensors) access openCV - image manipulation and processing for computer vision imutils - image manipulation for computer vision pyaspeller - spelling dictionary for text Installation: The Gap framework is supported on Windows, MacOS, and Linux. It has been packaged for distribution via PyPi on launch. \\ For pre-launch, after you have clone the source code, from the root of the source tree do the following to complete the install: python setup.py install PyPi Dependencies The dependencies for python packages distributed at PyPi are automatically checked for and installed by the setup.py script. These include: nltk : http://www.nltk.org/ numpy : http://www.numpy.org/ h5py : https://www.h5py.org/ unidecode : https://pypi.org/project/Unidecode/ openCV : https://www.opencv.org/ imultils : https://github.com/jrosebr1/imutils pyaspeller : https://pypi.org/project/pyaspeller/ 3rd Party Dependencies The dependencies for non-python packages are automatically checked for and installed by the setup.py script. These include: Ghostscript : https://www.ghostscript.com Tesseract : https://github.com/tesseract-ocr/tesseract/wiki Imagik : https://www.imagemagick.org Modules The framework provides the following pipeline of modules to support your data and knowledge extraction from both digital and scanned PDF documents, TIFF facsimiles and image captured documents. SPLITTER The splitter module is the NLP entry point into the pipeline. It consists of a Document and Page class. The Document class handles the splitting of PDF documents into PDF pages, TIFF facsimiles into TIFF pages, OCR and raw text extraction. PDF splitting and image extraction is handled by the open source Artifex\u2019s Ghostscript \u00a9, and TIFF splitting by open source Image Magic\u2019s Magick \u00a9. OCR is handled by the open source Google\u2019s Tesseract \u00a9. The Document object stores the individual PDF/TIFF/image pages and corresponding raw text and optionally page images (when scanned PDF, TIFF or images) in the specified storage path. The splitting process can be done synchronously or asynchronously, where in the latter case an event handler signals when the splitting/OCR has been completed and the page table is accessible. For OCR, the resolution of the image extraction is settable, which will affect the quality of the OCR, and corresponding processing time. If the resolution of the original scanned page is lower than the setting, it will be up-sampled, and conversely if it is higher it will be down-sampled. The Page class handles access to the individual pages, via the page table of the document class. Access is provided to the individual PDF, TIFF or image page, the scanned image (when scanned PDF, TIFF or images), raw text and the Natural Language Processing (NLP) processed tokens (when SYNTAX module is installed). NLP processing of the raw text is deferred until first access (JIT), and then preserved in memory as long as the corresponding page object is referenced. The NLP processed tokens may be further segmented into regions, consisting of tables, paragraphs, columns, etc. when the SEGMENTATION module is installed. The document and corresponding pages may be classified (i.e., category of the content) when the CLASSIFICATION module is installed. SYNTAX The syntax module follows the splitter module in the pipeline. It consists of the Words and Vocabulary classes. The Words class handles natural language processing (NLP) of the extracted text. The NLP processing can be configured for tokenization, stemming, lemmatizing, stop word removal, syntax analysis and word classification, with Unicode support. The word classifier recognizes: Syntax Units: Articles, Demonstratives, Prepositions, Pronouns, Conjunctions, Quantifiers, Questions Abbreviations Acronyms Gender (inclusive of Transgender) Date of Birth USA and Canadian Addresses USA and Canadian Telephone Numbers USA Social Security numbers USA and ISO Standard for Dates USA and ISO Standard for Numbers and units of measure. Geographic Locations Sentiment Dates, numbers and units of measure can be converted to either USA Standard or ISO Standard. USA and Canadian postal addresses are converted to the USPO standard for address matching. Along with the builtin stemmer and lemmatizer, the module can optionally be configured to use the NLTK (open source) stemmers, lemmatizer and parts of speech annotations. SEGMENTATION The segmentation module was introduced as part of the pre-launch of Gap v0.9. It currently is in the demonstration stage, and not ready for commericial-product code ready. The segmentation module examines the whitespace layout of the text to identify 'human' layout of text and corresponding context, such as paragraphs, headings, columns, page numbering, letterhead, etc. The text is then separated into segments based on recognized layout and within the segments the text is NLP preprocessed. In this mode, the NLP preprocessed text is hierarchical. At the top level are the segments, with corresponding segment tag, and the child is the NLP preprocessed text within the segment. VISION The splitter module is the CV entry point into the pipeline. It consists of a Images and Image class. The Images class handles the storage and (random access) batch retrieval of CV machine learning ready data, using open source openCV image processing, numpy high performance arrays (tensors) and HDF5 high performance disk (tensor) access. The Image class handles preprocessing of individual images into CV machine learning ready data. The batch and image preprocessing can be done synchronously or asynchronously, where in the latter case an event handler signals when the preprocessing of an image or batch has been completed and the machine learning ready data is accessible. The vision module handles: Mixed image size, format, resolution, number of channels Decompression, Resizing, Normalizing, Flattening User's Guide The User's (Programming) Quick Start Guide can be found here Releases -- describe here Testing The GAP framework is developed using Test Driven Development methodology. The automated unit tests for the framework use pytest, which is a xUnit style form of testing (e.g., jUnit, nUnit, jsUnit, etc). Installation and Documentation The pytest application can be installed using pip: pip install pytest Online documentation for pytest Execution The following are the pre-built automated unit tests, which are located under the subdirectory tests: document_test.py # Tests the Document Class in the Splitter Module page_test.py # Tests the Page Class in the Splitter Module words_test.py # Tests the Words and Addresses Class in the Syntax Module segment_test.py # Tests the Segment Class in the Segment Module image_test.py # Tests the Image and Images Class in the Vision Module The automated tests are executed as follows: pytest -v document_test.py pytest -v page_test.py pytest -v words_test.py pytest -v segment_test.py pytest -v image_test.py Code Coverage Information on the percent of code that is covered (and what source lines not covered) by the automated tests is obtained using pytest-cov. This version of pytest is installed using pip: pip install pytest-cov Testing with code coverage is executed as follows: pytest --cov=splitter document_test.py page_test.py Statements=456, Missed=60, Percent Covered: 87% pytest --cov=syntax words_test.py Statements=456, Missed=60, Percent Covered: 93% pytest --cov=address words_test.py Statements=456, Missed=60, Percent Covered: 90% pytest --cov=vision image_test.py Statements=456, Missed=60, Percent Covered: 89%","title":"Home"},{"location":"#gap-nlpcv-data-engineering-framework-v092-pre-launch-alpha","text":"","title":"Gap : NLP/CV Data Engineering Framework, v0.9.2 (Pre-launch: alpha)"},{"location":"#natural-language-processing-for-pdf-tiff-and-camera-captured-documents-and","text":"","title":"Natural Language Processing for PDF, TIFF, and Camera Captured Documents, and"},{"location":"#computer-vision-processing-for-images","text":"","title":"Computer Vision Processing for Images"},{"location":"#framework","text":"The Gap NLP/CV data engineering framework provides an easy to get started into the world of machine learning for your unstructured data in PDF documents, scanned documents, TIFF facsimiles and camera captured documents, and your image data in image files and image repositories. NLP Automatic OCR of scanned PDF and camera captured images. Automatic Text Extraction from documents. Automatic Syntax Analysis. Optional Romanization of Latin-1 diacritic characters. Optional Spell Correction. Programmatic control for data extraction or redaction (de-identification). Names, Addresses, Proper Places Social Security Numbers, Data of Birth, Gender, Age Telephone Numbers Numerical Information (e.g., medical, financial, \u2026) and units of measurement. Unit conversion from US Standard to Metric, and vice-versa Unicode character recognition Machine Training of Document and Page Classification. Asynchronous processing of documents. Automatic generation of NLP machine learning ready data. CV Automatic storage and retrieval with high performance HDF5 files. Automatic handling of mixed channels (grayscale, RGB and RGBA) and pixel size. Programmatic control of resizing. Programmatic control of conversion into machine learning ready data format: decompression, normalize, flatten. Programmatic control of minibatch generation. Programmatic control of image augmentation. Asynchronous processing of images. Automatic generation of CV machine learning ready data. The framework consists of a sequence of Python modules which can be retrofitted into a variety of configurations. The framework is designed to fit seamlessly and scale with an accompanying infrastructure. To achieve this, the design incorporates: Problem and Modular Decomposition utilizing Object Oriented Programming Principles. Isolation of Operations and Parallel Execution utilizing Functional Programming Principles. High Performance utilizing Performance Optimized Python Structures and Libraries. High Reliability and Accuracy using Test Driven Development Methodology.","title":"Framework"},{"location":"#audience","text":"This framework is ideal for any organization planning to do: Data extraction from their repository of documents into an RDBMS system for CART analysis, linear/logistic regressions, or generating word vectors for natural language deep learning (DeepNLP). Generating machine learning ready datan from their repository of images for computer vision.","title":"Audience"},{"location":"#license","text":"The source code is made available under the Apache 2.0 license: Apache 2.0","title":"License"},{"location":"#prerequites","text":"The Gap framework extensively uses a number of open source applications/modules. The following applications and modules will be downloaded and installed on your computer/laptop, when the package is installed. Artifex's Ghostscript - extracting text from text PDF ImageMagic's Magick - extracting image from scanned PDF Google's Tesseract - OCR of scanned/image captured text NLTK (Natural Language Toolkit) - stemming/lemmatizer/parts of speech annotation unidecode - romanization of latin character codes numpy - high performance in-memory arrays (tensors) HDF5 - high performance of on-disk data (tensors) access openCV - image manipulation and processing for computer vision imutils - image manipulation for computer vision pyaspeller - spelling dictionary for text","title":"Prerequites"},{"location":"#installation","text":"The Gap framework is supported on Windows, MacOS, and Linux. It has been packaged for distribution via PyPi on launch. \\ For pre-launch, after you have clone the source code, from the root of the source tree do the following to complete the install: python setup.py install","title":"Installation:"},{"location":"#pypi-dependencies","text":"The dependencies for python packages distributed at PyPi are automatically checked for and installed by the setup.py script. These include: nltk : http://www.nltk.org/ numpy : http://www.numpy.org/ h5py : https://www.h5py.org/ unidecode : https://pypi.org/project/Unidecode/ openCV : https://www.opencv.org/ imultils : https://github.com/jrosebr1/imutils pyaspeller : https://pypi.org/project/pyaspeller/","title":"PyPi Dependencies"},{"location":"#3rd-party-dependencies","text":"The dependencies for non-python packages are automatically checked for and installed by the setup.py script. These include: Ghostscript : https://www.ghostscript.com Tesseract : https://github.com/tesseract-ocr/tesseract/wiki Imagik : https://www.imagemagick.org","title":"3rd Party Dependencies"},{"location":"#modules","text":"The framework provides the following pipeline of modules to support your data and knowledge extraction from both digital and scanned PDF documents, TIFF facsimiles and image captured documents.","title":"Modules"},{"location":"#splitter","text":"The splitter module is the NLP entry point into the pipeline. It consists of a Document and Page class. The Document class handles the splitting of PDF documents into PDF pages, TIFF facsimiles into TIFF pages, OCR and raw text extraction. PDF splitting and image extraction is handled by the open source Artifex\u2019s Ghostscript \u00a9, and TIFF splitting by open source Image Magic\u2019s Magick \u00a9. OCR is handled by the open source Google\u2019s Tesseract \u00a9. The Document object stores the individual PDF/TIFF/image pages and corresponding raw text and optionally page images (when scanned PDF, TIFF or images) in the specified storage path. The splitting process can be done synchronously or asynchronously, where in the latter case an event handler signals when the splitting/OCR has been completed and the page table is accessible. For OCR, the resolution of the image extraction is settable, which will affect the quality of the OCR, and corresponding processing time. If the resolution of the original scanned page is lower than the setting, it will be up-sampled, and conversely if it is higher it will be down-sampled. The Page class handles access to the individual pages, via the page table of the document class. Access is provided to the individual PDF, TIFF or image page, the scanned image (when scanned PDF, TIFF or images), raw text and the Natural Language Processing (NLP) processed tokens (when SYNTAX module is installed). NLP processing of the raw text is deferred until first access (JIT), and then preserved in memory as long as the corresponding page object is referenced. The NLP processed tokens may be further segmented into regions, consisting of tables, paragraphs, columns, etc. when the SEGMENTATION module is installed. The document and corresponding pages may be classified (i.e., category of the content) when the CLASSIFICATION module is installed.","title":"SPLITTER"},{"location":"#syntax","text":"The syntax module follows the splitter module in the pipeline. It consists of the Words and Vocabulary classes. The Words class handles natural language processing (NLP) of the extracted text. The NLP processing can be configured for tokenization, stemming, lemmatizing, stop word removal, syntax analysis and word classification, with Unicode support. The word classifier recognizes: Syntax Units: Articles, Demonstratives, Prepositions, Pronouns, Conjunctions, Quantifiers, Questions Abbreviations Acronyms Gender (inclusive of Transgender) Date of Birth USA and Canadian Addresses USA and Canadian Telephone Numbers USA Social Security numbers USA and ISO Standard for Dates USA and ISO Standard for Numbers and units of measure. Geographic Locations Sentiment Dates, numbers and units of measure can be converted to either USA Standard or ISO Standard. USA and Canadian postal addresses are converted to the USPO standard for address matching. Along with the builtin stemmer and lemmatizer, the module can optionally be configured to use the NLTK (open source) stemmers, lemmatizer and parts of speech annotations.","title":"SYNTAX"},{"location":"#segmentation","text":"The segmentation module was introduced as part of the pre-launch of Gap v0.9. It currently is in the demonstration stage, and not ready for commericial-product code ready. The segmentation module examines the whitespace layout of the text to identify 'human' layout of text and corresponding context, such as paragraphs, headings, columns, page numbering, letterhead, etc. The text is then separated into segments based on recognized layout and within the segments the text is NLP preprocessed. In this mode, the NLP preprocessed text is hierarchical. At the top level are the segments, with corresponding segment tag, and the child is the NLP preprocessed text within the segment.","title":"SEGMENTATION"},{"location":"#vision","text":"The splitter module is the CV entry point into the pipeline. It consists of a Images and Image class. The Images class handles the storage and (random access) batch retrieval of CV machine learning ready data, using open source openCV image processing, numpy high performance arrays (tensors) and HDF5 high performance disk (tensor) access. The Image class handles preprocessing of individual images into CV machine learning ready data. The batch and image preprocessing can be done synchronously or asynchronously, where in the latter case an event handler signals when the preprocessing of an image or batch has been completed and the machine learning ready data is accessible. The vision module handles: Mixed image size, format, resolution, number of channels Decompression, Resizing, Normalizing, Flattening","title":"VISION"},{"location":"#users-guide","text":"The User's (Programming) Quick Start Guide can be found here","title":"User's Guide"},{"location":"#releases","text":"-- describe here","title":"Releases"},{"location":"#testing","text":"The GAP framework is developed using Test Driven Development methodology. The automated unit tests for the framework use pytest, which is a xUnit style form of testing (e.g., jUnit, nUnit, jsUnit, etc).","title":"Testing"},{"location":"#installation-and-documentation","text":"The pytest application can be installed using pip: pip install pytest Online documentation for pytest","title":"Installation and Documentation"},{"location":"#execution","text":"The following are the pre-built automated unit tests, which are located under the subdirectory tests: document_test.py # Tests the Document Class in the Splitter Module page_test.py # Tests the Page Class in the Splitter Module words_test.py # Tests the Words and Addresses Class in the Syntax Module segment_test.py # Tests the Segment Class in the Segment Module image_test.py # Tests the Image and Images Class in the Vision Module The automated tests are executed as follows: pytest -v document_test.py pytest -v page_test.py pytest -v words_test.py pytest -v segment_test.py pytest -v image_test.py","title":"Execution"},{"location":"#code-coverage","text":"Information on the percent of code that is covered (and what source lines not covered) by the automated tests is obtained using pytest-cov. This version of pytest is installed using pip: pip install pytest-cov Testing with code coverage is executed as follows: pytest --cov=splitter document_test.py page_test.py Statements=456, Missed=60, Percent Covered: 87% pytest --cov=syntax words_test.py Statements=456, Missed=60, Percent Covered: 93% pytest --cov=address words_test.py Statements=456, Missed=60, Percent Covered: 90% pytest --cov=vision image_test.py Statements=456, Missed=60, Percent Covered: 89%","title":"Code Coverage"},{"location":"about/","text":"Gap Natural Language Processing for PDF/TIFF/Image Documents and Computer Vision for Images Framework The Gap NLP/CV open source framework provides an easy to get started into the world of machine learning for your unstructured data in PDF documents, scanned documents, TIFF facsimiles, camera captured documents, and computer vision for your image data. Automatic OCR of scanned and camera captured images. Automatic Text Extraction from documents. Automatic Syntax Analysis. Programmatic control for data extraction or redaction (de-identification) Names, Addresses, Proper Places Social Security Numbers, Data of Birth, Gender Telephone Numbers Numerical Information (e.g., medical, financial, \u2026) and units of measurement. Unit conversion from US Standard to Metric, and vice-versa Unicode character recognition Machine Training of Document and Page Classification. Automatic image preparation (resizing, sampling) and storage (HD5) for convolutional neural networks. The framework consists of a sequence of Python modules which can be retrofitted into a variety of configurations. The framework is designed to fit seamlessly and scale with an accompanying infrastructure. To achieve this, the design incorporates: Problem and Modular Decomposition utilizing Object Oriented Programming Principles. Isolation of Operations and Parallel Execution utilizing Functional Programming Principles. High Performance utilizing Performance Optimized Python Structures and Libraries. High Reliability and Accuracy using Test Driven Development Methodology. The framework provides the following pipeline of modules to support your data and knowledge extraction from both digital and scanned PDF documents, TIFF facsimiles and image captured documents, and for preparing and storing image data for computer vision. This framework is ideal for any organization planning to do data extraction from their repository of documents into an RDBMS system for CART analysis or generating word vectors for natural language deep learning (DeepNLP), and/or computer vision with convolutional neural networks (CNN). SPLITTER The splitter module is the entry point into the pipeline. It consists of a Document and Page class. The Document class handles the splitting of PDF documents into PDF pages, TIFF facsimiles into TIFF pages, OCR and raw text extraction. PDF splitting and image extraction is handled by the open source Artifex\u2019s Ghostscript \u00a9, and TIFF splitting by open source Image Magic\u2019s Magick \u00a9. OCR is handled by the open source Google\u2019s Tesseract \u00a9. The Document object stores the individual PDF/TIFF/image pages and corresponding raw text and optionally page images (when scanned PDF, TIFF or images) in the specified storage path. The splitting process can be done synchronously or asynchronously, where in the latter case an event handler signals when the splitting/OCR has been completed and the page table is accessible. For OCR, the resolution of the image extraction is settable, which will affect the quality of the OCR, and corresponding processing time. If the resolution of the original scanned page is lower than the setting, it will be up-sampled, and conversely if it is higher it will be down-sampled. The Page class handles access to the individual pages, via the page table of the document class. Access is provided to the individual PDF, TIFF or image page, the scanned image (when scanned PDF, TIFF or images), raw text and the Natural Language Processing (NLP) processed tokens (when SYNTAX module is installed). NLP processing of the raw text is deferred until first access (JIT), and then preserved in memory as long as the corresponding page object is referenced. The NLP processed tokens may be further segmented into regions, consisting of tables, paragraphs, columns, etc. when the SEGMENTATION module is installed. The document and corresponding pages may be classified (i.e., category of the content) when the CLASSIFICATION module is installed. SYNTAX The syntax module follows the splitter module in the pipeline. It consists of the Words and Vocabulary classes. The Words class handles natural language processing (NLP) of the extracted text. The NLP processing can be configured for tokenization, stemming, lemmatizing, stop word removal, syntax analysis and word classification, with Unicode support. The word classifier recognizes: Syntax Units: Articles, Demonstratives, Prepositions, Pronouns, Conjunctions, Quantifiers, Questions Abbreviations Acronyms Gender (inclusive of Transgender) Date of Birth USA and Canadian Addresses USA and Canadian Telephone Numbers USA Social Security numbers USA and ISO Standard for Dates USA and ISO Standard for Numbers and units of measure. Geographic Locations Sentiment Dates, numbers and units of measure can be converted to either USA Standard or ISO Standard. USA and Canadian postal addresses are converted to the USPO standard for address matching. Along with the builtin stemmer and lemmatizer, the module can optionally be configured to use the NLTK (open source) stemmers, lemmatizer and parts of speech annotations. VISION The vision module provides preprocessing and storage of images into machine learning ready data. The module supports a wide variety of formats: JPG, PNG, BMP, and TIF, and number of channels (grayscale, RGB, RGBA). Images can be processed incrementally, or in batches. Preprocessing options include conversion to grayscale, resizing, normalizing and flattening. The machine ready image data is stored and retrievable from high performance HD5 file. The HD5 storage provides fast and random access to the machine ready image data and corresponding labels. Preprocessing can be done either synchronously or asynchronously, where in the latter case an event handler signals when the preprocessing has been completed and the machine ready datta is accessible. Further disclosure requires an Non-Disclosure Agreement. \u2003 MODULES Proprietary and Confidential Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"About"},{"location":"about/#gap","text":"","title":"Gap"},{"location":"about/#natural-language-processing-for-pdftiffimage-documents-and-computer-vision-for-images","text":"","title":"Natural Language Processing for PDF/TIFF/Image Documents and Computer Vision for Images"},{"location":"about/#framework","text":"The Gap NLP/CV open source framework provides an easy to get started into the world of machine learning for your unstructured data in PDF documents, scanned documents, TIFF facsimiles, camera captured documents, and computer vision for your image data. Automatic OCR of scanned and camera captured images. Automatic Text Extraction from documents. Automatic Syntax Analysis. Programmatic control for data extraction or redaction (de-identification) Names, Addresses, Proper Places Social Security Numbers, Data of Birth, Gender Telephone Numbers Numerical Information (e.g., medical, financial, \u2026) and units of measurement. Unit conversion from US Standard to Metric, and vice-versa Unicode character recognition Machine Training of Document and Page Classification. Automatic image preparation (resizing, sampling) and storage (HD5) for convolutional neural networks. The framework consists of a sequence of Python modules which can be retrofitted into a variety of configurations. The framework is designed to fit seamlessly and scale with an accompanying infrastructure. To achieve this, the design incorporates: Problem and Modular Decomposition utilizing Object Oriented Programming Principles. Isolation of Operations and Parallel Execution utilizing Functional Programming Principles. High Performance utilizing Performance Optimized Python Structures and Libraries. High Reliability and Accuracy using Test Driven Development Methodology. The framework provides the following pipeline of modules to support your data and knowledge extraction from both digital and scanned PDF documents, TIFF facsimiles and image captured documents, and for preparing and storing image data for computer vision. This framework is ideal for any organization planning to do data extraction from their repository of documents into an RDBMS system for CART analysis or generating word vectors for natural language deep learning (DeepNLP), and/or computer vision with convolutional neural networks (CNN).","title":"Framework"},{"location":"about/#splitter","text":"The splitter module is the entry point into the pipeline. It consists of a Document and Page class. The Document class handles the splitting of PDF documents into PDF pages, TIFF facsimiles into TIFF pages, OCR and raw text extraction. PDF splitting and image extraction is handled by the open source Artifex\u2019s Ghostscript \u00a9, and TIFF splitting by open source Image Magic\u2019s Magick \u00a9. OCR is handled by the open source Google\u2019s Tesseract \u00a9. The Document object stores the individual PDF/TIFF/image pages and corresponding raw text and optionally page images (when scanned PDF, TIFF or images) in the specified storage path. The splitting process can be done synchronously or asynchronously, where in the latter case an event handler signals when the splitting/OCR has been completed and the page table is accessible. For OCR, the resolution of the image extraction is settable, which will affect the quality of the OCR, and corresponding processing time. If the resolution of the original scanned page is lower than the setting, it will be up-sampled, and conversely if it is higher it will be down-sampled. The Page class handles access to the individual pages, via the page table of the document class. Access is provided to the individual PDF, TIFF or image page, the scanned image (when scanned PDF, TIFF or images), raw text and the Natural Language Processing (NLP) processed tokens (when SYNTAX module is installed). NLP processing of the raw text is deferred until first access (JIT), and then preserved in memory as long as the corresponding page object is referenced. The NLP processed tokens may be further segmented into regions, consisting of tables, paragraphs, columns, etc. when the SEGMENTATION module is installed. The document and corresponding pages may be classified (i.e., category of the content) when the CLASSIFICATION module is installed.","title":"SPLITTER"},{"location":"about/#syntax","text":"The syntax module follows the splitter module in the pipeline. It consists of the Words and Vocabulary classes. The Words class handles natural language processing (NLP) of the extracted text. The NLP processing can be configured for tokenization, stemming, lemmatizing, stop word removal, syntax analysis and word classification, with Unicode support. The word classifier recognizes: Syntax Units: Articles, Demonstratives, Prepositions, Pronouns, Conjunctions, Quantifiers, Questions Abbreviations Acronyms Gender (inclusive of Transgender) Date of Birth USA and Canadian Addresses USA and Canadian Telephone Numbers USA Social Security numbers USA and ISO Standard for Dates USA and ISO Standard for Numbers and units of measure. Geographic Locations Sentiment Dates, numbers and units of measure can be converted to either USA Standard or ISO Standard. USA and Canadian postal addresses are converted to the USPO standard for address matching. Along with the builtin stemmer and lemmatizer, the module can optionally be configured to use the NLTK (open source) stemmers, lemmatizer and parts of speech annotations.","title":"SYNTAX"},{"location":"about/#vision","text":"The vision module provides preprocessing and storage of images into machine learning ready data. The module supports a wide variety of formats: JPG, PNG, BMP, and TIF, and number of channels (grayscale, RGB, RGBA). Images can be processed incrementally, or in batches. Preprocessing options include conversion to grayscale, resizing, normalizing and flattening. The machine ready image data is stored and retrievable from high performance HD5 file. The HD5 storage provides fast and random access to the machine ready image data and corresponding labels. Preprocessing can be done either synchronously or asynchronously, where in the latter case an event handler signals when the preprocessing has been completed and the machine ready datta is accessible. Further disclosure requires an Non-Disclosure Agreement.","title":"VISION"},{"location":"about/#modules","text":"Proprietary and Confidential Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"MODULES"},{"location":"quick-start-guide/","text":"Natural Language Processing for PDF/TIFF/Image Documents - Computer Vision for Image Data Users Guide High Precision Natural Language Processing for PDF/TIFF/Image Documents and Computer Vision for Images Users Guide, Gap v0.9.2 1 Introduction The target audience for this users guide are your software developers whom will be integrating the core inner block into your product and/or service. It is not meant to be a complete reference guide or comprehensive tutorial, but a brief get started guide. To utilize this module, the Gap framework will automatically install: 1. This Python module. 2. Python 3.6 or latter 3. Ghostscript \u00a9(open source from Artifex). [will auto-install with pip install on Linux/Mac]. 4. Tesseract \u00a9(open source from Google). [will auto-install with pip install on Linux/Mac]. 5. Magick \u00a9(open source from Image Magic). [will auto-install with pip install on Linux/Mac]. 6. NLTK Toolkit (open source) [will auto-install with pip install]. 7. Unidecode (open source) [will auto-install with pip install]. 8. HD5 (open source) [will auto-install with pip install]. 9. Numpy (open source) [will auto-install with pip install]. 10. OpenCV (open source) [will auto-install with pip install]. 11. Imutils (open source) [will auto-install with pip install]. 12. Pyaspeller (open source) [will auto-install with pip install]. 2 SPLITTER Module 2.1 Document Loading To load a PDF document, TIFF facsimile or image captured document you create a Document (class) object, passing as parameters the path to the PDF/TIFF/image document and a path for storing the split pages/text. Below is a code example. from splitter import Document, Page document = Document(\"yourdocument.pdf\", \"storage_path\") 2.2 Page Splitting Upon instantiating a document object, the corresponding PDF document or TIFF facsimile is automatically split into the corresponding PDF or TIFF pages, utilizing Ghostscript (PDF) and Magick (TIFF). Each PDF/TIFF page will be stored separately in the storage path with the following naming convention: <document basename><pageno>.<suffix> , where <suffix> is either pdf or tif The module automatically detects if a PDF document is a digital (text) or scanned PDF (image). For digital documents, the text is extracted directly from the PDF page using Ghostscript and stored separately in the storage path with the following naming convention: <document basename><pageno>.txt 2.3 OCR If the document is a scanned PDF, each page image will be extracted using Ghostscript, then OCR using Tesseract to extract the text content from the page image. The page image and corresponding page text are stored separately in the storage path with the following naming convention: <document basename><pageno>.png <document basename><pageno>.txt If the document is a TIFF facsimile, each page image will be extracted using Magick, then OCR using Tesseract to extract the text content from the page image. The page image and corresponding page text are stored separately in the storage path with the following naming convention: <document basename><pageno>.tif <document basename><pageno>.txt If the document is an image capture (e.g., JPG), the image is OCR using Tesseract to extract the text content from the page image. The page image and corresponding page text are stored separately in the storage path with the following naming convention: <document basename><pageno>.<suffix> , where <suffix> is png or jpg <document basename><pageno>.txt 2.4 Image Resolution for OCR The resolution of the image rendered by Ghostscript from a scanned PDF page will affect the OCR quality and processing time. By default the resolution is set to 300. The resolution can be set for a (or all) documents with the static member RESOLUTION of the Document class. This property only affects the rendering of scanned PDF; it does not affect TIFF facsimile or image capture. # Set the Resolution of Image Extraction of all scanned PDF pages Document.RESOLUTION = 150 # Image Extraction and OCR will be done at 150 dpi for all subsequent documents document = Document(\"scanneddocument.pdf\", \"storage_path\") 2.5 Page Access Each page is represented by a Page (class) object. Access to the page object is obtained from the pages property member of the Document object. The number of pages in the document is returned by the len() builtin operator for the Document class. document = Document(\"yourdocument.pdf\", \"storage_path\") # Get the number of pages in the PDF document npages = len(document) # Get the page table pages = document.pages # Get the first page page1 = pages[0] # or alternately page1 = document[0] # full path location of the PDF/TIFF or image capture page in storage page1_path = page1.path 2.6 Adding Pages Additional pages can be added to the end of an existing Document object using the += (overridden) operator, where the new page will be fully processed. document = Document(\"1page.pdf\") # This will print 1 for 1 page print(len(document)) # Create a Page object for an existing PDF page new_page = Page(\"page_to_add.pdf\") # Add the page to the end of the document. document += new_page # This will print 2 showing now that it is a 2 page document. print(len(document)) 2.7 Text Extraction The raw text for the page is obtained by the text property of the page class. The byte size of the raw text is obtained from the size() method of the page class. # Get the page table pages = document.pages # Get the first page page1 = pages[0] # Get the total byte size of the raw text bytes = page1.size() # Get the raw text for the page text = page1.text The property scanned is set to True if the text was extracted using OCR; otherwise it is false (i.e., origin was digital text). The property additionally returns a second value which is the estimated quality of the scan as a percentage (between 0 and 1). # Determine if text extraction was obtained by OCR scanned, quality = document.scanned 2.8 Asynchronous Processing To enhance concurrent execution between a main thread and worker activities, the Document class supports asynchronous processing of the document (i.e., Page Splitting, OCR and Text Extraction). Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Document object. Upon completion of the processing, the ehandler is called, where the Document object is passed as a parameter. def done(d): \"\"\" Event Handler for when processing of document is completed \"\"\" print(\"DONE\", d.document) # Process the document asynchronously document = Document(\"yourdocument.pdf\", \"storage_path\", ehandler=done) 2.9 NLP Preprocessing of the Text NLP preprocessing of the text requires the SYNTAX module. The processing of the raw text into NLP sequenced tokens (syntax) is deferred and is executed in a JIT (Just in Time) principle. If installed, the NLP sequenced tokens are access through the words property of the Page class. The first time the property is accessed for a page, the raw text is preprocessed, and then retained in memory for subsequent access. # Get the page table pages = document.pages # Get the first page page1 = pages[0] # Get the NLP preprocessed text words = page1.words The NLP preprocessed text is stored separately in the storage path with the following naming convention: <document basename><pageno>.json 2.10 NLP Preprocessing Settings (Config) NLP Preprocessing of the text may be configured for several settings when instantiating a Document object with the optional config parameter, which consists of a list of one or more predefined options. document = Document(\"yourdocument.pdf\", \"storage_path\", config=[options]) # options: bare # do bare tokenization stem = internal | # use builtin stemmer porter | # use NLTK Porter stemmer snowball | # use NLTK Snowball stemmer lancaster | # use NLTK Lancaster stemmer lemma | # use NLTK WordNet lemmatizer nostem # no stemming pos # Tag each word with NLTK parts of speech roman # Romanize latin-1 character encodings into ASCII 2.11 Document Reloading Once a Document object has been stored, it can later be retrieved from storage, reconstructing the Page and corresponding Words objects. A document object is first instantiated, and then the load() method is called specifying the document name and corresponding storage path. The document name and storage path are used to identify and locate the corresponding stored pages. # Instantiate a Document object document = Document() # Reload the document's pages from storage document.load( \"mydoc.pdf\", \"mystorage\" ) This will reload pages whose filenames in the storage match the sequence: mystorage/mydoc1.json mystorage/mydoc2.json ... 2.13 Word Frequency Distributions The distribution of word occurrences and percentage in a document and individual pages are obtained using the properties: bagOfWords , freqDist , and termFreq . The bagOfWords property returns an unordered dictionary of each unique word in the document (or page) as a key, and the number of occurrences as the value. # Get the bag of words for the document bow = document.bagOfWords print(bow) will output: { '<word>': <no. of occurrences>, '<word>': <no. of occurrences>, \u2026 } e.g., { 'plan': 20, 'medical': 31, 'doctor': 2, \u2026 } # Get the bag of words for each page in the document for page in document.pages: bow = page.bagOfWords The freqDist property returns a sorted list of each unique word in the document (or page), as a tuple of the word and number of occurrences, sorted by the number of occurrences in descending order. # Get the word frequency (count) distribution for the document count = document.freqDist print(count) will output: [ ('<word>', <no. of occurrences>), ('<word>': <no. of occurrences>), \u2026 ] e.g., [ ('medical', 31), ('plan', 20), \u2026, ('doctor', 2), \u2026 ] # Get the word frequency distribution for each page in the document for page in document.pages: count = page.freqDist The termFreq property returns a sorted list of each unique word in the document (or page), as a tuple of the word and the percentage it occurs in the document, sorted by the percentage in descending order. # Get the term frequency (TF) distribution for the document tf = document.freqDist print(tf) will output: [ ('<word>', <percent>), ('<word>': <percent>), \u2026 ] e.g., [ ('medical', 0.02), ('plan', 0.015), \u2026 ] 2.14 Document and Page Classification Semantic Classification (e.g., category) of the document and individual pages requires the CLASSIFICATION module. The classification is deferred and is executed in a JIT (Just in Time) principle. If installed, the classification is access through the classification property of the document and page classes, respectively. The first time the property is accessed for a document or page, the NLP sequenced tokens for each page are processed for classification of the content of individual pages and the first page is further processed for the classification of the content of the entire document. # Get the classification for the document document_classification = document.label # Get the classification for each page for page in document.pages: classification = page.label 3 SYNTAX Module 3.1 NLP Processing The Words (class) object does the NLP preprocessing of the extracted (raw) text. If the extracted text is from a Page object (see SPLITTER), the NLP preprocessing occurs the first time the words property of the Page object is accessed. from syntax import Words, Vocabulary # Get the first page in the document page = document.pages[0] # Get the raw text from the page as a string text = page.text # Get the NLP processed words (Words class) object from the page as a list. words = page.words # Print the object type of words => <class 'Document.Words'> type(words) 3.2 Words Properties The Words (class) object has four public properties: text , words , bagOfWords , and freqDist . The text property is used to access the raw text and the words property is used to access the NLP processed tokens from the raw text. # Get the NLP processed words (Words class) object from the page as a list. words = page.words # Get the original (raw) text as a string text = words.text # Get the NLP processed words from the original text as a Python list. words = words.words # Print the object type of words => <class 'list'> type(words) The bagOfWords and freqDist properties are explained later in the guide. 3.3 Vocabulary Dictionary The words property returns a sequenced Python list of words as a dictionary from the Vocabulary class. Each word in the list is of the dictionary format: { 'word' : word, # The stemmed version of the word 'lemma' : word, # The lemma version of the word 'tag' : tag # The word classification } 3.4 Traversing the NLP Processed Words The NLP processed words returned from the words property are sequenced in the same order as the original text. All punctuation is removed, and except for detected Acronyms, all remaining words are lowercased. The sequenced list of words may be a subset of the original words, depending on the stopwords properties and may be stemmed, lemma, or replaced. # Get the NLP processed words from the original text as a Python list. words = words.words # Traverse the sequenced list of NLP processed words for word in words: text = word.word # original or replaced version of the word tag = word.tag # syntactical classification of the word lemma = word.lemma # The lemma version of the word 3.5 Stopwords The properties which determine which words are removed, stemmed, lemmatized, or replaced are set as keyword parameters in the constructor for the Words class. If no keyword parameters are specified, then all stopwords are removed after being stemmed/lemmatized. The list of stopwords is a superset of the Porter list and additionally includes removing additionally syntactical constructs such as numbers, dates, etc. For a complete list, see the reference manual. If the keyword parameter stopwords is set to False, then all word removal is disabled, while stemming/lemmatization/reducing are still enabled, along with the removal of punctuation. Note in the example below, while stopwords is disabled, the word jumping is replaced with its stem jump. # No stopword removal words = Words(\"The lazy brown fox jumped over the fence.\", stopwords=False) # words => \"the\", \"lazy\", \"brown\", \"fox\", \"jump\", \"over\", \"the\", \"fence\" # All stopword removal words = Words(\"The lazy brown fox jumped over the fence.\", stopwords=True) # words => \"lazy\", \"brown\", \"fox\", \"jump\", \"fence\" 3.6 Bare When the keyword parameter bare is True, all stopword removal, stemming/lemmatization/reducing ad punctuation removal are disabled. # Bare Mode words = Words(\"The lazy brown fox jumped over the fence.\", bare=False) # words => \"the\", \"lazy\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"fence\", \".\" 3.7 Numbers When the keyword parameter number is True, text and numeric version of numbers are preserved; otherwise they are removed. Numbers which are text based (e.g., one) are converted to their numeric representation (e.g., one => 1). The tag value for numbers is set to Vocabulary.NUMBER . # keep/replace numbers words = Words(\"one twenty-one 33.7 1/4\", number=True) print(words.words) will output: [ { 'word': '1', tag: Vocabulary.NUMBER }, { 'word': '21', tag: Vocabulary.NUMBER }, { 'word': '33.7', tag: tag: Vocabulary.NUMBER }, { 'word': '0.25', tag: tag: Vocabulary.NUMBER }, ] If a number is followed by a text representation of a multiplier unit (i.e., million), the number and multiplier unit are replaced by the multiplied value. words = Words(\"two million\", number=True) print(words.words) will output: [ { 'word': '2000000', tag: Vocabulary.NUMBER}, ] 3.8 Unit of Measurement When the keyword parameter unit is True, US Standard and Metric units of measurement are preserved; otherwise they are removed. Both US and EU spelling of metric units are recognized (e.g., meter/metre, liter/litre). The tag value for units of measurement is set to Vocabulary.UNIT . # keep/replace unit words = Words(\"10 liters\", number=True, unit=True) print(words.words) will output: [ { 'word': '10', tag: Vocabulary.NUMBER }, { 'word': 'liter', tag: Vocabulary.UNIT }, ] 3.9 Standard vs. Metric When the keyword parameter standard is True, Metric units of measurement are converted to US Standard. When the keyword parameter metric is True, Standard units of measurement are converted to Metric Standard. # keep/replace unit words = Words(\"10 liters\", number=True, unit=True standard=True) print(words.words) will output: [ { 'word': '2.64172', tag: Vocabulary.NUMBER }, { 'word': 'gallon', tag: Vocabulary.UNIT }, ] 3.10 Date When the keyword parameter date is True, USA and ISO standard date representation and text representation of dates are preserved; otherwise they are removed. Dates are converted to the ISO standard and the tag value is set to Vocabulary.DATE . # keep/replace dates words = Words(\"Jan 2, 2017 and 01/02/2017\", date=True) print(words.words) will output: [ { 'word': '2017-01-02', tag: Vocabulary.DATE }, { 'word': '2017-01-02', tag: Vocabulary.DATE }, ] 3.11 Date of Birth When the keyword parameter dob is True, date of births are preserved; otherwise they are removed. Date of births are converted to the ISO standard and the tag value is set to Vocabulary.DOB . # keep/replace dates words = Words(\"Date of Birth: Jan. 2 2017 DOB: 01-02-2017\", dob=True) print(words.words) will output: [ { 'word': '2017-01-02', tag: Vocabulary.DOB }, { 'word': '2017-01-02', tag: Vocabulary.DOB }, ] If date is set to True without date of birth set to True, date of births will be removed while other dates will be preserved. \u2003 3.12 Social Security Number When the keyword parameter ssn is True, USA Social Security numbers are preserved; otherwise they are removed. Social Security numbers are detected from the prefix presence of text sequences indicating a Social Security number will follow, such as SSN, Soc. Sec., Social Security, etc. Social Security numbers are converted to their single 9 digit value and the tag value is set to Vocabulary.SSN . # keep/replace dates words = Words(\"SSN: 12-123-1234 Social Security 12 123 1234\", ssn=True) print(words.words) will output: [ { 'word': '121231234', tag: Vocabulary.SSN }, { 'word': '121231234', tag: Vocabulary.SSN }, ] 3.13 Telephone Number When the keyword parameter telephone is True, USA/CA telephone numbers are preserved; otherwise they are removed. Telephone numbers are detected from the prefix presence of text sequences indicating a telephone number will follow, such Phone:, Mobile Number, etc. Telephone numbers are converted to their single 10 digit value, inclusive of area code, and the tag value is set to one of: Vocabulary.TELEPHONE Vocabulary.TELEPHONE_HOME Vocabulary.TELEPHONE_WORK Vocabulary.TELEPHONE_OFFICE Vocabulary.TELEPHONE_FAX # keep/replace dates words = Words(\"Phone: (360) 123-1234, Office Number: 360-123-1234\", telephone=True) print(words.words) will output: [ { 'word': '3601231234', tag: Vocabulary.TELEPHONE }, { 'word': '3601231234', tag: Vocabulary.TELEPHONE_WORK}, ] 3.14 Address When the keyword parameter address is True, USA/CA street and postal addresses are preserved; otherwise they are removed. Each component in the address is tagged according to the above street/postal address component type, as follows: \u2022 Postal Box (Vocabulary.POB) \u2022 Street Number (Vocabuary.STREET_NUM) \u2022 Street Direction (Vocabuary.STREET_DIR) \u2022 Street Name (Vocabuary.STREET_NAME) \u2022 Street Type (Vocabuary.STREET_TYPE) \u2022 Secondary Address (Vocabuary.STREET_ADDR2) \u2022 City (Vocabulary.CITY) \u2022 State (Vocabulary.STATE) \u2022 Postal (Vocabulary.POSTAL) # keep/replace street addresses words = Words(\"12 S.E. Main Ave, Seattle, WA\", gender=True) print(words.words) will output: [ { 'word': '12', tag: Vocabulary.STREET_NUM }, { 'word': 'southeast', tag: Vocabulary.STREET_DIR }, { 'word': 'main', tag: Vocabulary.STREET_NAME }, { 'word': 'avenue', tag: Vocabulary.STREET_TYPE }, { 'word': 'seattle', tag: Vocabulary.CITY }, { 'word': 'ISO316-2:US-WA', tag: Vocabulary.STATE }, ] 3.15 Gender When the keyword parameter gender is True, words indicating gender are preserved; otherwise they are removed. Transgender is inclusive in the recognition. The tag value is set to one of Vocabulary.MALE , Vocabulary.FEMALE or Vocabulary.TRANSGENDER . # keep/replace gender indicating words words = Words(\"man uncle mother women tg\", gender=True) print(words.words) will output: [ { 'word': 'man', tag: Vocabulary.MALE }, { 'word': 'uncle', tag: Vocabulary.MALE }, { 'word': 'mother', tag: Vocabulary.FEMALE }, { 'word': 'women', tag: Vocabulary.FEMALE }, { 'word': 'transgender', tag: Vocabulary.TRANSGENDER }, ] 3.16 Sentiment When the keyword parameter sentiment is True, word and word phrases indicating sentiment are preserved; otherwise they are removed. Sentiment phrases are reduced to the single primary word indicating the sentiment and the tag value is set to either Vocabulary.POSITIVE or Vocabulary.NEGATIVE . # keep/replace sentiment indicating phrases words = Words(\"the food was not good\", sentiment=True) print(words.words) will output: [ { 'word': 'food', tag: Vocabulary.UNTAG }, { 'word': 'not', tag: Vocabulary.NEGATIVE}, ] 3.17 Spell Checking When the keyword parameter spell is True, each tokenized word is looked up in the pyaspeller word dictionary. If the word is not found (presumed misspelled) and the pyaspeller recommends a safe replacement, the word is replaced with the pyaspeller's safe replacement. The spell check/replacement occurs prior to stemming, lemmatizing, and stopword removal. # add parts of speech tagging words = Words(\"mispelled\", spell=True) print(words.words) will output: [ { 'word': 'misspell', 'tag': Vocabulary.UNTAG}, ] 3.18 Parts of Speech When the keyword parameter pos is True, each tokenized word is further annotated with it's corresponding NLTK parts of speech tag. # add parts of speech tagging words = Words(\"Jim Smith\", pos=True) print(words.words) will output: [ { 'word': 'food', 'tag': Vocabulary.UNTAG, 'pos': NN }, { 'word': 'not', 'tag': Vocabulary.NEGATIVE, 'pos': NN }, ] 3.19 Romanization When the keyword parameter roman is True, the latin-1 character encoding of each tokenized is converted to ASCII. # Romanization of latin-1 character encodings words = Words(\"Qu\u00e9bec\", roman=True) print(words.words) will output: [ { 'word': 'quebec', 'tag': Vocabulary.UNTAG, ] 3.20 Bag of Words and Word Frequency Distribution The property bagsOfWords returns an unordered dictionary of each occurrence of a unique word in the tokenized sequence, where the word is the dictionary key, and the number of occurrences is the corresponding value. # Get the Bag of Words representation words = Words(\"Jack and Jill went up the hill to fetch a pail of water. Jack fell down and broke his crown and Jill came tumbling after.\", stopwords=True) print(words.bagOfWords) will output: { 'pail': 1, 'the': 1, 'a': 1, 'water': 1, 'fetch': 1, 'went': 1, 'and': 2, 'jack': 2, 'jill': 2, 'down': 1, 'come': 1, 'fell': 1, 'up': 1, 'of': 1, 'tumble': 1, 'to': 1, 'hill': 1, 'after': 1 } The property freqDist returns a sorted list of tuples, in descending order, of word frequencies (i.e., the number of occurrences of the word in the tokenized sequence. # Get the Word Frequency Distribution words = Words(\"Jack and Jill went up the hill to fetch a pail of water. Jack fell down and broke his crown and Jill came tumbling after.\", stopwords=True) print(words.freqDist) will output: [ ('jack', 2), ('jill', 2), ('and', 2), ('water', 1), ('the', 1), \u2026. ] 4 VISION Module 4.1 Image Processing To preprocess an image for computer vision machine learning, you create an Image (class) object, passing as parameters the path to the image, the corresponding label and a path for storing the preprocessed image data, the original image and optionally a thumbnail. The label must be specified as an integer value. Below is a code example. from vision import Image image = Image(\"yourimage.jpg\", 101, \"storage_path\") The above will generate the following output files: storage_path/yourimage.h5 # preprocessed image and raw data and optional thumbnail Alternately, the image path may be an URL; in which case, an HTTP request is made to obtain the image data from the remote location. image = Image(\"http://yourimage.jpg\", 101, \"storage_path\") The Image class supports processing of JPEG, PNG, TIF, BMP and GIF images. Images maybe of any pixel size, and number of channels (i.e. Grayscale, RGB and RGBA). Alternately, the input may be raw pixel data as a numpy array. raw = [...], [...], [\u2026] ] image = Image(raw, 101, \"storage_path\") 4.2 Image Processing Settings (Config) CV Preprocessing of the image may be configured for several settings when instantiating an Image object with the optional config parameter, which consists of a list of one or more predefined options. image = Image(\"yourimage.jpg\", 101, \"storage_path\", config=[options]) options: gray | grayscale # convert to grayscale (single channel) normal | normalize # normalize the pixel data for values between 0 .. 1 flat | flatten # flatten the pixel data into a 1D vector resize=(height,width) # resize the image thumb=(height,width) # generate a thumbnail nostore # do not store the preprocessed image, raw and thumbnail data Example image = Image(\"image.jpg\", 101, \"path\", config=['flatten', 'thumb=(16,16)']) # will preprocess the image.jpg into machine learning ready data as a 1D vector, and # store the raw (unprocessed) decompressed data, preprocessed data and 16 x 16 4.4 Get Properties of Preprocessed Image Data After an image has been preprocessed, several properties of the preprocessed image data can be obtained from the Image class properties: name - The root name of the image. type - The image format (e.g., png). shape - The shape of the preprocessed image data (e.g., (100, 100,3) ). data - The preprocessed image data as a numpy array. raw - The unprocessed decompressed image data as a numpy array. size - The byte size of the original image. thumb \u2013 The thumbnail image data as a numpy array. image = Image(\"yourimage.jpg\", \"storage_path\", 101) print(image.shape) Will output something like: (100,100,3) 4.5 Asynchronous Processing To enhance concurrent execution between a main thread and worker activities, the Image class supports asynchronous processing of the image. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Image object. Upon completion of the processing, the ehandler is called, where the Image object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of image is completed \"\"\" print(\"DONE\", i.image) # Process the image asynchronously image = Image(\"yourimage.png\", \"storage_path\", 101, ehandler=done) 4.6 Image Reloading Once an Image object has been stored, it can later be retrieved from storage, reconstructing the Image object. An Image object is first instantiated, and then the load() method is called specifying the image name and corresponding storage path. The image name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Image object image = Image() # Reload the image's data from storage image.load( \"myimage.png\", \"mystorage\" ) 4.7 Image Collection Processing To preprocess a collection of images for computer vision machine learning, you create an Images (class) object, passing as parameters a list of the paths to the images, a list of the corresponding label and a path for storing the collection of preprocessed image data, the original images and optionally thumbnails. Each label must be specified as an integer value. Below is a code example. from images import Images images = Images([\"image1.jpg\", \"image2.jpg\"], labels=[101, 102], name=' c1') The above will generate the following output files: train/c1.h5 # preprocessed image data The Images object will implicitly add the 'nostore' setting to the configuration parameter of each Image object created. This will direct each of the Image objects to not store the corresponding image data in an HD5 file. Instead, upon completion of the preprocessing of the collection of image data, the entire collection of preprocessed data is stored in a single HD5 file. Alternately, the list of image paths parameter may be a list of directories containing images. images = Images([\"subfolder1\", \"subfolder2\"], labels=[101, 102], name=' c1') Alternately, the list of labels parameter may be a single value; in which case the label value applies to all the images. images = Images([\"image1.jpg\", \"image2.jpg\"], labels=101, name=' c1')\u2003 4.8 Image Collection Processing Settings (Config) Configuration settings supported by the Image class may be specified as an optional parameter to the Images object, which are then passed down to each Image object generated for the collection. # Preprocess each image by normalizing the pixel data and then flatten into a 1D vector images = Images([\"image1.jpg\", \"image2.jpg\"], \"train\", labels=[101, 102], config=['normal', 'flatten']) 4.9 Get Properties of a Collection After a collection of images has been preprocessed, several properties of the preprocessed image data can be obtained from the Images class properties: name \u2013 The name of the collection file. time \u2013 The time to preprocess the image. data \u2013 List of Image objects in the collection. len() \u2013 The len() operator will return the number of images in the collection. [] \u2013 The index operator will access the image objects in sequential order. # Access each Image object in the collection for ix in range(len(images)): image = images[ix] 4.10 Splitting a Collection into Training and Test Data Batch, mini-batch and stochastic feed modes are supported. The percentage of data that is test (vs. training) is set by the split property, where the default is 0.2. Optionally, a mini-batch size is set by the minibatch property. Prior to the split, the data is randomized. The split() property when called as a getter will return the training data, training labels, test data, and test labels. # Set 30% of the images in the collection to be test data images.split = 0.3 # Get the entire training and test data and corresponding labels as lists. X_train, X_test, Y_train, Y_test = images.split Alternately, the next() operator will iterate through the image data, and corresponding label, in the training set. # Set 30% of the images in the collection to be test data images.split = 0.3 # Iterate through the training data while ( data, label = next(images) ) is not None: pass Training data can also be fetched in minibatches. The mini batch size is set using the minibatch property. The minibatch property when called as a getter will return a generator. The generator will iterate through each image, and corresponding label, of the generated mini-batch. Successive calls to the minibatch property will iterate through the training data. # Set 30% of the images in the collection to be test data images.split = 0.3 # Train the model in mini-batches of 30 images images.minibatch = 30 # loop for each mini-batch in training data for _ in range(nbatches) # create the generator g = images.minibatch # iterate through the mini-batch for data, label in g: pass The split property when used as a setter may optionally take a seed for initializing the randomized shuffling of the training set. # Set the seed for the random shuffle to 42 images.split = 0.3, 42 4.11 Image Augmentation Image augmentation is supported. By default, images are not augmented. If the property augment is set to True, then for each image generated for feeding (see next() and minibatch) an additional image will be generated. The additional image will be a randomized rotation between -90 and 90 degrees of the corresponding image. For example, if a training set has a 1000 images, then 2000 images will be feed when the property augment is set to True, where 1000 of the images are the original images, and another 1000 are the generated augmented images. images.split = 0.3, 42 # Enable image augmentation images.augment = True # Iterate through the training data, where every other image will be an augmented image while ( data, label = next(images) ) is not None: pass 4.12 Asynchronous Collection Processing To enhance concurrent execution between a main thread and worker activities, the Images class supports asynchronous processing of the collection of images. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Images object. Upon completion of the processing, the ehandler is called, where the Images object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of collection of images is completed \"\"\" print(\"DONE\", i.images) # Process the collection of images asynchronously images = Images([\"img1.png\", \"img2.png\"], \"train\", labels=[0,1], ehandler=done) 4.13 Collection Reloading Once an Images object has been stored, it can later be retrieved from storage, reconstructing the Images object, and corresponding list of Image objects. An Images object is first instantiated, and then the load() method is called specifying the collection name and corresponding storage path. The collection name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Images object images = Images() # Reload the collection of image data from storage images.load( \"mycollection\", \"mystorage\" ) 5 SEGMENTATION Module The segmentation module is newly introduced in Gap v0.9 prelaunch. It is in the early stage, and should be considered experimental, and not for commercial-product-ready yet. The segmentation module analyzes the whitespace layout of the text to identify the 'human' perceived grouping/purpose of text, such as paragraphs, headings, columns, page numbering, letterhead, etc., and the associated context. In this mode, the text is separated into segments, corresponding to identified layout, where each segment is then NLP preprocessed. The resulting NLP output is then hierarchical, where at the top level is the segment identification, and it's child is the NLP preprocessed text. 5.1 Text Segmentation When the config option 'segment' is specified on a Document object, the corresponding text per page is segmented. # import the segmentation module from segment import Segment segment = Segment(\"para 1\\n\\npara 2\") print(segment.segments) will output: [ { 'tag': 1002, words: [ { 'word': 'para', 'tag': 0}, {'word': 1, 'tag': 1}]}, { 'tag': 1002, words: [ { 'word': 'para', 'tag': 0}, {'word': 2, 'tag': 1}]} ] Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"Quick Start Guide"},{"location":"quick-start-guide/#natural-language-processing-for-pdftiffimage-documents-computer-vision-for-image-data","text":"Users Guide High Precision Natural Language Processing for PDF/TIFF/Image Documents and Computer Vision for Images Users Guide, Gap v0.9.2","title":"Natural Language Processing for PDF/TIFF/Image Documents - Computer Vision for Image Data"},{"location":"quick-start-guide/#1-introduction","text":"The target audience for this users guide are your software developers whom will be integrating the core inner block into your product and/or service. It is not meant to be a complete reference guide or comprehensive tutorial, but a brief get started guide. To utilize this module, the Gap framework will automatically install: 1. This Python module. 2. Python 3.6 or latter 3. Ghostscript \u00a9(open source from Artifex). [will auto-install with pip install on Linux/Mac]. 4. Tesseract \u00a9(open source from Google). [will auto-install with pip install on Linux/Mac]. 5. Magick \u00a9(open source from Image Magic). [will auto-install with pip install on Linux/Mac]. 6. NLTK Toolkit (open source) [will auto-install with pip install]. 7. Unidecode (open source) [will auto-install with pip install]. 8. HD5 (open source) [will auto-install with pip install]. 9. Numpy (open source) [will auto-install with pip install]. 10. OpenCV (open source) [will auto-install with pip install]. 11. Imutils (open source) [will auto-install with pip install]. 12. Pyaspeller (open source) [will auto-install with pip install].","title":"1   Introduction"},{"location":"quick-start-guide/#2-splitter-module","text":"","title":"2  SPLITTER Module"},{"location":"quick-start-guide/#21-document-loading","text":"To load a PDF document, TIFF facsimile or image captured document you create a Document (class) object, passing as parameters the path to the PDF/TIFF/image document and a path for storing the split pages/text. Below is a code example. from splitter import Document, Page document = Document(\"yourdocument.pdf\", \"storage_path\")","title":"2.1  Document Loading"},{"location":"quick-start-guide/#22-page-splitting","text":"Upon instantiating a document object, the corresponding PDF document or TIFF facsimile is automatically split into the corresponding PDF or TIFF pages, utilizing Ghostscript (PDF) and Magick (TIFF). Each PDF/TIFF page will be stored separately in the storage path with the following naming convention: <document basename><pageno>.<suffix> , where <suffix> is either pdf or tif The module automatically detects if a PDF document is a digital (text) or scanned PDF (image). For digital documents, the text is extracted directly from the PDF page using Ghostscript and stored separately in the storage path with the following naming convention: <document basename><pageno>.txt","title":"2.2  Page Splitting"},{"location":"quick-start-guide/#23-ocr","text":"If the document is a scanned PDF, each page image will be extracted using Ghostscript, then OCR using Tesseract to extract the text content from the page image. The page image and corresponding page text are stored separately in the storage path with the following naming convention: <document basename><pageno>.png <document basename><pageno>.txt If the document is a TIFF facsimile, each page image will be extracted using Magick, then OCR using Tesseract to extract the text content from the page image. The page image and corresponding page text are stored separately in the storage path with the following naming convention: <document basename><pageno>.tif <document basename><pageno>.txt If the document is an image capture (e.g., JPG), the image is OCR using Tesseract to extract the text content from the page image. The page image and corresponding page text are stored separately in the storage path with the following naming convention: <document basename><pageno>.<suffix> , where <suffix> is png or jpg <document basename><pageno>.txt","title":"2.3  OCR"},{"location":"quick-start-guide/#24-image-resolution-for-ocr","text":"The resolution of the image rendered by Ghostscript from a scanned PDF page will affect the OCR quality and processing time. By default the resolution is set to 300. The resolution can be set for a (or all) documents with the static member RESOLUTION of the Document class. This property only affects the rendering of scanned PDF; it does not affect TIFF facsimile or image capture. # Set the Resolution of Image Extraction of all scanned PDF pages Document.RESOLUTION = 150 # Image Extraction and OCR will be done at 150 dpi for all subsequent documents document = Document(\"scanneddocument.pdf\", \"storage_path\")","title":"2.4  Image Resolution for OCR"},{"location":"quick-start-guide/#25-page-access","text":"Each page is represented by a Page (class) object. Access to the page object is obtained from the pages property member of the Document object. The number of pages in the document is returned by the len() builtin operator for the Document class. document = Document(\"yourdocument.pdf\", \"storage_path\") # Get the number of pages in the PDF document npages = len(document) # Get the page table pages = document.pages # Get the first page page1 = pages[0] # or alternately page1 = document[0] # full path location of the PDF/TIFF or image capture page in storage page1_path = page1.path","title":"2.5  Page Access"},{"location":"quick-start-guide/#26-adding-pages","text":"Additional pages can be added to the end of an existing Document object using the += (overridden) operator, where the new page will be fully processed. document = Document(\"1page.pdf\") # This will print 1 for 1 page print(len(document)) # Create a Page object for an existing PDF page new_page = Page(\"page_to_add.pdf\") # Add the page to the end of the document. document += new_page # This will print 2 showing now that it is a 2 page document. print(len(document))","title":"2.6  Adding Pages"},{"location":"quick-start-guide/#27-text-extraction","text":"The raw text for the page is obtained by the text property of the page class. The byte size of the raw text is obtained from the size() method of the page class. # Get the page table pages = document.pages # Get the first page page1 = pages[0] # Get the total byte size of the raw text bytes = page1.size() # Get the raw text for the page text = page1.text The property scanned is set to True if the text was extracted using OCR; otherwise it is false (i.e., origin was digital text). The property additionally returns a second value which is the estimated quality of the scan as a percentage (between 0 and 1). # Determine if text extraction was obtained by OCR scanned, quality = document.scanned","title":"2.7  Text Extraction"},{"location":"quick-start-guide/#28-asynchronous-processing","text":"To enhance concurrent execution between a main thread and worker activities, the Document class supports asynchronous processing of the document (i.e., Page Splitting, OCR and Text Extraction). Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Document object. Upon completion of the processing, the ehandler is called, where the Document object is passed as a parameter. def done(d): \"\"\" Event Handler for when processing of document is completed \"\"\" print(\"DONE\", d.document) # Process the document asynchronously document = Document(\"yourdocument.pdf\", \"storage_path\", ehandler=done)","title":"2.8  Asynchronous Processing"},{"location":"quick-start-guide/#29-nlp-preprocessing-of-the-text","text":"NLP preprocessing of the text requires the SYNTAX module. The processing of the raw text into NLP sequenced tokens (syntax) is deferred and is executed in a JIT (Just in Time) principle. If installed, the NLP sequenced tokens are access through the words property of the Page class. The first time the property is accessed for a page, the raw text is preprocessed, and then retained in memory for subsequent access. # Get the page table pages = document.pages # Get the first page page1 = pages[0] # Get the NLP preprocessed text words = page1.words The NLP preprocessed text is stored separately in the storage path with the following naming convention: <document basename><pageno>.json","title":"2.9  NLP Preprocessing of the Text"},{"location":"quick-start-guide/#210-nlp-preprocessing-settings-config","text":"NLP Preprocessing of the text may be configured for several settings when instantiating a Document object with the optional config parameter, which consists of a list of one or more predefined options. document = Document(\"yourdocument.pdf\", \"storage_path\", config=[options]) # options: bare # do bare tokenization stem = internal | # use builtin stemmer porter | # use NLTK Porter stemmer snowball | # use NLTK Snowball stemmer lancaster | # use NLTK Lancaster stemmer lemma | # use NLTK WordNet lemmatizer nostem # no stemming pos # Tag each word with NLTK parts of speech roman # Romanize latin-1 character encodings into ASCII","title":"2.10  NLP Preprocessing Settings (Config)"},{"location":"quick-start-guide/#211-document-reloading","text":"Once a Document object has been stored, it can later be retrieved from storage, reconstructing the Page and corresponding Words objects. A document object is first instantiated, and then the load() method is called specifying the document name and corresponding storage path. The document name and storage path are used to identify and locate the corresponding stored pages. # Instantiate a Document object document = Document() # Reload the document's pages from storage document.load( \"mydoc.pdf\", \"mystorage\" ) This will reload pages whose filenames in the storage match the sequence: mystorage/mydoc1.json mystorage/mydoc2.json ...","title":"2.11  Document Reloading"},{"location":"quick-start-guide/#213-word-frequency-distributions","text":"The distribution of word occurrences and percentage in a document and individual pages are obtained using the properties: bagOfWords , freqDist , and termFreq . The bagOfWords property returns an unordered dictionary of each unique word in the document (or page) as a key, and the number of occurrences as the value. # Get the bag of words for the document bow = document.bagOfWords print(bow) will output: { '<word>': <no. of occurrences>, '<word>': <no. of occurrences>, \u2026 } e.g., { 'plan': 20, 'medical': 31, 'doctor': 2, \u2026 } # Get the bag of words for each page in the document for page in document.pages: bow = page.bagOfWords The freqDist property returns a sorted list of each unique word in the document (or page), as a tuple of the word and number of occurrences, sorted by the number of occurrences in descending order. # Get the word frequency (count) distribution for the document count = document.freqDist print(count) will output: [ ('<word>', <no. of occurrences>), ('<word>': <no. of occurrences>), \u2026 ] e.g., [ ('medical', 31), ('plan', 20), \u2026, ('doctor', 2), \u2026 ] # Get the word frequency distribution for each page in the document for page in document.pages: count = page.freqDist The termFreq property returns a sorted list of each unique word in the document (or page), as a tuple of the word and the percentage it occurs in the document, sorted by the percentage in descending order. # Get the term frequency (TF) distribution for the document tf = document.freqDist print(tf) will output: [ ('<word>', <percent>), ('<word>': <percent>), \u2026 ] e.g., [ ('medical', 0.02), ('plan', 0.015), \u2026 ]","title":"2.13  Word Frequency Distributions"},{"location":"quick-start-guide/#214-document-and-page-classification","text":"Semantic Classification (e.g., category) of the document and individual pages requires the CLASSIFICATION module. The classification is deferred and is executed in a JIT (Just in Time) principle. If installed, the classification is access through the classification property of the document and page classes, respectively. The first time the property is accessed for a document or page, the NLP sequenced tokens for each page are processed for classification of the content of individual pages and the first page is further processed for the classification of the content of the entire document. # Get the classification for the document document_classification = document.label # Get the classification for each page for page in document.pages: classification = page.label","title":"2.14  Document and Page Classification"},{"location":"quick-start-guide/#3-syntax-module","text":"","title":"3  SYNTAX Module"},{"location":"quick-start-guide/#31-nlp-processing","text":"The Words (class) object does the NLP preprocessing of the extracted (raw) text. If the extracted text is from a Page object (see SPLITTER), the NLP preprocessing occurs the first time the words property of the Page object is accessed. from syntax import Words, Vocabulary # Get the first page in the document page = document.pages[0] # Get the raw text from the page as a string text = page.text # Get the NLP processed words (Words class) object from the page as a list. words = page.words # Print the object type of words => <class 'Document.Words'> type(words)","title":"3.1  NLP Processing"},{"location":"quick-start-guide/#32-words-properties","text":"The Words (class) object has four public properties: text , words , bagOfWords , and freqDist . The text property is used to access the raw text and the words property is used to access the NLP processed tokens from the raw text. # Get the NLP processed words (Words class) object from the page as a list. words = page.words # Get the original (raw) text as a string text = words.text # Get the NLP processed words from the original text as a Python list. words = words.words # Print the object type of words => <class 'list'> type(words) The bagOfWords and freqDist properties are explained later in the guide.","title":"3.2  Words Properties"},{"location":"quick-start-guide/#33-vocabulary-dictionary","text":"The words property returns a sequenced Python list of words as a dictionary from the Vocabulary class. Each word in the list is of the dictionary format: { 'word' : word, # The stemmed version of the word 'lemma' : word, # The lemma version of the word 'tag' : tag # The word classification }","title":"3.3  Vocabulary Dictionary"},{"location":"quick-start-guide/#34-traversing-the-nlp-processed-words","text":"The NLP processed words returned from the words property are sequenced in the same order as the original text. All punctuation is removed, and except for detected Acronyms, all remaining words are lowercased. The sequenced list of words may be a subset of the original words, depending on the stopwords properties and may be stemmed, lemma, or replaced. # Get the NLP processed words from the original text as a Python list. words = words.words # Traverse the sequenced list of NLP processed words for word in words: text = word.word # original or replaced version of the word tag = word.tag # syntactical classification of the word lemma = word.lemma # The lemma version of the word","title":"3.4   Traversing the NLP Processed Words"},{"location":"quick-start-guide/#35-stopwords","text":"The properties which determine which words are removed, stemmed, lemmatized, or replaced are set as keyword parameters in the constructor for the Words class. If no keyword parameters are specified, then all stopwords are removed after being stemmed/lemmatized. The list of stopwords is a superset of the Porter list and additionally includes removing additionally syntactical constructs such as numbers, dates, etc. For a complete list, see the reference manual. If the keyword parameter stopwords is set to False, then all word removal is disabled, while stemming/lemmatization/reducing are still enabled, along with the removal of punctuation. Note in the example below, while stopwords is disabled, the word jumping is replaced with its stem jump. # No stopword removal words = Words(\"The lazy brown fox jumped over the fence.\", stopwords=False) # words => \"the\", \"lazy\", \"brown\", \"fox\", \"jump\", \"over\", \"the\", \"fence\" # All stopword removal words = Words(\"The lazy brown fox jumped over the fence.\", stopwords=True) # words => \"lazy\", \"brown\", \"fox\", \"jump\", \"fence\"","title":"3.5  Stopwords"},{"location":"quick-start-guide/#36-bare","text":"When the keyword parameter bare is True, all stopword removal, stemming/lemmatization/reducing ad punctuation removal are disabled. # Bare Mode words = Words(\"The lazy brown fox jumped over the fence.\", bare=False) # words => \"the\", \"lazy\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"fence\", \".\"","title":"3.6  Bare"},{"location":"quick-start-guide/#37-numbers","text":"When the keyword parameter number is True, text and numeric version of numbers are preserved; otherwise they are removed. Numbers which are text based (e.g., one) are converted to their numeric representation (e.g., one => 1). The tag value for numbers is set to Vocabulary.NUMBER . # keep/replace numbers words = Words(\"one twenty-one 33.7 1/4\", number=True) print(words.words) will output: [ { 'word': '1', tag: Vocabulary.NUMBER }, { 'word': '21', tag: Vocabulary.NUMBER }, { 'word': '33.7', tag: tag: Vocabulary.NUMBER }, { 'word': '0.25', tag: tag: Vocabulary.NUMBER }, ] If a number is followed by a text representation of a multiplier unit (i.e., million), the number and multiplier unit are replaced by the multiplied value. words = Words(\"two million\", number=True) print(words.words) will output: [ { 'word': '2000000', tag: Vocabulary.NUMBER}, ]","title":"3.7  Numbers"},{"location":"quick-start-guide/#38-unit-of-measurement","text":"When the keyword parameter unit is True, US Standard and Metric units of measurement are preserved; otherwise they are removed. Both US and EU spelling of metric units are recognized (e.g., meter/metre, liter/litre). The tag value for units of measurement is set to Vocabulary.UNIT . # keep/replace unit words = Words(\"10 liters\", number=True, unit=True) print(words.words) will output: [ { 'word': '10', tag: Vocabulary.NUMBER }, { 'word': 'liter', tag: Vocabulary.UNIT }, ]","title":"3.8  Unit of Measurement"},{"location":"quick-start-guide/#39-standard-vs-metric","text":"When the keyword parameter standard is True, Metric units of measurement are converted to US Standard. When the keyword parameter metric is True, Standard units of measurement are converted to Metric Standard. # keep/replace unit words = Words(\"10 liters\", number=True, unit=True standard=True) print(words.words) will output: [ { 'word': '2.64172', tag: Vocabulary.NUMBER }, { 'word': 'gallon', tag: Vocabulary.UNIT }, ]","title":"3.9  Standard vs. Metric"},{"location":"quick-start-guide/#310-date","text":"When the keyword parameter date is True, USA and ISO standard date representation and text representation of dates are preserved; otherwise they are removed. Dates are converted to the ISO standard and the tag value is set to Vocabulary.DATE . # keep/replace dates words = Words(\"Jan 2, 2017 and 01/02/2017\", date=True) print(words.words) will output: [ { 'word': '2017-01-02', tag: Vocabulary.DATE }, { 'word': '2017-01-02', tag: Vocabulary.DATE }, ]","title":"3.10 Date"},{"location":"quick-start-guide/#311-date-of-birth","text":"When the keyword parameter dob is True, date of births are preserved; otherwise they are removed. Date of births are converted to the ISO standard and the tag value is set to Vocabulary.DOB . # keep/replace dates words = Words(\"Date of Birth: Jan. 2 2017 DOB: 01-02-2017\", dob=True) print(words.words) will output: [ { 'word': '2017-01-02', tag: Vocabulary.DOB }, { 'word': '2017-01-02', tag: Vocabulary.DOB }, ] If date is set to True without date of birth set to True, date of births will be removed while other dates will be preserved.","title":"3.11 Date of Birth"},{"location":"quick-start-guide/#312-social-security-number","text":"When the keyword parameter ssn is True, USA Social Security numbers are preserved; otherwise they are removed. Social Security numbers are detected from the prefix presence of text sequences indicating a Social Security number will follow, such as SSN, Soc. Sec., Social Security, etc. Social Security numbers are converted to their single 9 digit value and the tag value is set to Vocabulary.SSN . # keep/replace dates words = Words(\"SSN: 12-123-1234 Social Security 12 123 1234\", ssn=True) print(words.words) will output: [ { 'word': '121231234', tag: Vocabulary.SSN }, { 'word': '121231234', tag: Vocabulary.SSN }, ]","title":"3.12  Social Security Number"},{"location":"quick-start-guide/#313-telephone-number","text":"When the keyword parameter telephone is True, USA/CA telephone numbers are preserved; otherwise they are removed. Telephone numbers are detected from the prefix presence of text sequences indicating a telephone number will follow, such Phone:, Mobile Number, etc. Telephone numbers are converted to their single 10 digit value, inclusive of area code, and the tag value is set to one of: Vocabulary.TELEPHONE Vocabulary.TELEPHONE_HOME Vocabulary.TELEPHONE_WORK Vocabulary.TELEPHONE_OFFICE Vocabulary.TELEPHONE_FAX # keep/replace dates words = Words(\"Phone: (360) 123-1234, Office Number: 360-123-1234\", telephone=True) print(words.words) will output: [ { 'word': '3601231234', tag: Vocabulary.TELEPHONE }, { 'word': '3601231234', tag: Vocabulary.TELEPHONE_WORK}, ]","title":"3.13  Telephone Number"},{"location":"quick-start-guide/#314-address","text":"When the keyword parameter address is True, USA/CA street and postal addresses are preserved; otherwise they are removed. Each component in the address is tagged according to the above street/postal address component type, as follows: \u2022 Postal Box (Vocabulary.POB) \u2022 Street Number (Vocabuary.STREET_NUM) \u2022 Street Direction (Vocabuary.STREET_DIR) \u2022 Street Name (Vocabuary.STREET_NAME) \u2022 Street Type (Vocabuary.STREET_TYPE) \u2022 Secondary Address (Vocabuary.STREET_ADDR2) \u2022 City (Vocabulary.CITY) \u2022 State (Vocabulary.STATE) \u2022 Postal (Vocabulary.POSTAL) # keep/replace street addresses words = Words(\"12 S.E. Main Ave, Seattle, WA\", gender=True) print(words.words) will output: [ { 'word': '12', tag: Vocabulary.STREET_NUM }, { 'word': 'southeast', tag: Vocabulary.STREET_DIR }, { 'word': 'main', tag: Vocabulary.STREET_NAME }, { 'word': 'avenue', tag: Vocabulary.STREET_TYPE }, { 'word': 'seattle', tag: Vocabulary.CITY }, { 'word': 'ISO316-2:US-WA', tag: Vocabulary.STATE }, ]","title":"3.14  Address"},{"location":"quick-start-guide/#315-gender","text":"When the keyword parameter gender is True, words indicating gender are preserved; otherwise they are removed. Transgender is inclusive in the recognition. The tag value is set to one of Vocabulary.MALE , Vocabulary.FEMALE or Vocabulary.TRANSGENDER . # keep/replace gender indicating words words = Words(\"man uncle mother women tg\", gender=True) print(words.words) will output: [ { 'word': 'man', tag: Vocabulary.MALE }, { 'word': 'uncle', tag: Vocabulary.MALE }, { 'word': 'mother', tag: Vocabulary.FEMALE }, { 'word': 'women', tag: Vocabulary.FEMALE }, { 'word': 'transgender', tag: Vocabulary.TRANSGENDER }, ]","title":"3.15  Gender"},{"location":"quick-start-guide/#316-sentiment","text":"When the keyword parameter sentiment is True, word and word phrases indicating sentiment are preserved; otherwise they are removed. Sentiment phrases are reduced to the single primary word indicating the sentiment and the tag value is set to either Vocabulary.POSITIVE or Vocabulary.NEGATIVE . # keep/replace sentiment indicating phrases words = Words(\"the food was not good\", sentiment=True) print(words.words) will output: [ { 'word': 'food', tag: Vocabulary.UNTAG }, { 'word': 'not', tag: Vocabulary.NEGATIVE}, ]","title":"3.16  Sentiment"},{"location":"quick-start-guide/#317-spell-checking","text":"When the keyword parameter spell is True, each tokenized word is looked up in the pyaspeller word dictionary. If the word is not found (presumed misspelled) and the pyaspeller recommends a safe replacement, the word is replaced with the pyaspeller's safe replacement. The spell check/replacement occurs prior to stemming, lemmatizing, and stopword removal. # add parts of speech tagging words = Words(\"mispelled\", spell=True) print(words.words) will output: [ { 'word': 'misspell', 'tag': Vocabulary.UNTAG}, ]","title":"3.17  Spell Checking"},{"location":"quick-start-guide/#318-parts-of-speech","text":"When the keyword parameter pos is True, each tokenized word is further annotated with it's corresponding NLTK parts of speech tag. # add parts of speech tagging words = Words(\"Jim Smith\", pos=True) print(words.words) will output: [ { 'word': 'food', 'tag': Vocabulary.UNTAG, 'pos': NN }, { 'word': 'not', 'tag': Vocabulary.NEGATIVE, 'pos': NN }, ]","title":"3.18  Parts of Speech"},{"location":"quick-start-guide/#319-romanization","text":"When the keyword parameter roman is True, the latin-1 character encoding of each tokenized is converted to ASCII. # Romanization of latin-1 character encodings words = Words(\"Qu\u00e9bec\", roman=True) print(words.words) will output: [ { 'word': 'quebec', 'tag': Vocabulary.UNTAG, ]","title":"3.19  Romanization"},{"location":"quick-start-guide/#320-bag-of-words-and-word-frequency-distribution","text":"The property bagsOfWords returns an unordered dictionary of each occurrence of a unique word in the tokenized sequence, where the word is the dictionary key, and the number of occurrences is the corresponding value. # Get the Bag of Words representation words = Words(\"Jack and Jill went up the hill to fetch a pail of water. Jack fell down and broke his crown and Jill came tumbling after.\", stopwords=True) print(words.bagOfWords) will output: { 'pail': 1, 'the': 1, 'a': 1, 'water': 1, 'fetch': 1, 'went': 1, 'and': 2, 'jack': 2, 'jill': 2, 'down': 1, 'come': 1, 'fell': 1, 'up': 1, 'of': 1, 'tumble': 1, 'to': 1, 'hill': 1, 'after': 1 } The property freqDist returns a sorted list of tuples, in descending order, of word frequencies (i.e., the number of occurrences of the word in the tokenized sequence. # Get the Word Frequency Distribution words = Words(\"Jack and Jill went up the hill to fetch a pail of water. Jack fell down and broke his crown and Jill came tumbling after.\", stopwords=True) print(words.freqDist) will output: [ ('jack', 2), ('jill', 2), ('and', 2), ('water', 1), ('the', 1), \u2026. ]","title":"3.20  Bag of Words and Word Frequency Distribution"},{"location":"quick-start-guide/#4-vision-module","text":"","title":"4  VISION Module"},{"location":"quick-start-guide/#41-image-processing","text":"To preprocess an image for computer vision machine learning, you create an Image (class) object, passing as parameters the path to the image, the corresponding label and a path for storing the preprocessed image data, the original image and optionally a thumbnail. The label must be specified as an integer value. Below is a code example. from vision import Image image = Image(\"yourimage.jpg\", 101, \"storage_path\") The above will generate the following output files: storage_path/yourimage.h5 # preprocessed image and raw data and optional thumbnail Alternately, the image path may be an URL; in which case, an HTTP request is made to obtain the image data from the remote location. image = Image(\"http://yourimage.jpg\", 101, \"storage_path\") The Image class supports processing of JPEG, PNG, TIF, BMP and GIF images. Images maybe of any pixel size, and number of channels (i.e. Grayscale, RGB and RGBA). Alternately, the input may be raw pixel data as a numpy array. raw = [...], [...], [\u2026] ] image = Image(raw, 101, \"storage_path\")","title":"4.1  Image Processing"},{"location":"quick-start-guide/#42-image-processing-settings-config","text":"CV Preprocessing of the image may be configured for several settings when instantiating an Image object with the optional config parameter, which consists of a list of one or more predefined options. image = Image(\"yourimage.jpg\", 101, \"storage_path\", config=[options]) options: gray | grayscale # convert to grayscale (single channel) normal | normalize # normalize the pixel data for values between 0 .. 1 flat | flatten # flatten the pixel data into a 1D vector resize=(height,width) # resize the image thumb=(height,width) # generate a thumbnail nostore # do not store the preprocessed image, raw and thumbnail data Example image = Image(\"image.jpg\", 101, \"path\", config=['flatten', 'thumb=(16,16)']) # will preprocess the image.jpg into machine learning ready data as a 1D vector, and # store the raw (unprocessed) decompressed data, preprocessed data and 16 x 16","title":"4.2  Image Processing Settings (Config)"},{"location":"quick-start-guide/#44-get-properties-of-preprocessed-image-data","text":"After an image has been preprocessed, several properties of the preprocessed image data can be obtained from the Image class properties: name - The root name of the image. type - The image format (e.g., png). shape - The shape of the preprocessed image data (e.g., (100, 100,3) ). data - The preprocessed image data as a numpy array. raw - The unprocessed decompressed image data as a numpy array. size - The byte size of the original image. thumb \u2013 The thumbnail image data as a numpy array. image = Image(\"yourimage.jpg\", \"storage_path\", 101) print(image.shape) Will output something like: (100,100,3)","title":"4.4  Get Properties of Preprocessed Image Data"},{"location":"quick-start-guide/#45-asynchronous-processing","text":"To enhance concurrent execution between a main thread and worker activities, the Image class supports asynchronous processing of the image. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Image object. Upon completion of the processing, the ehandler is called, where the Image object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of image is completed \"\"\" print(\"DONE\", i.image) # Process the image asynchronously image = Image(\"yourimage.png\", \"storage_path\", 101, ehandler=done)","title":"4.5  Asynchronous Processing"},{"location":"quick-start-guide/#46-image-reloading","text":"Once an Image object has been stored, it can later be retrieved from storage, reconstructing the Image object. An Image object is first instantiated, and then the load() method is called specifying the image name and corresponding storage path. The image name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Image object image = Image() # Reload the image's data from storage image.load( \"myimage.png\", \"mystorage\" )","title":"4.6  Image Reloading"},{"location":"quick-start-guide/#47-image-collection-processing","text":"To preprocess a collection of images for computer vision machine learning, you create an Images (class) object, passing as parameters a list of the paths to the images, a list of the corresponding label and a path for storing the collection of preprocessed image data, the original images and optionally thumbnails. Each label must be specified as an integer value. Below is a code example. from images import Images images = Images([\"image1.jpg\", \"image2.jpg\"], labels=[101, 102], name=' c1') The above will generate the following output files: train/c1.h5 # preprocessed image data The Images object will implicitly add the 'nostore' setting to the configuration parameter of each Image object created. This will direct each of the Image objects to not store the corresponding image data in an HD5 file. Instead, upon completion of the preprocessing of the collection of image data, the entire collection of preprocessed data is stored in a single HD5 file. Alternately, the list of image paths parameter may be a list of directories containing images. images = Images([\"subfolder1\", \"subfolder2\"], labels=[101, 102], name=' c1') Alternately, the list of labels parameter may be a single value; in which case the label value applies to all the images. images = Images([\"image1.jpg\", \"image2.jpg\"], labels=101, name=' c1')","title":"4.7  Image Collection Processing"},{"location":"quick-start-guide/#48-image-collection-processing-settings-config","text":"Configuration settings supported by the Image class may be specified as an optional parameter to the Images object, which are then passed down to each Image object generated for the collection. # Preprocess each image by normalizing the pixel data and then flatten into a 1D vector images = Images([\"image1.jpg\", \"image2.jpg\"], \"train\", labels=[101, 102], config=['normal', 'flatten'])","title":"4.8 Image Collection Processing Settings (Config)"},{"location":"quick-start-guide/#49-get-properties-of-a-collection","text":"After a collection of images has been preprocessed, several properties of the preprocessed image data can be obtained from the Images class properties: name \u2013 The name of the collection file. time \u2013 The time to preprocess the image. data \u2013 List of Image objects in the collection. len() \u2013 The len() operator will return the number of images in the collection. [] \u2013 The index operator will access the image objects in sequential order. # Access each Image object in the collection for ix in range(len(images)): image = images[ix]","title":"4.9  Get Properties of a Collection"},{"location":"quick-start-guide/#410-splitting-a-collection-into-training-and-test-data","text":"Batch, mini-batch and stochastic feed modes are supported. The percentage of data that is test (vs. training) is set by the split property, where the default is 0.2. Optionally, a mini-batch size is set by the minibatch property. Prior to the split, the data is randomized. The split() property when called as a getter will return the training data, training labels, test data, and test labels. # Set 30% of the images in the collection to be test data images.split = 0.3 # Get the entire training and test data and corresponding labels as lists. X_train, X_test, Y_train, Y_test = images.split Alternately, the next() operator will iterate through the image data, and corresponding label, in the training set. # Set 30% of the images in the collection to be test data images.split = 0.3 # Iterate through the training data while ( data, label = next(images) ) is not None: pass Training data can also be fetched in minibatches. The mini batch size is set using the minibatch property. The minibatch property when called as a getter will return a generator. The generator will iterate through each image, and corresponding label, of the generated mini-batch. Successive calls to the minibatch property will iterate through the training data. # Set 30% of the images in the collection to be test data images.split = 0.3 # Train the model in mini-batches of 30 images images.minibatch = 30 # loop for each mini-batch in training data for _ in range(nbatches) # create the generator g = images.minibatch # iterate through the mini-batch for data, label in g: pass The split property when used as a setter may optionally take a seed for initializing the randomized shuffling of the training set. # Set the seed for the random shuffle to 42 images.split = 0.3, 42","title":"4.10  Splitting a Collection into Training and Test Data"},{"location":"quick-start-guide/#411-image-augmentation","text":"Image augmentation is supported. By default, images are not augmented. If the property augment is set to True, then for each image generated for feeding (see next() and minibatch) an additional image will be generated. The additional image will be a randomized rotation between -90 and 90 degrees of the corresponding image. For example, if a training set has a 1000 images, then 2000 images will be feed when the property augment is set to True, where 1000 of the images are the original images, and another 1000 are the generated augmented images. images.split = 0.3, 42 # Enable image augmentation images.augment = True # Iterate through the training data, where every other image will be an augmented image while ( data, label = next(images) ) is not None: pass","title":"4.11  Image Augmentation"},{"location":"quick-start-guide/#412-asynchronous-collection-processing","text":"To enhance concurrent execution between a main thread and worker activities, the Images class supports asynchronous processing of the collection of images. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Images object. Upon completion of the processing, the ehandler is called, where the Images object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of collection of images is completed \"\"\" print(\"DONE\", i.images) # Process the collection of images asynchronously images = Images([\"img1.png\", \"img2.png\"], \"train\", labels=[0,1], ehandler=done)","title":"4.12  Asynchronous Collection Processing"},{"location":"quick-start-guide/#413-collection-reloading","text":"Once an Images object has been stored, it can later be retrieved from storage, reconstructing the Images object, and corresponding list of Image objects. An Images object is first instantiated, and then the load() method is called specifying the collection name and corresponding storage path. The collection name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Images object images = Images() # Reload the collection of image data from storage images.load( \"mycollection\", \"mystorage\" )","title":"4.13  Collection Reloading"},{"location":"quick-start-guide/#5-segmentation-module","text":"The segmentation module is newly introduced in Gap v0.9 prelaunch. It is in the early stage, and should be considered experimental, and not for commercial-product-ready yet. The segmentation module analyzes the whitespace layout of the text to identify the 'human' perceived grouping/purpose of text, such as paragraphs, headings, columns, page numbering, letterhead, etc., and the associated context. In this mode, the text is separated into segments, corresponding to identified layout, where each segment is then NLP preprocessed. The resulting NLP output is then hierarchical, where at the top level is the segment identification, and it's child is the NLP preprocessed text.","title":"5   SEGMENTATION Module"},{"location":"quick-start-guide/#51-text-segmentation","text":"When the config option 'segment' is specified on a Document object, the corresponding text per page is segmented. # import the segmentation module from segment import Segment segment = Segment(\"para 1\\n\\npara 2\") print(segment.segments) will output: [ { 'tag': 1002, words: [ { 'word': 'para', 'tag': 0}, {'word': 1, 'tag': 1}]}, { 'tag': 1002, words: [ { 'word': 'para', 'tag': 0}, {'word': 2, 'tag': 1}]} ] Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"5.1  Text Segmentation"},{"location":"specs/segmentation_spec/","text":"Gap Framework - Natural Language Processing for PDF/TIFF/Image Documents SEGMENTATION MODULE High Precision Document Segmentation Technical Specification, Gap v0.9.2 1. Segment 1.1 Segment Overview The segment NLP preprocessor contains the following primary classes, and their relationships: Segment - This is the base class for the representation of a Natural Language Processed text segmented into human perceived text layout, such as headings, paragraphs, table columns, etc. The constructor takes as a parameter a text to segment. segments = Segment(text) Fig. 1a High Level view of Segment Class Object Relationships 1.2 Segment Initializer (Constructor) Synopsis Segment(text=None) Parameters text : A Unicode text string Usage When specified without parameters, an empty Segments object is created. Otherwise, the text sequence is analyzed for region detection based on whitespace surrounding the text. Regions include: Headings Paragraphs Page Numbering Exceptions A TypeError is raised if the type of the parameter is not the expected type. 1.3 Segment Properties 1.3.1 segments Synopsis # Getter segments = segment.segments Usage When used as a getter the property returns REWRITE IN PROGRESS 1.4 Segment Overridden Operators 1.4.1 len() Synopsis nsegments = len(segment) Usage The len() (__len__) operator is overridden to return the number of Image objects in the collection. 1.4.1 [] Synopsis image = images[n] Usage The [] (__getitem__) operator is overridden to return the segment at the specified index. Exceptions A IndexError is raised if the index is out of range. 1.5 Segment Private Methods The segment class contains the following private methods: _segmentation() \u2013 This method is called by the constructor. It parses the text to identify text layouts, such as headings, paragraphs, columns, page numbering, etc, and separates the text into segments according to the identified layout. 1.6 Segment Public Methods The Segment class contains of following public methods: There are no public methods. APPENDIX I: Updates Pre-Gap (Epipog) v1.4 + An initial prototype was built Gap v0.91 (alpha) + Rewrite of Specification\u2003 APPENDIX II: Anticipated Engineering The following has been identified as enhancement/issues to be addressed in subsequent update: Add support for splitting dual column pages. Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"Segmentation"},{"location":"specs/segmentation_spec/#gap-framework-natural-language-processing-for-pdftiffimage-documents","text":"","title":"Gap Framework - Natural Language Processing for PDF/TIFF/Image Documents"},{"location":"specs/segmentation_spec/#segmentation-module","text":"","title":"SEGMENTATION MODULE"},{"location":"specs/segmentation_spec/#high-precision-document-segmentation","text":"","title":"High Precision Document Segmentation"},{"location":"specs/segmentation_spec/#technical-specification-gap-v092","text":"","title":"Technical Specification, Gap v0.9.2"},{"location":"specs/segmentation_spec/#1-segment","text":"","title":"1.  Segment"},{"location":"specs/segmentation_spec/#11-segment-overview","text":"The segment NLP preprocessor contains the following primary classes, and their relationships: Segment - This is the base class for the representation of a Natural Language Processed text segmented into human perceived text layout, such as headings, paragraphs, table columns, etc. The constructor takes as a parameter a text to segment. segments = Segment(text) Fig. 1a High Level view of Segment Class Object Relationships","title":"1.1  Segment Overview"},{"location":"specs/segmentation_spec/#12-segment-initializer-constructor","text":"","title":"1.2  Segment Initializer (Constructor)"},{"location":"specs/segmentation_spec/#synopsis","text":"Segment(text=None)","title":"Synopsis"},{"location":"specs/segmentation_spec/#parameters","text":"text : A Unicode text string","title":"Parameters"},{"location":"specs/segmentation_spec/#usage","text":"When specified without parameters, an empty Segments object is created. Otherwise, the text sequence is analyzed for region detection based on whitespace surrounding the text. Regions include: Headings Paragraphs Page Numbering","title":"Usage"},{"location":"specs/segmentation_spec/#exceptions","text":"A TypeError is raised if the type of the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/segmentation_spec/#13-segment-properties","text":"","title":"1.3  Segment Properties"},{"location":"specs/segmentation_spec/#131-segments","text":"","title":"1.3.1  segments"},{"location":"specs/segmentation_spec/#synopsis_1","text":"# Getter segments = segment.segments","title":"Synopsis"},{"location":"specs/segmentation_spec/#usage_1","text":"When used as a getter the property returns REWRITE IN PROGRESS","title":"Usage"},{"location":"specs/segmentation_spec/#14-segment-overridden-operators","text":"","title":"1.4  Segment Overridden Operators"},{"location":"specs/segmentation_spec/#141-len","text":"","title":"1.4.1  len()"},{"location":"specs/segmentation_spec/#synopsis_2","text":"nsegments = len(segment)","title":"Synopsis"},{"location":"specs/segmentation_spec/#usage_2","text":"The len() (__len__) operator is overridden to return the number of Image objects in the collection.","title":"Usage"},{"location":"specs/segmentation_spec/#141","text":"","title":"1.4.1  []"},{"location":"specs/segmentation_spec/#synopsis_3","text":"image = images[n]","title":"Synopsis"},{"location":"specs/segmentation_spec/#usage_3","text":"The [] (__getitem__) operator is overridden to return the segment at the specified index.","title":"Usage"},{"location":"specs/segmentation_spec/#exceptions_1","text":"A IndexError is raised if the index is out of range.","title":"Exceptions"},{"location":"specs/segmentation_spec/#15-segment-private-methods","text":"The segment class contains the following private methods: _segmentation() \u2013 This method is called by the constructor. It parses the text to identify text layouts, such as headings, paragraphs, columns, page numbering, etc, and separates the text into segments according to the identified layout.","title":"1.5  Segment Private Methods"},{"location":"specs/segmentation_spec/#16-segment-public-methods","text":"The Segment class contains of following public methods: There are no public methods.","title":"1.6  Segment Public Methods"},{"location":"specs/segmentation_spec/#appendix-i-updates","text":"Pre-Gap (Epipog) v1.4 + An initial prototype was built Gap v0.91 (alpha) + Rewrite of Specification","title":"APPENDIX I: Updates"},{"location":"specs/segmentation_spec/#appendix-ii-anticipated-engineering","text":"The following has been identified as enhancement/issues to be addressed in subsequent update: Add support for splitting dual column pages. Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"APPENDIX II: Anticipated Engineering"},{"location":"specs/splitter_spec/","text":"Gap Framework - Natural Language Processing for PDF/TIFF/Image Documents SPLITTER Module High Precision PDF Page Splitting/OCR/Text Extraction Technical Specification, Gap v0.9.2 1. Document 1.1 Document Overview The document classifier contains the following primary classes, and their relationships: Document \u2013 This is the base class for the representation of a stored document. The constructor for the class object takes as parameters the stored path to the document, optionally a directory path for storing extracted pages and text, and optionally an event completion handler when processing the document asynchronously, and optionally a config parameter for configuring the NLP preprocessing. document = Document(\u201c/somedir/mydocument.pdf\u201d, \u201c/mypages/mydocument\u201d) The constructors calls the _exists() and _collate() private methods for the specified document. Page \u2013 This is a base class for the representation of an extracted page from the document. The Document class contains a list (index) of the extracted pages as Page objects. Fig. 1a High Level view of Document Class Object Relationships 1.2 Initializer (Constructor) Synopsis Document( document=None, dir=\u2019./\u2019, ehandler=None, config=None) Parameters document: If not None, a string that is either: 1. local path to document 2. remote path to document ((i.e., http[s]://\u2026.) The document must be one of the following types: PDF. JPG, PNG, BMP or TIF dir: The directory where to store the machine learning ready data. ehandler: If not None, the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(images): # where images is the Images object that was preprocessed. config: If not None, a list of one or more configuration settings as strings: bare pos roman segment stem=gap|porter|Lancaster|snowball|lemma Usage When specified with no parameters, an empty Document object is created. The Document object may then be used to subsequent load (retrieve) previously stored preprocessed machine learning ready data (see load() ). Otherwise, the document parameter must be specified. The document specified by the document parameter will be preprocessed according to the optional parameters and configuration settings. By default, the document will be preprocessed as follows: The document will be split into individual pages. A Page object will be created for each page. If the document (or page) is an image (e.g., scanned PDF), it will be OCR\u2019d. The digital text will be extracted from each page and stored in the Page object. The text will be optionally segmented into regions if the configuration setting segment is specified. The text from each page object will be preprocessed into machine learning ready data (see syntax module specification), according to the optional parameters and configuration settings. If the document was a scanned or image document, the quality of the scan will be estimated, unless Document.SCANCHECK is set to zero. The machine learning ready data will be stored on a per page basis in the directory specified by the parameter dir. The following files are created and stored: <document><pageno>.<suffix> <document><pageno>.txt <document>.<pageno>.json The <document> is the root name of the document, and <pageno> is the corresponding page number starting at page 1. The file ending in the original file suffix <suffix> is the split page. The file ending in the file suffix .txt is the extracted text. The file ending in the file suffix .json is the NLP preprocessed machine learning data stored in a JSON format. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Document object passed as a parameter. If the path to the document file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A FileNotFoundError is raised if the document file does not exist. A IOError is raised if an error occurs reading in the document file. 1.3 Document Properties 1.3.1 document Synopsis # Getter path = document.document # Setter document.document = path Usage When used as a getter the property returns the path to the document file. When used as a setter the property specifies the path of the document file to preprocess into machine learning ready data (see initializer). Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the document file does not exist. A IOError is raised if an error occurs reading in the document file. 1.3.2 name Synopsis # Getter root = document.name Usage When used as a getter the property returns the root name of the document file (e.g., /mydir/mydocument.pdf -> mydocument). 1.3.3 type Synopsis # Getter suffix = document.type Usage When used as a getter the property returns the file suffix of the document file (e.g., pdf). 1.3.4 size Synopsis # Getter size = document.size Usage When used as a getter the property returns the file size of the document file in bytes. 1.3.5 dir Synopsis # Getter subfolder = document.dir # Setter document.dir = subfolder Usage When used as a getter the property returns the directory path where the corresponding files of the associated page objects are stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the files associated with the page objects are stored. Otherwise, it is ignored. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist. 1.3.6 label Synopsis # Getter label = document.label # Setter document.label = label Usage When used as a getter the property returns the integer label specified for the document. When used as a setter the property sets the label of the document to the specified integer value. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 1.3.7 lang Synopsis # Getter lang = document.lang Usage When used as a getter the property returns whether the language of the document, which may be either 'en' (English), 'es' (Spanish) or 'fr' (French). 1.3.8 scanned Synopsis # Getter scanned, quality = document.scanned Usage When used as a getter the property returns whether the document is a scanned image True or digital text False document, and the estimated quality of the scan as a percentage (between 0 and 1). 1.3.9 time Synopsis # Getter secs = document.time Usage When used as a getter the property returns the amount of time (in seconds) it took to preprocess the document into machine learning ready data. 1.3.10 text Synopsis # Getter text = document.text Usage When used as a getter the property returns a list, one entry per page, of the extracted text from the document in its original Unicode format. 1.3.11 pages Synopsis # Getter pages = document.pages 1.3.12 bagOfWords Synopsis # Getter bag = document.bagOfWords Usage When used as a getter the property returns the document\u2019s word sequences as a Bag of Words, represented as an unordered dictionary, where the key is the word and the value is the number of occurrences: { \u2018<word\u2019> : <no. of occurrences>, \u2026 } 1.3.13 freqDist Synopsis # Getter freq = document.freqDist Usage When used as a getter the property returns the sorted tuples of a frequency distribution of words (from bag of words), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <no. of occurrences> ), \u2026. ] 1.3.14 termFreq Synopsis # Getter tf = document.termFreq Usage When used as a getter the property returns the sorted tuples of a term frequency distribution (percent that term occurs), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <percentage of occurrences> ), \u2026. ] 1.3.15 Static Variables The Document class contains the following static variables: RESOLUTION \u2013 The image resolution when converting PDF to PNG for OCR (default 300 ). SCANCHECK \u2013 The number of OCR words to check to estimate the quality of the scan. WORDDICT - The word dictionary to use for scan spell check (default to norvig ). 1.4 Document Overridden Operators 1.4.1 len() Synopsis npages = len(document) Usage The len() (__len__) operator is overridden to return the number of pages in the document. 1.4.2 += Synopsis document += page Usage The += (__iadd__) method is overridden to append a Page object to the document. 1.4.3 [] Synopsis page= documents[n] document[n] = page Usage The [] (__getitem__) operator is overridden to return the Page object at the specified index. The __setitem__() method is overridden to replace the Page object at the specified index (i.e., page number \u2013 1). Exceptions A IndexError is raised if the index is out of range. 1.4.4 str() Synopsis label = str(image) Usage The str() (__str__) operator is overridden to return the label of the document as a string. 1.5 Document Public Methods 1.5.1 load() Synopsis document.load(name, dir=None) Parameters name: The name of the document. Usage This method will load into memory a preprocessed machine learning ready data from the corresponding JSON files specified by the document (root) name. The method will load the JSON files by the filename <name><pageno>.json . If dir is None , then it will look for the files where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the files under the directory specified by the dir parameter. Once loaded, the Document object will have the same characteristics as when the Document object was created. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None . 1.6 Document Private Methods The Document class contains the following private methods: _exists() \u2013 This method checks if the document exists at the specified stored path. If not, a FileNotFound exception is thrown. _collate() \u2013 This method performs the collation task, which includes: Determines the number of pages in the document. Splits the document into individual pages, where each page is individually stored in the same format as the document. The pages are named as follows: <name><pageno>.<suffix> Each page is stored in the subdirectory specified by the property dir. If dir is None, then the page is stored in the same directory where program is ran; otherwise, if the subdirectory does not exist, it is created. - If the page is a scanned PDF page, the scanned image is extracted and saved as a PNG image. The PNG image is then OCR\u2019d to convert to text. <name><pageno>.png If the page is a TIFF facsimile, the TIFF image is then OCR\u2019d to convert to text. <name><pageno>.tif If the page is an image capture (e.g. camera capture), the captured image (e.g., JPG) is then OCR\u2019d to convert to text. <name><pageno>.jpg Extracts the raw text from the page , where each page is individually stored in a raw text format. The pages are named as follows: <name><pageno>.txt Each page is stored in the subdirectory specified by the property dir. If dir is None, then the page is stored in the same directory where program is ran. Create a Page object for each page and adds them to the pages index property. If the document format is raw text, then: Treats as a single page. Stores only a single page text file. If the document format is PDF, then page splitting and extraction of the raw text per page is done with the open source version of Ghostscript. If the document is a scanned PDF, the image is extracted and converted to PNG using Ghostscript and then OCR\u2019d using open source Tesseract. If the document format is TIFF, then page splitting is done with the open source Magick and then OCR\u2019d using open source Tesseract. _langcheck() - This method is called after NLP preprocessing of the document has been completed. The method will sample upto ten words to probabilistically determine the language of the document. The detected languages are English, French and Spanish. _scancheck() \u2013 This method is called after NLP preprocessing of the document has been completed, and the document was a scanned image. The method will sample upto SCANCHECK number of words for recognition in the detected language dictionary (i.e., English, Spanish or French). The method will check the words on either page 1 or page 2, depending on which page has a greater number of words. Punctuation, symbols, acronyms or single letter words are excluded. The method then sets the internal variable _quality to the percentage of the words that were recognized (between 0 and 1). _async() \u2013 This method performs asynchronous processing of the _collate() function, when the optional ehandler parameter to the constructor is not None . When processing is completed, the ehandler parameter value is called as a function to signal completion of the processing, and the document object is passed as a parameter. 2. Page 2.1 Page Overview The page classifier contains the following primary classes, and their relationships: Page \u2013 This is a base class for the representation of an extracted page from a document. The constructor for the class object takes optionally as parameters the stored path to the page, and the extracted raw text. page = Page( \u2018/mypages/page1.pdf\u2019, \u2018some text\u2019) Words \u2013 This is a base class for representation of the text as NLP preprocessed list of words. Fig. 2a High Level view of Page Class Object Relationships 2.2 Page Initializer (Constructor) Synopsis Page( page=None, text=None, pageno=None) Parameters page: If not None, the local path to the page. text: If not None, the text corresponding to the page. pageno: If not None, the page number in the corresponding Document object. Usage If the text parameter is not None, a Words object is created and instantiated with the corresponding text. The text is then NLP preprocessed according to the configuration settings stored as static members in the Page class (i.e., set by the parent Document object): BARE : If True, then the bare configuration setting is passed to the Words object. STEM : If not None, then the stem configuration setting is passed to the Words object. ROMAN : If True, then the roman configuration setting is passed to the Words object. POS : If True, then the pos configuration setting is passed to the Words object. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file specified by page parameter does not exist. 2.3 Page Properties 2.3.1 path Synopsis # Getter path = page.path # Setter page.path= path Usage When used as a getter the property returns the path of the corresponding page (i.e., split by Document object) in its native format. When used as a setter the property sets the path of the corresponding split page. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file specified by path does not exist. 2.3.2 pageno Synopsis # Getter pageno = page.pageno Usage When used as a getter the property returns the pageno set for the Page object in the corresponding parent Document object. 2.3.3 size Synopsis # Getter nbytes = page.size Usage When used as a getter the property returns the byte size of the text parameter. 2.3.4 label Synopsis # Getter label = page.label # Setter page.label = label Usage When used as a getter the property returns the integer label that has been assigned to the page. When used as a setter the property assigns sets a label to the page. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 2.3.5 words Synopsis # Getter words = page.words Usage When used as a getter the property returns the Words object of the corresponding NLP preprocessed text. 2.3.6 bagOfWords Synopsis # Getter bag = page.bagOfWords Usage When used as a getter the property returns the page\u2019s word sequences as a Bag of Words, represented as an unordered dictionary, where the key is the word and the value is the number of occurrences: { \u2018<word\u2019> : <no. of occurrences>, \u2026 } 2.3.7 freqDist Synopsis # Getter freq = page.freqDist Usage When used as a getter the property returns the sorted tuples of a frequency distribution of words (from bag of words), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <no. of occurrences> ), \u2026. ] 2.3.8 termFreq Synopsis # Getter tf = page.termFreq Usage When used as a getter the property returns the sorted tuples of a term frequency distribution (percent that term occurs), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <percentage of occurrences> ), \u2026. ] 2.3.9 Static Variables The Page class contains the following static variables: BARE : If True, then the bare configuration setting is passed to the Words object. STEM : If not None, then the stem configuration setting is passed to the Words object. ROMAN : If True, then the roman configuration setting is passed to the Words object. POS : If True, then the pos configuration setting is passed to the Words object. 2.4 Page Overwritten Operators 2.4.1 len() Synopsis nwords = len(pages) Usage The len() (__len__) operator is overridden to return the number of NLP tokenized words in the page. 2.4.2 += Synopsis page += text Usage The += (__iadd__) method is overridden to append text to the page, which is then NLP preprocessed. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 2.4.3 str() Synopsis label = str(image) Usage The str() (__str__) operator is overridden to return the label of the page as a string. 2.5 Page Private Methods The Page class contains no private methods. \u2003 2.6 Page Public Methods 2.6.1 store() Synopsis image.store(path) Parameters path: the file path to write to. Usage The store() method writes the NLP tokenized sequence as a JSON object to the specified file. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file path is invalid. 2.6.2 load() Synopsis image.load(path) Parameters path: the file path to read from. Usage The load() method writes the NLP tokenized sequence as a JSON object from the specified file. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file path is invalid. APPENDIX I: Updates Pre-Gap (Epipog) v1.1 1. Added time property. 2. Added scanned property. 3. Added support for TIFF and JPG/PNG. Pre-Gap (Epipog) v1.3 1. Add direct read of PDF resource element to determine if scanned page. 2. Fix not detecting scanned PDF if text extraction produced noise. Pre-Gap (Epipog) v1.4 1. Added pageno property to Page class. 2. Added methods store() and load() to Page class to store/load NLP tokenized words to file. 3. Added method load() to Document class to reload NLP tokenized words from storage. 4. Added config keyword arguent to Document initializer to configure NLP preprocessing. Pre-Gap (Epipog) v1.5 1. Added bagOfWords , freqDist , and termFreq properties to Document and Page class. Gap v0.9.1 (alpha) 1. Rewrote Specification 2. Add OCR quality estimate Gap v0.9.2 (alpha) 1. Add language detection for English, Spanish and French. APPENDIX II: Anticipated Engineering The following has been identified as enhancement/issues to be addressed in subsequent update: 1. What does it mean to add text to a document. 2. Break raw text into pages for > 50 lines 3. Refactor page counting for faster performance 4. Add page split endpoint for streaming interface and URL 5. Add more pdf test files 6. Fix bug of not handling Cryllic characters in page load() method. Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"Splitter"},{"location":"specs/splitter_spec/#gap-framework-natural-language-processing-for-pdftiffimage-documents","text":"","title":"Gap Framework - Natural Language Processing for PDF/TIFF/Image Documents"},{"location":"specs/splitter_spec/#splitter-module","text":"High Precision PDF Page Splitting/OCR/Text Extraction Technical Specification, Gap v0.9.2","title":"SPLITTER Module"},{"location":"specs/splitter_spec/#1-document","text":"","title":"1.  Document"},{"location":"specs/splitter_spec/#11-document-overview","text":"The document classifier contains the following primary classes, and their relationships: Document \u2013 This is the base class for the representation of a stored document. The constructor for the class object takes as parameters the stored path to the document, optionally a directory path for storing extracted pages and text, and optionally an event completion handler when processing the document asynchronously, and optionally a config parameter for configuring the NLP preprocessing. document = Document(\u201c/somedir/mydocument.pdf\u201d, \u201c/mypages/mydocument\u201d) The constructors calls the _exists() and _collate() private methods for the specified document. Page \u2013 This is a base class for the representation of an extracted page from the document. The Document class contains a list (index) of the extracted pages as Page objects. Fig. 1a High Level view of Document Class Object Relationships","title":"1.1  Document Overview"},{"location":"specs/splitter_spec/#12-initializer-constructor","text":"","title":"1.2  Initializer (Constructor)"},{"location":"specs/splitter_spec/#synopsis","text":"Document( document=None, dir=\u2019./\u2019, ehandler=None, config=None)","title":"Synopsis"},{"location":"specs/splitter_spec/#parameters","text":"document: If not None, a string that is either: 1. local path to document 2. remote path to document ((i.e., http[s]://\u2026.) The document must be one of the following types: PDF. JPG, PNG, BMP or TIF dir: The directory where to store the machine learning ready data. ehandler: If not None, the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(images): # where images is the Images object that was preprocessed. config: If not None, a list of one or more configuration settings as strings: bare pos roman segment stem=gap|porter|Lancaster|snowball|lemma","title":"Parameters"},{"location":"specs/splitter_spec/#usage","text":"When specified with no parameters, an empty Document object is created. The Document object may then be used to subsequent load (retrieve) previously stored preprocessed machine learning ready data (see load() ). Otherwise, the document parameter must be specified. The document specified by the document parameter will be preprocessed according to the optional parameters and configuration settings. By default, the document will be preprocessed as follows: The document will be split into individual pages. A Page object will be created for each page. If the document (or page) is an image (e.g., scanned PDF), it will be OCR\u2019d. The digital text will be extracted from each page and stored in the Page object. The text will be optionally segmented into regions if the configuration setting segment is specified. The text from each page object will be preprocessed into machine learning ready data (see syntax module specification), according to the optional parameters and configuration settings. If the document was a scanned or image document, the quality of the scan will be estimated, unless Document.SCANCHECK is set to zero. The machine learning ready data will be stored on a per page basis in the directory specified by the parameter dir. The following files are created and stored: <document><pageno>.<suffix> <document><pageno>.txt <document>.<pageno>.json The <document> is the root name of the document, and <pageno> is the corresponding page number starting at page 1. The file ending in the original file suffix <suffix> is the split page. The file ending in the file suffix .txt is the extracted text. The file ending in the file suffix .json is the NLP preprocessed machine learning data stored in a JSON format. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Document object passed as a parameter. If the path to the document file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions","text":"A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A FileNotFoundError is raised if the document file does not exist. A IOError is raised if an error occurs reading in the document file.","title":"Exceptions"},{"location":"specs/splitter_spec/#13-document-properties","text":"","title":"1.3  Document Properties"},{"location":"specs/splitter_spec/#131-document","text":"","title":"1.3.1  document"},{"location":"specs/splitter_spec/#synopsis_1","text":"# Getter path = document.document # Setter document.document = path","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_1","text":"When used as a getter the property returns the path to the document file. When used as a setter the property specifies the path of the document file to preprocess into machine learning ready data (see initializer).","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_1","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the document file does not exist. A IOError is raised if an error occurs reading in the document file.","title":"Exceptions"},{"location":"specs/splitter_spec/#132-name","text":"","title":"1.3.2  name"},{"location":"specs/splitter_spec/#synopsis_2","text":"# Getter root = document.name","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_2","text":"When used as a getter the property returns the root name of the document file (e.g., /mydir/mydocument.pdf -> mydocument).","title":"Usage"},{"location":"specs/splitter_spec/#133-type","text":"","title":"1.3.3  type"},{"location":"specs/splitter_spec/#synopsis_3","text":"# Getter suffix = document.type","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_3","text":"When used as a getter the property returns the file suffix of the document file (e.g., pdf).","title":"Usage"},{"location":"specs/splitter_spec/#134-size","text":"","title":"1.3.4  size"},{"location":"specs/splitter_spec/#synopsis_4","text":"# Getter size = document.size","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_4","text":"When used as a getter the property returns the file size of the document file in bytes.","title":"Usage"},{"location":"specs/splitter_spec/#135-dir","text":"","title":"1.3.5  dir"},{"location":"specs/splitter_spec/#synopsis_5","text":"# Getter subfolder = document.dir # Setter document.dir = subfolder","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_5","text":"When used as a getter the property returns the directory path where the corresponding files of the associated page objects are stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the files associated with the page objects are stored. Otherwise, it is ignored.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_2","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist.","title":"Exceptions"},{"location":"specs/splitter_spec/#136-label","text":"","title":"1.3.6  label"},{"location":"specs/splitter_spec/#synopsis_6","text":"# Getter label = document.label # Setter document.label = label","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_6","text":"When used as a getter the property returns the integer label specified for the document. When used as a setter the property sets the label of the document to the specified integer value.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_3","text":"A TypeError is raised if the type of the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/splitter_spec/#137-lang","text":"","title":"1.3.7 lang"},{"location":"specs/splitter_spec/#synopsis_7","text":"# Getter lang = document.lang","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_7","text":"When used as a getter the property returns whether the language of the document, which may be either 'en' (English), 'es' (Spanish) or 'fr' (French).","title":"Usage"},{"location":"specs/splitter_spec/#138-scanned","text":"","title":"1.3.8  scanned"},{"location":"specs/splitter_spec/#synopsis_8","text":"# Getter scanned, quality = document.scanned","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_8","text":"When used as a getter the property returns whether the document is a scanned image True or digital text False document, and the estimated quality of the scan as a percentage (between 0 and 1).","title":"Usage"},{"location":"specs/splitter_spec/#139-time","text":"","title":"1.3.9  time"},{"location":"specs/splitter_spec/#synopsis_9","text":"# Getter secs = document.time","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_9","text":"When used as a getter the property returns the amount of time (in seconds) it took to preprocess the document into machine learning ready data.","title":"Usage"},{"location":"specs/splitter_spec/#1310-text","text":"","title":"1.3.10  text"},{"location":"specs/splitter_spec/#synopsis_10","text":"# Getter text = document.text","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_10","text":"When used as a getter the property returns a list, one entry per page, of the extracted text from the document in its original Unicode format.","title":"Usage"},{"location":"specs/splitter_spec/#1311-pages","text":"","title":"1.3.11  pages"},{"location":"specs/splitter_spec/#synopsis_11","text":"# Getter pages = document.pages","title":"Synopsis"},{"location":"specs/splitter_spec/#1312-bagofwords","text":"","title":"1.3.12  bagOfWords"},{"location":"specs/splitter_spec/#synopsis_12","text":"# Getter bag = document.bagOfWords","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_11","text":"When used as a getter the property returns the document\u2019s word sequences as a Bag of Words, represented as an unordered dictionary, where the key is the word and the value is the number of occurrences: { \u2018<word\u2019> : <no. of occurrences>, \u2026 }","title":"Usage"},{"location":"specs/splitter_spec/#1313-freqdist","text":"","title":"1.3.13  freqDist"},{"location":"specs/splitter_spec/#synopsis_13","text":"# Getter freq = document.freqDist","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_12","text":"When used as a getter the property returns the sorted tuples of a frequency distribution of words (from bag of words), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <no. of occurrences> ), \u2026. ]","title":"Usage"},{"location":"specs/splitter_spec/#1314-termfreq","text":"","title":"1.3.14  termFreq"},{"location":"specs/splitter_spec/#synopsis_14","text":"# Getter tf = document.termFreq","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_13","text":"When used as a getter the property returns the sorted tuples of a term frequency distribution (percent that term occurs), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <percentage of occurrences> ), \u2026. ]","title":"Usage"},{"location":"specs/splitter_spec/#1315-static-variables","text":"The Document class contains the following static variables: RESOLUTION \u2013 The image resolution when converting PDF to PNG for OCR (default 300 ). SCANCHECK \u2013 The number of OCR words to check to estimate the quality of the scan. WORDDICT - The word dictionary to use for scan spell check (default to norvig ).","title":"1.3.15  Static Variables"},{"location":"specs/splitter_spec/#14-document-overridden-operators","text":"","title":"1.4  Document Overridden Operators"},{"location":"specs/splitter_spec/#141-len","text":"","title":"1.4.1  len()"},{"location":"specs/splitter_spec/#synopsis_15","text":"npages = len(document)","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_14","text":"The len() (__len__) operator is overridden to return the number of pages in the document.","title":"Usage"},{"location":"specs/splitter_spec/#142","text":"","title":"1.4.2  +="},{"location":"specs/splitter_spec/#synopsis_16","text":"document += page","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_15","text":"The += (__iadd__) method is overridden to append a Page object to the document.","title":"Usage"},{"location":"specs/splitter_spec/#143","text":"","title":"1.4.3  []"},{"location":"specs/splitter_spec/#synopsis_17","text":"page= documents[n] document[n] = page","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_16","text":"The [] (__getitem__) operator is overridden to return the Page object at the specified index. The __setitem__() method is overridden to replace the Page object at the specified index (i.e., page number \u2013 1).","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_4","text":"A IndexError is raised if the index is out of range.","title":"Exceptions"},{"location":"specs/splitter_spec/#144-str","text":"","title":"1.4.4  str()"},{"location":"specs/splitter_spec/#synopsis_18","text":"label = str(image)","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_17","text":"The str() (__str__) operator is overridden to return the label of the document as a string.","title":"Usage"},{"location":"specs/splitter_spec/#15-document-public-methods","text":"","title":"1.5  Document Public Methods"},{"location":"specs/splitter_spec/#151-load","text":"","title":"1.5.1  load()"},{"location":"specs/splitter_spec/#synopsis_19","text":"document.load(name, dir=None)","title":"Synopsis"},{"location":"specs/splitter_spec/#parameters_1","text":"name: The name of the document.","title":"Parameters"},{"location":"specs/splitter_spec/#usage_18","text":"This method will load into memory a preprocessed machine learning ready data from the corresponding JSON files specified by the document (root) name. The method will load the JSON files by the filename <name><pageno>.json . If dir is None , then it will look for the files where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the files under the directory specified by the dir parameter. Once loaded, the Document object will have the same characteristics as when the Document object was created.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_5","text":"A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None .","title":"Exceptions"},{"location":"specs/splitter_spec/#16-document-private-methods","text":"The Document class contains the following private methods: _exists() \u2013 This method checks if the document exists at the specified stored path. If not, a FileNotFound exception is thrown. _collate() \u2013 This method performs the collation task, which includes: Determines the number of pages in the document. Splits the document into individual pages, where each page is individually stored in the same format as the document. The pages are named as follows: <name><pageno>.<suffix> Each page is stored in the subdirectory specified by the property dir. If dir is None, then the page is stored in the same directory where program is ran; otherwise, if the subdirectory does not exist, it is created. - If the page is a scanned PDF page, the scanned image is extracted and saved as a PNG image. The PNG image is then OCR\u2019d to convert to text. <name><pageno>.png If the page is a TIFF facsimile, the TIFF image is then OCR\u2019d to convert to text. <name><pageno>.tif If the page is an image capture (e.g. camera capture), the captured image (e.g., JPG) is then OCR\u2019d to convert to text. <name><pageno>.jpg Extracts the raw text from the page , where each page is individually stored in a raw text format. The pages are named as follows: <name><pageno>.txt Each page is stored in the subdirectory specified by the property dir. If dir is None, then the page is stored in the same directory where program is ran. Create a Page object for each page and adds them to the pages index property. If the document format is raw text, then: Treats as a single page. Stores only a single page text file. If the document format is PDF, then page splitting and extraction of the raw text per page is done with the open source version of Ghostscript. If the document is a scanned PDF, the image is extracted and converted to PNG using Ghostscript and then OCR\u2019d using open source Tesseract. If the document format is TIFF, then page splitting is done with the open source Magick and then OCR\u2019d using open source Tesseract. _langcheck() - This method is called after NLP preprocessing of the document has been completed. The method will sample upto ten words to probabilistically determine the language of the document. The detected languages are English, French and Spanish. _scancheck() \u2013 This method is called after NLP preprocessing of the document has been completed, and the document was a scanned image. The method will sample upto SCANCHECK number of words for recognition in the detected language dictionary (i.e., English, Spanish or French). The method will check the words on either page 1 or page 2, depending on which page has a greater number of words. Punctuation, symbols, acronyms or single letter words are excluded. The method then sets the internal variable _quality to the percentage of the words that were recognized (between 0 and 1). _async() \u2013 This method performs asynchronous processing of the _collate() function, when the optional ehandler parameter to the constructor is not None . When processing is completed, the ehandler parameter value is called as a function to signal completion of the processing, and the document object is passed as a parameter.","title":"1.6  Document Private Methods"},{"location":"specs/splitter_spec/#2-page","text":"","title":"2.  Page"},{"location":"specs/splitter_spec/#21-page-overview","text":"The page classifier contains the following primary classes, and their relationships: Page \u2013 This is a base class for the representation of an extracted page from a document. The constructor for the class object takes optionally as parameters the stored path to the page, and the extracted raw text. page = Page( \u2018/mypages/page1.pdf\u2019, \u2018some text\u2019) Words \u2013 This is a base class for representation of the text as NLP preprocessed list of words. Fig. 2a High Level view of Page Class Object Relationships","title":"2.1  Page Overview"},{"location":"specs/splitter_spec/#22-page-initializer-constructor","text":"","title":"2.2  Page Initializer (Constructor)"},{"location":"specs/splitter_spec/#synopsis_20","text":"Page( page=None, text=None, pageno=None)","title":"Synopsis"},{"location":"specs/splitter_spec/#parameters_2","text":"page: If not None, the local path to the page. text: If not None, the text corresponding to the page. pageno: If not None, the page number in the corresponding Document object.","title":"Parameters"},{"location":"specs/splitter_spec/#usage_19","text":"If the text parameter is not None, a Words object is created and instantiated with the corresponding text. The text is then NLP preprocessed according to the configuration settings stored as static members in the Page class (i.e., set by the parent Document object): BARE : If True, then the bare configuration setting is passed to the Words object. STEM : If not None, then the stem configuration setting is passed to the Words object. ROMAN : If True, then the roman configuration setting is passed to the Words object. POS : If True, then the pos configuration setting is passed to the Words object.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_6","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file specified by page parameter does not exist.","title":"Exceptions"},{"location":"specs/splitter_spec/#23-page-properties","text":"","title":"2.3  Page Properties"},{"location":"specs/splitter_spec/#231-path","text":"","title":"2.3.1  path"},{"location":"specs/splitter_spec/#synopsis_21","text":"# Getter path = page.path # Setter page.path= path","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_20","text":"When used as a getter the property returns the path of the corresponding page (i.e., split by Document object) in its native format. When used as a setter the property sets the path of the corresponding split page.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_7","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file specified by path does not exist.","title":"Exceptions"},{"location":"specs/splitter_spec/#232-pageno","text":"","title":"2.3.2  pageno"},{"location":"specs/splitter_spec/#synopsis_22","text":"# Getter pageno = page.pageno","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_21","text":"When used as a getter the property returns the pageno set for the Page object in the corresponding parent Document object.","title":"Usage"},{"location":"specs/splitter_spec/#233-size","text":"","title":"2.3.3  size"},{"location":"specs/splitter_spec/#synopsis_23","text":"# Getter nbytes = page.size","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_22","text":"When used as a getter the property returns the byte size of the text parameter.","title":"Usage"},{"location":"specs/splitter_spec/#234-label","text":"","title":"2.3.4  label"},{"location":"specs/splitter_spec/#synopsis_24","text":"# Getter label = page.label # Setter page.label = label","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_23","text":"When used as a getter the property returns the integer label that has been assigned to the page. When used as a setter the property assigns sets a label to the page.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_8","text":"A TypeError is raised if the type of the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/splitter_spec/#235-words","text":"Synopsis # Getter words = page.words","title":"2.3.5  words"},{"location":"specs/splitter_spec/#usage_24","text":"When used as a getter the property returns the Words object of the corresponding NLP preprocessed text.","title":"Usage"},{"location":"specs/splitter_spec/#236-bagofwords","text":"","title":"2.3.6  bagOfWords"},{"location":"specs/splitter_spec/#synopsis_25","text":"# Getter bag = page.bagOfWords","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_25","text":"When used as a getter the property returns the page\u2019s word sequences as a Bag of Words, represented as an unordered dictionary, where the key is the word and the value is the number of occurrences: { \u2018<word\u2019> : <no. of occurrences>, \u2026 }","title":"Usage"},{"location":"specs/splitter_spec/#237-freqdist","text":"","title":"2.3.7  freqDist"},{"location":"specs/splitter_spec/#synopsis_26","text":"# Getter freq = page.freqDist","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_26","text":"When used as a getter the property returns the sorted tuples of a frequency distribution of words (from bag of words), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <no. of occurrences> ), \u2026. ]","title":"Usage"},{"location":"specs/splitter_spec/#238-termfreq","text":"","title":"2.3.8  termFreq"},{"location":"specs/splitter_spec/#synopsis_27","text":"# Getter tf = page.termFreq","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_27","text":"When used as a getter the property returns the sorted tuples of a term frequency distribution (percent that term occurs), in descending order (i.e., highest first) [ ( \u2018<word\u2019>: <percentage of occurrences> ), \u2026. ]","title":"Usage"},{"location":"specs/splitter_spec/#239-static-variables","text":"The Page class contains the following static variables: BARE : If True, then the bare configuration setting is passed to the Words object. STEM : If not None, then the stem configuration setting is passed to the Words object. ROMAN : If True, then the roman configuration setting is passed to the Words object. POS : If True, then the pos configuration setting is passed to the Words object.","title":"2.3.9  Static Variables"},{"location":"specs/splitter_spec/#24-page-overwritten-operators","text":"","title":"2.4  Page Overwritten Operators"},{"location":"specs/splitter_spec/#241-len","text":"","title":"2.4.1  len()"},{"location":"specs/splitter_spec/#synopsis_28","text":"nwords = len(pages)","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_28","text":"The len() (__len__) operator is overridden to return the number of NLP tokenized words in the page.","title":"Usage"},{"location":"specs/splitter_spec/#242","text":"","title":"2.4.2  +="},{"location":"specs/splitter_spec/#synopsis_29","text":"page += text","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_29","text":"The += (__iadd__) method is overridden to append text to the page, which is then NLP preprocessed.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_9","text":"A TypeError is raised if the type of the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/splitter_spec/#243-str","text":"","title":"2.4.3  str()"},{"location":"specs/splitter_spec/#synopsis_30","text":"label = str(image)","title":"Synopsis"},{"location":"specs/splitter_spec/#usage_30","text":"The str() (__str__) operator is overridden to return the label of the page as a string.","title":"Usage"},{"location":"specs/splitter_spec/#25-page-private-methods","text":"The Page class contains no private methods.","title":"2.5  Page Private Methods"},{"location":"specs/splitter_spec/#26-page-public-methods","text":"","title":"2.6  Page Public Methods"},{"location":"specs/splitter_spec/#261-store","text":"","title":"2.6.1 store()"},{"location":"specs/splitter_spec/#synopsis_31","text":"image.store(path)","title":"Synopsis"},{"location":"specs/splitter_spec/#parameters_3","text":"path: the file path to write to.","title":"Parameters"},{"location":"specs/splitter_spec/#usage_31","text":"The store() method writes the NLP tokenized sequence as a JSON object to the specified file. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file path is invalid.","title":"Usage"},{"location":"specs/splitter_spec/#262-load","text":"","title":"2.6.2  load()"},{"location":"specs/splitter_spec/#synopsis_32","text":"image.load(path)","title":"Synopsis"},{"location":"specs/splitter_spec/#parameters_4","text":"path: the file path to read from.","title":"Parameters"},{"location":"specs/splitter_spec/#usage_32","text":"The load() method writes the NLP tokenized sequence as a JSON object from the specified file.","title":"Usage"},{"location":"specs/splitter_spec/#exceptions_10","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the file path is invalid.","title":"Exceptions"},{"location":"specs/splitter_spec/#appendix-i-updates","text":"Pre-Gap (Epipog) v1.1 1. Added time property. 2. Added scanned property. 3. Added support for TIFF and JPG/PNG. Pre-Gap (Epipog) v1.3 1. Add direct read of PDF resource element to determine if scanned page. 2. Fix not detecting scanned PDF if text extraction produced noise. Pre-Gap (Epipog) v1.4 1. Added pageno property to Page class. 2. Added methods store() and load() to Page class to store/load NLP tokenized words to file. 3. Added method load() to Document class to reload NLP tokenized words from storage. 4. Added config keyword arguent to Document initializer to configure NLP preprocessing. Pre-Gap (Epipog) v1.5 1. Added bagOfWords , freqDist , and termFreq properties to Document and Page class. Gap v0.9.1 (alpha) 1. Rewrote Specification 2. Add OCR quality estimate Gap v0.9.2 (alpha) 1. Add language detection for English, Spanish and French.","title":"APPENDIX I: Updates"},{"location":"specs/splitter_spec/#appendix-ii-anticipated-engineering","text":"The following has been identified as enhancement/issues to be addressed in subsequent update: 1. What does it mean to add text to a document. 2. Break raw text into pages for > 50 lines 3. Refactor page counting for faster performance 4. Add page split endpoint for streaming interface and URL 5. Add more pdf test files 6. Fix bug of not handling Cryllic characters in page load() method. Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"APPENDIX II: Anticipated Engineering"},{"location":"specs/syntax_spec/","text":"Gap Framework - Natural Language Processing for PDF/TIFF/Image Documents SYNTAX MODULE High Precision Natural Language Processing Technical Specification, Gap v0.9.2 1 Words 1.1 Words Overview The words NLP preprocessor contains the following primary classes, and their relationships: Words - This is the base class for the representation of a Natural Language Processing (NLP) preprocessed list of words. The constructor optionally takes as parameters the raw text to tokenize and flags for NLP preprocessing the text. words = Words(\"some text\", flags \u2026) The constructor calls the private methods _split() , _stem() , and _stopwords() . Word \u2013 A single NLP preprocessed word (token). Vocabulary \u2013 A performance optimized python dictionary for word classification and lemmatizing. Fig. 1a High Level view of Words Class Object Relationships 1.2 Words Initializer (Constructor) Synopsis Words( text, flags \u2026 ) Parameters text: A Unicode string of text. flags: Zero or more keyword parameters bare: Tokenize only True - do not preprocess. punct: Keep/classify True or remove False punctuation. stopwords: Keep True all stop words or remove False . If True, it supersedes all other flags. If False, other flags may be used to include specific categories. The stop words are a superset of the Porter list. stem: Value indicating which stemmer to use: - builtin: gap - NLTK: porter, snowball, lancaster or the WordLemmatizer: lemma pos: Annotate True or not annotate False NLP preprocessed tokens with parts of speech using NLTK pos_tag() . spell: Spell check and replace misspelled words using norvig. The parameter may be set to one of en (English), es (Spanish) or fr (French). roman: Romanize True or not Romanize False latin-1 encodings of NLP preprocessed tokens into ASCII encoding. number: Keep/classify True or remove False numerical numbers. Ex. - 1 / 4.5 / 1,000 / three unit: Keep/classify True or remove False units of measurement. Ex - inches / ft / cm - height / width / weight / ht / wt - temperature / \u00b0F / \u00b0C quantifier: Keep/classify True size specifying words or to remove False . Ex. - all / any / more preposition: Keep/classify True preposition words or to remove False . Ex. - to / from / above conjunction: Keep/classify True conjunction words or to remove False . Ex. - and / or / but article: keep/classify True article words or to remove False . Ex. - a / an / the demonstrative: Keep/classify True article words or to remove False . Ex. - this / that / these question: Keep/classify True question words or to remove False . Ex. - who / want / how pronoun: Keep/classify True pronoun words or to remove False . Ex. - he / she / them date: Keep/classify/reduce True dates or to remove False . Ex - Jan. 1, 2000 / 01/01/2000 / 2000-01-01 dob: Keep/classify/reduce True date of births or to remove False . Ex. - DOB: Jan 1, 2000 / date of birth is 01-02-2012 ssn: Keep/classify True social security numbers or to remove False . Ex. - 123-84-1234 / 123 84 1234 telephone: Keep/classify True telephone numbers or to remove False . Ex. - (360) 123-1234 / +13601231234 name: Keep/classify True telephone numbers or to remove False . Ex. - Albert Einstein / Donald J. Trump address: Keep/classify/reduce street address or to remove False . Ex - 124 NE 34th Cir, Home Town, AZ, 99123 gender: Keep/classify True gender specifying words or remove False . Ex. - male / man / gal / mom sentiment: Keep/classify/reduce True sentiment word sequences or remove False . - not bad / disgusting Exceptions A TypeError is raised if the parameter is not the expected type. 1.3 Words Properties 1.3.1 text Synopsis # Getter text = images.text # Setter images.text = text Usage When used as a getter the property returns the original text. When used as a setter the property re-preprocesses the text into machine learning ready data. Exceptions A TypeError is raised if the parameter is not the expected type. 1.3.2 bare Synopsis # Getter tokens = images.bare Usage When used as a getter the property returns the NLP tokenized list unprocessed. All punctuation, words, capitalization and diacritic characters and script are preserved. The tokenized list is in a dictionary format of the form: [ { 'word': word1, 'tag': tag }, {'word': word2, 'tag': tag } .. ] Except for numbers and acronyms, the tag values are set to untagged (0). 1.3.3 words Synopsis # Getter words = images.words Usage When used as a getter the property returns the NLP tokenized list according to the specified parameters. The tokenized list is in dictionary format of the form, when the parameter pos is False: [ { 'word': word1, 'tag': tag }, {'word': word2, 'tag': tag } .. ] Otherwise, when the pos parameter is set to True: [ { 'word': word1, 'tag': tag, 'pos': POS }, {'word': word2, 'tag': tag, 'pos': POS } .. ] 1.3.4 bagOfWords Synopsis # Getter bag = images.bagOfWords Usage When used as a getter the property returns the word sequence as a Bag of Words, represented as a unordered dictionary, where the key is the word and the value is the number of occurrences: { '<word'> : <no. of occurrences>, \u2026 } 1.3.5 freqDist Synopsis # Getter freq = images.freqDist Usage When used as a getter the property returns the sorted tuples of a frequency distribution of words (from bag of words), in descending order (i.e., highest first) [ ( '<word'>: <no. of occurrences> ), \u2026. ] 1.3.5 termFreq Synopsis # Getter tf = images.termFreq Usage When used as a getter the property returns the sorted tuples of a term frequency distribution (percent that term occurs), in descending order (i.e., highest first) [ ( '<word'>: <percentage of occurrences> ), \u2026. ] 1.3.6 Static Variables The Words class contains the following static variables: DECIMAL \u2013 The decimal point (US Standard: period , EU: comma) THOUSANDS \u2013 The thousandths unit separator (US Standard comma , EU period) 1.4 Words Overridden Operators 1.4.1 len() Synopsis nwords = len(words) Usage The len() (__len__) operator is overridden to return the number of NLP tokenized words. 1.4.2 += Synopsis words += text Usage The += (__iadd__) method is overridden to add words to the sequenced word list (append). 1.5 Words Private Methods The Words class contains the following private methods, which are called by the initializer: _split() \u2013 This method performs the first phase of NLP preprocessing of the raw text into a sequenced list of words (bare processing mode). Contractions are expanded (e.g., can't => can not). Newlines, carriage returns and tabs removed. Duplicated whitespace is removed. Text is split into words and punctuation. Punctuation is removed (except in numerical and date representations when property number and/or date and/or dob is True). \u2003 _preprocess() - This method performs the second of NLP preprocessing of the 'bare' tokenized words. Identify acronyms. Identify proper names. Words are lowercased. Optionally words are Romanized, if roman attribute is set to True. _stem() \u2013 This method performs the third phase of NLP preprocessing of the tokenized words by removing word endings and reducing word to its root stem (e.g., rider -> ride). Remove plural endings (e.g., flies -> fly). Remove past tense endings (e.g., baked -> bake). Remove present participle endings (e.g., eating -> eat). Remove verb to noun and comparative endings (e.g., rider -> ride, taller-> tall). Remove noun to verb endings (e.g., flatten -> flat). Remove adjective to adverb endings (e.g, costly -> cost). Remove superlative endings (e.g., greatest -> great). Spell check/replacement (according to the specified language), if enabled, occurs prior to stemming. _nltkStem() \u2013 This method uses the open source NLTK stemmer methods to perform the third phase of NLP preprocessing of the tokenized words, as an alternative to the internal stemmer (i.e. stem)). The Porter, Snowball, Lancaster and WordNetLemmatizer are selectable. _stopwords() \u2013 This method performs the fourth phase of NLP preprocessing of the tokenized words by removing/keeping stop words. Remove word (including infinity) and numeric representations of numbers, unless property number is True, then all numbers are retained. If retained, EU decimal and thousandths unit separators converted to US standard. +/- signs preserved. Thousandths unit separator removed. Hex numbers (starting with 0x prefix) are converted to integer value. Text represented numbers (e.g., ten) are converted to integer value. Text represented numeric ordering (e.g., 1st) are converted to integer value. Fractions are converted to floating point value. Remove units of measurement, unless property unit is True. US Standard and Metric, including abbreviations, are recognized. US and EU spelling of metric units are recognized. Remove dates, unless property date is True. Remove date of birth, unless property dob is True. Remove USA social security numbers, unless property ssn is True, where the SSN number is converted to a 9 digit value. Remove USA/CA telephone numbers, unless property telephone is True, where the telephone number is converted to a 10 digit number. Remove USA/CA addresses, unless property address is True, where addresses are converted to the USPO addressing standard. Remove gender indicating words (e.g., man) \u2013 inclusive of transgender, unless property telephone is True. Remove proper names and titles (e.g., Dr.), unless property name is True. Remove quantifier indicating words (e.g., all, any), unless property quantifier is True. Remove prepositions (e.g., above, under), unless property preposition is True. Remove conjunctions (e.g., and, or), unless property conjunction is True. Remove articles (e.g., a, an), unless property article is True. Remove demonstratives (e.g., this, that), unless property demonstrative is True. Remove pronouns (e.g., his, her), unless property pronoun is True. Remove question words (e.g., what, why), unless property question is True. Remove common high frequency words i.e., Porter List). Remove sentiment sequence (e.g., good, bad), unless sentiment property is True. Sequence (e.g., not bad) replaced with \"positive\" or \"negative\". Remove punctuation and symbols, unless punct property is True. _isdate() \u2013 This method is a support method for _stopwords(). It will recognize date strings and convert them to ISO 8601 format. The following formats are recognized: MM/DD/YY and MM/DD/YYYY MM-DD-YY and MM-DD-YYYY YYYY-MM-DD (ISO 8601) Month Day, Year (e.g., January 6, 2016) Abbr-Month Day, Year (e.g., Jan 6, 2016 and Jan. 6, 2016) If the preceding word is birth or DOB, then the date will be tagged as a date of birth (vs. date). _isnumber() - This method is a support method for _stopwords(). It will recognize numerical sequences and convert them to decimal base 10 format. The following formats are recognized Base 10 integer, floating point, exponent, fraction Base 16 hex integers _isSSN() - This method is a support method for _stopwords(). It will recognize USA Social Security numbers. The following formats are recognized: Prefixed with SSN or Soc. Sec. No. or Social Security Number Number Format: 12-123-1234 / 12 123 1234 / 121231234 _isTele() - This method is a support method for _stopwords(). It will recognize USA/CA Telephone numbers. The following formats are recognized: Prefixed with Tele, Phone, Mobile, Office, etc, optionally followed by Number, Num, No, # Number Format: 1231231234 / 123 123 1234 / 123-123-123 / (360) 123-1234 / \u2026 _isAddr() \u2013 This method is a support method for _stopwords() . It will recognize USA/CA postal addresses. The following formats are recognized: [POB[,]] Street-Number [Street-Direction] Street-Name [Street-Type] [Street-Direction] [,] [POB[,]] [Secondary-Address[,]][City[,]State] POB[,] City State _streetnum() \u2013 This method supports the _isAddr() method in recognizing street numbers. The following formats are recognized: [(N|S|W|E)]digits[letter][-][digits|letter Ex. N1300 / 123-33 / 33A / 33 _streetdir() \u2013 This method supports the _isAddr() method in recognizing directional phrases. The following formats are recognized: North|South[sp][West|East] N|S[.][w|e][.] _citystate() \u2013 This method supports the _isAddr() method in recognizing city/state references in a postal address. The following formats are recognized: City , USA and Canadian state names are replaced with their ISO 3166-2 codes (e.g., Alabama => ISO3166-2:US-AL). _pob() \u2013 This method supports the _isAddr() method in recognizing USA and Canadian Post Office Boxes and Private Mail Boxes in street addresses. The following formats are recognized: ( P.O.B | POB | P.O. Box | P.O. | PO ) digits [ (STN | RPO) words ] ( P.M.B | PMB | P.M. Box ) digits _streetaddr2() \u2013 This method supports the _isAddr() method in recognizing secondary address components in street addresses. The following formats are recognized: (Apt|Ste|Rm|Fl|Bldg|Dept) # _postalcode() \u2013 This method supports the _isAddr method() in recognizing USA and Canadian postal codes. The following formats are recognized: 5digits[-4digits] # USA [3letters][sp][3letters] # Canada _isGender() \u2013 This method supports gender recognition in stopwords. It recognizes phrases: (Sex|Gender)[:] (M|F|Male|Female) _conversion() \u2013 This method performs the fifth phase of NLP preprocessing of the tokenized words of converting Standard to Metric (standard=True) and vice-versa (metric=True). _partsofspeech() \u2013 This method performs the sixth phase of NLP preprocessing of the tokenized words of tagging words with their parts of speech tag (using NLTK). 1.5 Words Public Methods The Words class contains no public methods. APPENDIX I: Updates Pre-Gap (Epipog) v1.1 + Refactored Stopword Removal for Parts of Speech, Numbers and Dates + Added Vocabulary class and Lemmatizing + Added support for Date of Birth + Added support for recognizing word version of numbers. + Fixed handling of Hex numbers Pre-Gap (Epipog) v1.2 + Added support for Social Security Numbers + Added support for Telephone Numbers + Added support for Proper Names + Fix not recognizing Acronym if first word + Refactored tokenization and added bare mode. Pre-Gap (Epipog) v1.3 + Added support for Spanish punctuation: \u00bf\u00a1 + Added support for numeric multipliers (e.g., 10 million). + Added support for unit of measurements. + Added conversion of unit of measurements between Standard and Metric. + Added support for Sex/Gender[:] M/F + Added support for USA street addresses + Fix not recognizing single letter abbreviations. + Fixed not recognizing title (name) proceeded by a comma. + Fixed not recognizing number followed by unit of measurement when combined (e.g., 2cm). Pre-Gap (Epipog) v1.4 + Added NLTK options for stemming, lemmatization and parts of speech. + Change outputting of US State names to ISO 3166-2 standard. + Added support for PMB in postal address. + Added is/of separator between key and value (e.g., SSN is XXX-XX-XXXX). + Added support for Canadian Street/Postal addresses. + Added support for Romanizing latin-1 character encodings into ASCII. + Added support for measurements Pre-Gap (Epipog) v1.5 + Added Bag of Words, Word Frequency Distribution and Term Frequency (TF) Gap v0.9.1 (alpha) + Rewrote Specification. + Added spell check/replacement + Added UK to US spelling correction Gap v0.9.2 (alpha) + Extend Spell Checking to Spanish and French. APPENDIX II: Anticipated Engineering The following has been identified as enhancement/issues to be addressed in subsequent update: 1. Support for detecting Abbreviations 2. Support for email addresses 3. Add support for mail stop Secondary Street Address components. 4. Add support for Currency. 5. Add and annotation. 6. Fix lose next word after street/postal address Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"Syntax"},{"location":"specs/syntax_spec/#gap-framework-natural-language-processing-for-pdftiffimage-documents","text":"","title":"Gap Framework - Natural Language Processing for PDF/TIFF/Image Documents"},{"location":"specs/syntax_spec/#syntax-module","text":"High Precision Natural Language Processing Technical Specification, Gap v0.9.2","title":"SYNTAX MODULE"},{"location":"specs/syntax_spec/#1-words","text":"","title":"1  Words"},{"location":"specs/syntax_spec/#11-words-overview","text":"The words NLP preprocessor contains the following primary classes, and their relationships: Words - This is the base class for the representation of a Natural Language Processing (NLP) preprocessed list of words. The constructor optionally takes as parameters the raw text to tokenize and flags for NLP preprocessing the text. words = Words(\"some text\", flags \u2026) The constructor calls the private methods _split() , _stem() , and _stopwords() . Word \u2013 A single NLP preprocessed word (token). Vocabulary \u2013 A performance optimized python dictionary for word classification and lemmatizing. Fig. 1a High Level view of Words Class Object Relationships","title":"1.1  Words Overview"},{"location":"specs/syntax_spec/#12-words-initializer-constructor","text":"","title":"1.2 Words Initializer (Constructor)"},{"location":"specs/syntax_spec/#synopsis","text":"Words( text, flags \u2026 )","title":"Synopsis"},{"location":"specs/syntax_spec/#parameters","text":"text: A Unicode string of text. flags: Zero or more keyword parameters bare: Tokenize only True - do not preprocess. punct: Keep/classify True or remove False punctuation. stopwords: Keep True all stop words or remove False . If True, it supersedes all other flags. If False, other flags may be used to include specific categories. The stop words are a superset of the Porter list. stem: Value indicating which stemmer to use: - builtin: gap - NLTK: porter, snowball, lancaster or the WordLemmatizer: lemma pos: Annotate True or not annotate False NLP preprocessed tokens with parts of speech using NLTK pos_tag() . spell: Spell check and replace misspelled words using norvig. The parameter may be set to one of en (English), es (Spanish) or fr (French). roman: Romanize True or not Romanize False latin-1 encodings of NLP preprocessed tokens into ASCII encoding. number: Keep/classify True or remove False numerical numbers. Ex. - 1 / 4.5 / 1,000 / three unit: Keep/classify True or remove False units of measurement. Ex - inches / ft / cm - height / width / weight / ht / wt - temperature / \u00b0F / \u00b0C quantifier: Keep/classify True size specifying words or to remove False . Ex. - all / any / more preposition: Keep/classify True preposition words or to remove False . Ex. - to / from / above conjunction: Keep/classify True conjunction words or to remove False . Ex. - and / or / but article: keep/classify True article words or to remove False . Ex. - a / an / the demonstrative: Keep/classify True article words or to remove False . Ex. - this / that / these question: Keep/classify True question words or to remove False . Ex. - who / want / how pronoun: Keep/classify True pronoun words or to remove False . Ex. - he / she / them date: Keep/classify/reduce True dates or to remove False . Ex - Jan. 1, 2000 / 01/01/2000 / 2000-01-01 dob: Keep/classify/reduce True date of births or to remove False . Ex. - DOB: Jan 1, 2000 / date of birth is 01-02-2012 ssn: Keep/classify True social security numbers or to remove False . Ex. - 123-84-1234 / 123 84 1234 telephone: Keep/classify True telephone numbers or to remove False . Ex. - (360) 123-1234 / +13601231234 name: Keep/classify True telephone numbers or to remove False . Ex. - Albert Einstein / Donald J. Trump address: Keep/classify/reduce street address or to remove False . Ex - 124 NE 34th Cir, Home Town, AZ, 99123 gender: Keep/classify True gender specifying words or remove False . Ex. - male / man / gal / mom sentiment: Keep/classify/reduce True sentiment word sequences or remove False . - not bad / disgusting","title":"Parameters"},{"location":"specs/syntax_spec/#exceptions","text":"A TypeError is raised if the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/syntax_spec/#13-words-properties","text":"","title":"1.3 Words Properties"},{"location":"specs/syntax_spec/#131-text","text":"","title":"1.3.1 text"},{"location":"specs/syntax_spec/#synopsis_1","text":"# Getter text = images.text # Setter images.text = text","title":"Synopsis"},{"location":"specs/syntax_spec/#usage","text":"When used as a getter the property returns the original text. When used as a setter the property re-preprocesses the text into machine learning ready data.","title":"Usage"},{"location":"specs/syntax_spec/#exceptions_1","text":"A TypeError is raised if the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/syntax_spec/#132-bare","text":"","title":"1.3.2 bare"},{"location":"specs/syntax_spec/#synopsis_2","text":"# Getter tokens = images.bare","title":"Synopsis"},{"location":"specs/syntax_spec/#usage_1","text":"When used as a getter the property returns the NLP tokenized list unprocessed. All punctuation, words, capitalization and diacritic characters and script are preserved. The tokenized list is in a dictionary format of the form: [ { 'word': word1, 'tag': tag }, {'word': word2, 'tag': tag } .. ] Except for numbers and acronyms, the tag values are set to untagged (0).","title":"Usage"},{"location":"specs/syntax_spec/#133-words","text":"","title":"1.3.3 words"},{"location":"specs/syntax_spec/#synopsis_3","text":"# Getter words = images.words","title":"Synopsis"},{"location":"specs/syntax_spec/#usage_2","text":"When used as a getter the property returns the NLP tokenized list according to the specified parameters. The tokenized list is in dictionary format of the form, when the parameter pos is False: [ { 'word': word1, 'tag': tag }, {'word': word2, 'tag': tag } .. ] Otherwise, when the pos parameter is set to True: [ { 'word': word1, 'tag': tag, 'pos': POS }, {'word': word2, 'tag': tag, 'pos': POS } .. ]","title":"Usage"},{"location":"specs/syntax_spec/#134-bagofwords","text":"","title":"1.3.4 bagOfWords"},{"location":"specs/syntax_spec/#synopsis_4","text":"# Getter bag = images.bagOfWords","title":"Synopsis"},{"location":"specs/syntax_spec/#usage_3","text":"When used as a getter the property returns the word sequence as a Bag of Words, represented as a unordered dictionary, where the key is the word and the value is the number of occurrences: { '<word'> : <no. of occurrences>, \u2026 }","title":"Usage"},{"location":"specs/syntax_spec/#135-freqdist","text":"","title":"1.3.5 freqDist"},{"location":"specs/syntax_spec/#synopsis_5","text":"# Getter freq = images.freqDist","title":"Synopsis"},{"location":"specs/syntax_spec/#usage_4","text":"When used as a getter the property returns the sorted tuples of a frequency distribution of words (from bag of words), in descending order (i.e., highest first) [ ( '<word'>: <no. of occurrences> ), \u2026. ]","title":"Usage"},{"location":"specs/syntax_spec/#135-termfreq","text":"","title":"1.3.5 termFreq"},{"location":"specs/syntax_spec/#synopsis_6","text":"# Getter tf = images.termFreq","title":"Synopsis"},{"location":"specs/syntax_spec/#usage_5","text":"When used as a getter the property returns the sorted tuples of a term frequency distribution (percent that term occurs), in descending order (i.e., highest first) [ ( '<word'>: <percentage of occurrences> ), \u2026. ]","title":"Usage"},{"location":"specs/syntax_spec/#136-static-variables","text":"The Words class contains the following static variables: DECIMAL \u2013 The decimal point (US Standard: period , EU: comma) THOUSANDS \u2013 The thousandths unit separator (US Standard comma , EU period)","title":"1.3.6 Static Variables"},{"location":"specs/syntax_spec/#14-words-overridden-operators","text":"","title":"1.4  Words Overridden Operators"},{"location":"specs/syntax_spec/#141-len","text":"","title":"1.4.1  len()"},{"location":"specs/syntax_spec/#synopsis_7","text":"nwords = len(words)","title":"Synopsis"},{"location":"specs/syntax_spec/#usage_6","text":"The len() (__len__) operator is overridden to return the number of NLP tokenized words.","title":"Usage"},{"location":"specs/syntax_spec/#142","text":"","title":"1.4.2  +="},{"location":"specs/syntax_spec/#synopsis_8","text":"words += text","title":"Synopsis"},{"location":"specs/syntax_spec/#usage_7","text":"The += (__iadd__) method is overridden to add words to the sequenced word list (append).","title":"Usage"},{"location":"specs/syntax_spec/#15-words-private-methods","text":"The Words class contains the following private methods, which are called by the initializer: _split() \u2013 This method performs the first phase of NLP preprocessing of the raw text into a sequenced list of words (bare processing mode). Contractions are expanded (e.g., can't => can not). Newlines, carriage returns and tabs removed. Duplicated whitespace is removed. Text is split into words and punctuation. Punctuation is removed (except in numerical and date representations when property number and/or date and/or dob is True). \u2003 _preprocess() - This method performs the second of NLP preprocessing of the 'bare' tokenized words. Identify acronyms. Identify proper names. Words are lowercased. Optionally words are Romanized, if roman attribute is set to True. _stem() \u2013 This method performs the third phase of NLP preprocessing of the tokenized words by removing word endings and reducing word to its root stem (e.g., rider -> ride). Remove plural endings (e.g., flies -> fly). Remove past tense endings (e.g., baked -> bake). Remove present participle endings (e.g., eating -> eat). Remove verb to noun and comparative endings (e.g., rider -> ride, taller-> tall). Remove noun to verb endings (e.g., flatten -> flat). Remove adjective to adverb endings (e.g, costly -> cost). Remove superlative endings (e.g., greatest -> great). Spell check/replacement (according to the specified language), if enabled, occurs prior to stemming. _nltkStem() \u2013 This method uses the open source NLTK stemmer methods to perform the third phase of NLP preprocessing of the tokenized words, as an alternative to the internal stemmer (i.e. stem)). The Porter, Snowball, Lancaster and WordNetLemmatizer are selectable. _stopwords() \u2013 This method performs the fourth phase of NLP preprocessing of the tokenized words by removing/keeping stop words. Remove word (including infinity) and numeric representations of numbers, unless property number is True, then all numbers are retained. If retained, EU decimal and thousandths unit separators converted to US standard. +/- signs preserved. Thousandths unit separator removed. Hex numbers (starting with 0x prefix) are converted to integer value. Text represented numbers (e.g., ten) are converted to integer value. Text represented numeric ordering (e.g., 1st) are converted to integer value. Fractions are converted to floating point value. Remove units of measurement, unless property unit is True. US Standard and Metric, including abbreviations, are recognized. US and EU spelling of metric units are recognized. Remove dates, unless property date is True. Remove date of birth, unless property dob is True. Remove USA social security numbers, unless property ssn is True, where the SSN number is converted to a 9 digit value. Remove USA/CA telephone numbers, unless property telephone is True, where the telephone number is converted to a 10 digit number. Remove USA/CA addresses, unless property address is True, where addresses are converted to the USPO addressing standard. Remove gender indicating words (e.g., man) \u2013 inclusive of transgender, unless property telephone is True. Remove proper names and titles (e.g., Dr.), unless property name is True. Remove quantifier indicating words (e.g., all, any), unless property quantifier is True. Remove prepositions (e.g., above, under), unless property preposition is True. Remove conjunctions (e.g., and, or), unless property conjunction is True. Remove articles (e.g., a, an), unless property article is True. Remove demonstratives (e.g., this, that), unless property demonstrative is True. Remove pronouns (e.g., his, her), unless property pronoun is True. Remove question words (e.g., what, why), unless property question is True. Remove common high frequency words i.e., Porter List). Remove sentiment sequence (e.g., good, bad), unless sentiment property is True. Sequence (e.g., not bad) replaced with \"positive\" or \"negative\". Remove punctuation and symbols, unless punct property is True. _isdate() \u2013 This method is a support method for _stopwords(). It will recognize date strings and convert them to ISO 8601 format. The following formats are recognized: MM/DD/YY and MM/DD/YYYY MM-DD-YY and MM-DD-YYYY YYYY-MM-DD (ISO 8601) Month Day, Year (e.g., January 6, 2016) Abbr-Month Day, Year (e.g., Jan 6, 2016 and Jan. 6, 2016) If the preceding word is birth or DOB, then the date will be tagged as a date of birth (vs. date). _isnumber() - This method is a support method for _stopwords(). It will recognize numerical sequences and convert them to decimal base 10 format. The following formats are recognized Base 10 integer, floating point, exponent, fraction Base 16 hex integers _isSSN() - This method is a support method for _stopwords(). It will recognize USA Social Security numbers. The following formats are recognized: Prefixed with SSN or Soc. Sec. No. or Social Security Number Number Format: 12-123-1234 / 12 123 1234 / 121231234 _isTele() - This method is a support method for _stopwords(). It will recognize USA/CA Telephone numbers. The following formats are recognized: Prefixed with Tele, Phone, Mobile, Office, etc, optionally followed by Number, Num, No, # Number Format: 1231231234 / 123 123 1234 / 123-123-123 / (360) 123-1234 / \u2026 _isAddr() \u2013 This method is a support method for _stopwords() . It will recognize USA/CA postal addresses. The following formats are recognized: [POB[,]] Street-Number [Street-Direction] Street-Name [Street-Type] [Street-Direction] [,] [POB[,]] [Secondary-Address[,]][City[,]State] POB[,] City State _streetnum() \u2013 This method supports the _isAddr() method in recognizing street numbers. The following formats are recognized: [(N|S|W|E)]digits[letter][-][digits|letter Ex. N1300 / 123-33 / 33A / 33 _streetdir() \u2013 This method supports the _isAddr() method in recognizing directional phrases. The following formats are recognized: North|South[sp][West|East] N|S[.][w|e][.] _citystate() \u2013 This method supports the _isAddr() method in recognizing city/state references in a postal address. The following formats are recognized: City , USA and Canadian state names are replaced with their ISO 3166-2 codes (e.g., Alabama => ISO3166-2:US-AL). _pob() \u2013 This method supports the _isAddr() method in recognizing USA and Canadian Post Office Boxes and Private Mail Boxes in street addresses. The following formats are recognized: ( P.O.B | POB | P.O. Box | P.O. | PO ) digits [ (STN | RPO) words ] ( P.M.B | PMB | P.M. Box ) digits _streetaddr2() \u2013 This method supports the _isAddr() method in recognizing secondary address components in street addresses. The following formats are recognized: (Apt|Ste|Rm|Fl|Bldg|Dept) # _postalcode() \u2013 This method supports the _isAddr method() in recognizing USA and Canadian postal codes. The following formats are recognized: 5digits[-4digits] # USA [3letters][sp][3letters] # Canada _isGender() \u2013 This method supports gender recognition in stopwords. It recognizes phrases: (Sex|Gender)[:] (M|F|Male|Female) _conversion() \u2013 This method performs the fifth phase of NLP preprocessing of the tokenized words of converting Standard to Metric (standard=True) and vice-versa (metric=True). _partsofspeech() \u2013 This method performs the sixth phase of NLP preprocessing of the tokenized words of tagging words with their parts of speech tag (using NLTK).","title":"1.5 Words Private Methods"},{"location":"specs/syntax_spec/#15-words-public-methods","text":"The Words class contains no public methods.","title":"1.5 Words Public Methods"},{"location":"specs/syntax_spec/#appendix-i-updates","text":"Pre-Gap (Epipog) v1.1 + Refactored Stopword Removal for Parts of Speech, Numbers and Dates + Added Vocabulary class and Lemmatizing + Added support for Date of Birth + Added support for recognizing word version of numbers. + Fixed handling of Hex numbers Pre-Gap (Epipog) v1.2 + Added support for Social Security Numbers + Added support for Telephone Numbers + Added support for Proper Names + Fix not recognizing Acronym if first word + Refactored tokenization and added bare mode. Pre-Gap (Epipog) v1.3 + Added support for Spanish punctuation: \u00bf\u00a1 + Added support for numeric multipliers (e.g., 10 million). + Added support for unit of measurements. + Added conversion of unit of measurements between Standard and Metric. + Added support for Sex/Gender[:] M/F + Added support for USA street addresses + Fix not recognizing single letter abbreviations. + Fixed not recognizing title (name) proceeded by a comma. + Fixed not recognizing number followed by unit of measurement when combined (e.g., 2cm). Pre-Gap (Epipog) v1.4 + Added NLTK options for stemming, lemmatization and parts of speech. + Change outputting of US State names to ISO 3166-2 standard. + Added support for PMB in postal address. + Added is/of separator between key and value (e.g., SSN is XXX-XX-XXXX). + Added support for Canadian Street/Postal addresses. + Added support for Romanizing latin-1 character encodings into ASCII. + Added support for measurements Pre-Gap (Epipog) v1.5 + Added Bag of Words, Word Frequency Distribution and Term Frequency (TF) Gap v0.9.1 (alpha) + Rewrote Specification. + Added spell check/replacement + Added UK to US spelling correction Gap v0.9.2 (alpha) + Extend Spell Checking to Spanish and French.","title":"APPENDIX I: Updates"},{"location":"specs/syntax_spec/#appendix-ii-anticipated-engineering","text":"The following has been identified as enhancement/issues to be addressed in subsequent update: 1. Support for detecting Abbreviations 2. Support for email addresses 3. Add support for mail stop Secondary Street Address components. 4. Add support for Currency. 5. Add and annotation. 6. Fix lose next word after street/postal address Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"APPENDIX II: Anticipated Engineering"},{"location":"specs/vision_spec/","text":"Gap Framework - Computer Vision for Image Data VISION MODULE High Precision Image Processing Technical Specification, Gap v0.9.2 1 Images 1.1 Images Overview The Images CV preprocessor contains the following primary classes, and their relationships: Images - This is the base class for the representation of a Computer Vision (CV) preprocessed list of images. The constructor optionally takes as parameters a list of images (paths), and corresponding labels, and flags for CV preprocessing the images. images = Images([<list of images>], [<list_of_labels>], flags \u2026) Alternately, the list of images can be a list of directories which contain images. Alternately, the list of labels maybe a single value; in which case, the label applies to all the images. Image \u2013 This is the base class for the representation of a single Computer Vision (CV). The constructor optionally takes as parameters an image (path), corresponding label, and flags for CV preprocessing the image. Fig. 1a High Level view of Images Class Object Relationships 1.2 Images Initializer (Constructor) Synopsis Images( images=None, labels= None, dir=\u2019./\u2019, name=None, ehandler=None, config=None) Parameters images: If not None, a list of either: 1. local image files 2. remote image files (i.e., http[s]://\u2026.) 3. directories of local image files. labels: If not None, either: 1. A single integer value (i.e., label) which corresponds to all the images. 2. A list of the same size as images parameter list of integer values; where the index of each value is the label for the corresponding index in the images parameter. dir: The directory where to store the machine learning ready data. name: If not None, a name (string) for the collection. ehandler: If not None, the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(images): # Where images is the Images object that was preprocessed. config: If not None, a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width Usage When specified with no parameters, an empty Images object is created. The Images object may then be used to subsequent load (retrieve) previously stored preprocessed machine learning ready data (see load() ). Otherwise, both images and labels parameters must be specified. The labels parameter corresponds to the labels of the images. Each image specified by the images parameter will be preprocessed according to the optional parameters and configuration settings. By default, the images will be preprocessed as follows: An Image object is created for each image. The config parameter will have the \u2018nostore\u2019 setting, which instructs each image object to not separately store the generated preprocessed machine learning ready data. Upon completion, the preprocessed machine learning data for each image is stored as a single HDF5 file in the current working directory. The root name of the file will be the root name of the first image. If either or both the dir and config options are not None, they are passed down to each Image object. If the name parameter is specified, the value will be the root name of the HDF5 stored file. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Images object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A IndexError is raised if the size of the labels list does not match the size of the images list. 1.3 Images Properties 1.3.1 dir Synopsis # Getter path = images.dir # Setter images.dir = path Usage When used as a getter, the property returns the path where the HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the HDF5 file is found. Otherwise, it is ignored. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist. 1.3.2 name Synopsis # Getter collection = images.name Usage When used as a getter the property returns the root name of the HDF5 stored file (also referred to as the name of the collection). 1.3.3 images Synopsis # Getter images = images.images Usage When used as a getter the property returns the list of Image objects generated for the collection. 1.3.4 labels Synopsis # Getter labels = images.labels Usage When used as a getter the property returns the label or list of labels for the collection. 1.3.5 time Synopsis # Getter secs = images.time Usage When used as a getter the property returns the amount of time (in seconds) it took to preprocess the collection into machine learning ready data. \u2003 1.3.6 split Synopsis # Getter x_train, x_test, y_train, y_test = images.split # Setter images.split = percent [,seed] Usage When used as a setter, a training and test set is generated. The percent parameter specifies the percent that is training data. The data is first randomized before the split. By default, the seed for the split is 0. A seed may be optional specified as a second value. When repeated, the property will re-split the data and re-randomize it. When used as a getter, the split training, test, and corresponding labels are returned as lists. This is typically used in conjunction with next() operator or minibatch property. When the percent is 0, the data is not split. All the data will be returned in x_train and y_train , but will still be randomized; x_test and y_test will be None . Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if a parameter is out of range. A AttributeError is raised if the number of parameters passed to the setter property is incorrect. 1.3.7 minibatch Synopsis # Getter generator = images.minibatch # Setter images.minibatch = batch_size Usage When used as a setter, the [mini] batch size is set. When used as a getter, a generator is returned. The generator will iterate sequentially through the minibatches of the training set. If the property augment is set to True, for each image in the training set, an additional image is generated by rotating the image a random value between -90 and 90 degrees. Thus, if the mnibatch size is 100 images, the minibatch getter will build a generator for 200 images. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the batch_size is out of range. 1.3.8 augment Synopsis # Getter augment = images.augment # Setter images.augment = augment Usage When used as a setter and set to True, image augmentation is enabled during batch generation (see minibatch and next() ). When used as a getter, the property returns whether image augmentation is enabled. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 1.4 Images Overridden Operators 1.4.1 len() Synopsis n_images = len(images) Usage The len() (__len__) operator is overridden to return the number of Image objects in the collection. 1.4.2 [] Synopsis image = images[n] Usage The [] (__getitem__) operator is overridden to return the Image object at the specified index. Exceptions A IndexError is raised if the index is out of range. 1.4.3 next() Synopsis data, label = next(images) Usage The next() operator is overridden and is used in conjunction with the split property. Once the collection has been split in training and test data, the next() operator will iterate through the training dataset one image, and corresponding label at a time. Once the training set has been fully iterated, the next() operator returns None , None and will reset and start with the first element. Additionally, the training set will be randomly reshuffled. If the augment property is set, for each image in the training set, an additional image is generated by rotating the image a random value between -90 and 90 degrees. Thus, if the training set is 1000 images, the next() operator will iterate through 2000 images. \u2003 1.5 Images Public Methods Synopsis images.load(name, dir=None) Parameters name: The name of the collection. Usage This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the collection name. The method will load the HDF5 by the filename .h5. If dir is None, then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Images object will have the same characteristics as when the Images object was created. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None. \u2003 2 Image 2.1 Image Overview The Image CV preprocessor contains the following primary classes, and their relationships: Image - This is the base class for the representation of a Computer Vision (CV) preprocessed image. The constructor optionally takes as parameters an image (path), and corresponding label, and flags for CV preprocessing of the image. image = Image(<image_path>, <label>, flags \u2026) The image path maybe a local path or an URL to a remote location, or raw pixel data as a numpy array. For remote location, a HTTP request is made to obtain the image data. 2.2 Image Initializer (Constructor) Synopsis Image( image=None, label= 0, dir=\u2019./\u2019, ehandler=None, config=None) Parameters image: If not None, a string of either: 1. local path to an image file 2. remote location of an image file (i.e., http[s]://\u2026.) 3. raw pixel data as a numpy array label: An integer value which is the label corresponding to the image. dir: The directory where to store the machine learning ready data. ehandler: If not None, the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(image, dir): # Where image is the Image object that was preprocessed. config: If not None, a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width nostore Usage When specified with no parameters, an empty Image object is created. The Image object may then be used to subsequent load previously stored preprocessed machine learning ready data (see load() ). Otherwise, both image and label parameters must be specified. The label parameter corresponds to the label of the image. The image specified by the image option will be preprocessed according to the optional parameters and configuration settings. By default, the image will be preprocessed as follows: Decompressed into raw pixel data. Converted to RGB, if not already. The pixel values are normalized (i.e., pixel integer values 0..255 converted to floating point values between 0 and 1). Upon completion, the raw pixel data and the preprocessed machine learning data for the image is stored as a single HDF5 file in the current working directory. The root name of the file will be the root name of the image. Attributes of the raw and preprocessed image are stored in the HDF5 file. If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. If the parameter dir is specified, then the generated HDF5 file is stored in the specified directory. If the directory does not exist, it is created. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Image object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. If the configuration setting grayscale (may be shortened to gray) is specified, then the image is converted to a single channel grayscale image, if not already. If the configuration setting resize is specified, then the image is resized to the specified height and width. If the configuration setting flatten (may be shortened to flat) is specified, the image is flattened into a single 1D vector (i.e., for input to a ANN). If the configuration setting thumb is specified, then a thumbnail of the raw pixel data is generated to the specified height and width and stored in the HDF5 file. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A FileNotFoundError is raised if the image file does not exist. A IOError is raised if an error occurs reading in the image file. 2.3 Image Properties 2.3.1 image Synopsis # Getter path = image.image # Setter image.image = path Usage When used as a getter the property returns the path to the image file. When used as a setter the property specifies the path of the image file to preprocess into machine learning ready data (see initializer). Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the image file does not exist. A IOError is raised if an error occurs reading in the image file. 2.3.2 name Synopsis # Getter root = image.name Usage When used as a getter the property returns the root name of the image file(e.g., /mydir/myimage.jpg -> myimage). If the input was raw pixel data, the name property will return \u2018untitled\u2019. 2.3.3 type Synopsis # Getter suffix = image.type Usage When used as a getter the property returns the file suffix of the image file (e.g., jpg). If the input was raw pixel data, the property will return \u2018raw\u2019. 2.3.4 size Synopsis # Getter size = image.size Usage When used as a getter the property returns the file size of the image file in bytes. 2.3.5 raw Synopsis # Getter pixels = image.raw Usage When used as a getter the property returns the raw pixel data of the uncompressed image. 2.3.6 thumb Synopsis # Getter pixels = image.thumb ###### Usage When used as a getter the property returns the pixel data for the thumbnail image. 2.3.7 label Synopsis # Getter label = image.label # Setter image.label = label Usage When used as a getter the property returns the (integer) label specified for the image. When used as a setter the property sets the label of the image to the specified integer value. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 2.3.7 dir Synopsis # Getter subfolder = image.dir # Setter image.dir = subfolder Usage When used as a getter the property returns the directory path where the corresponding HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the HDF5 file is found. Otherwise, it is ignored. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist. 2.3.8 data Synopsis # Getter data = image.data Usage When used as a getter the property returns the preprocessed machine learning ready data. 2.3.9 shape Synopsis # Getter shape = image.shape Usage When used as a getter the property returns the shape of the preprocessed machine learning ready data (e.g., (50, 50, 3)). \u2003 2.3.10 time Synopsis # Getter secs = image.time Usage When used as a getter the property returns the amount of time (in seconds) it took to preprocess the image into machine learning ready data. 2.4 Image Overridden Operators 2.4.1 str() Synopsis label = str(image) Usage The str() (__str__) operator is overridden to return the label of the image as a string. 2.5 Image Public Methods 2.5.1 load() Synopsis image.load(name, dir=None) Parameters name: The filename of the stored HDF5 file. dir: The directory where the HDF5 file is located. Usage This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the parameter name. The method will load the HDF5 by the filename .h5. If dir is None, then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Image object will have the same characteristics as when the Image object was created. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None. 2.5.2 rotate() Synopsis image.rotate(degree) Parameters degree: The degree (angle) to rotate the image data. Usage This method generates a rotated copy of the raw image data. The parameter degree specifies the degree (angle) to rotate the image. The method uses the imutils module which will resize the image to prevent clipping prior to the rotation. Once rotated, the image is resized back to the target size. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the degree is not between 0 and 360. APPENDIX I: Updates Pre-Gap (Epipog) v1.5 1. Created first instance of module Gap v0.9 (alpha) 1. Added splitting collection into training and test data 2. Added iterating (next) through the training set 3. Added support for minibatch Gap v0.91 (alpha) 1. Added support for Images to take list of directories of images. 2. Added support for Image for image path is an URL (http request). 3. Added image rotation. 4. Rewrote Specification. 5. Added support for Images for image parameters to be folders of images. 6. Added support for GIF. 7. Added support for image augmentation in next() /minibatch. 8. Added support for raw pixel input to Image class. APPENDIX II: Anticipated Engineering The following has been identified as enhancement/issues to be addressed in subsequent update: 1. Add transformations Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"Vision"},{"location":"specs/vision_spec/#gap-framework-computer-vision-for-image-data","text":"","title":"Gap Framework - Computer Vision for Image Data"},{"location":"specs/vision_spec/#vision-module","text":"High Precision Image Processing Technical Specification, Gap v0.9.2","title":"VISION MODULE"},{"location":"specs/vision_spec/#1-images","text":"","title":"1  Images"},{"location":"specs/vision_spec/#11-images-overview","text":"The Images CV preprocessor contains the following primary classes, and their relationships: Images - This is the base class for the representation of a Computer Vision (CV) preprocessed list of images. The constructor optionally takes as parameters a list of images (paths), and corresponding labels, and flags for CV preprocessing the images. images = Images([<list of images>], [<list_of_labels>], flags \u2026) Alternately, the list of images can be a list of directories which contain images. Alternately, the list of labels maybe a single value; in which case, the label applies to all the images. Image \u2013 This is the base class for the representation of a single Computer Vision (CV). The constructor optionally takes as parameters an image (path), corresponding label, and flags for CV preprocessing the image. Fig. 1a High Level view of Images Class Object Relationships","title":"1.1  Images Overview"},{"location":"specs/vision_spec/#12-images-initializer-constructor","text":"","title":"1.2  Images Initializer (Constructor)"},{"location":"specs/vision_spec/#synopsis","text":"Images( images=None, labels= None, dir=\u2019./\u2019, name=None, ehandler=None, config=None)","title":"Synopsis"},{"location":"specs/vision_spec/#parameters","text":"images: If not None, a list of either: 1. local image files 2. remote image files (i.e., http[s]://\u2026.) 3. directories of local image files. labels: If not None, either: 1. A single integer value (i.e., label) which corresponds to all the images. 2. A list of the same size as images parameter list of integer values; where the index of each value is the label for the corresponding index in the images parameter. dir: The directory where to store the machine learning ready data. name: If not None, a name (string) for the collection. ehandler: If not None, the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(images): # Where images is the Images object that was preprocessed. config: If not None, a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width","title":"Parameters"},{"location":"specs/vision_spec/#usage","text":"When specified with no parameters, an empty Images object is created. The Images object may then be used to subsequent load (retrieve) previously stored preprocessed machine learning ready data (see load() ). Otherwise, both images and labels parameters must be specified. The labels parameter corresponds to the labels of the images. Each image specified by the images parameter will be preprocessed according to the optional parameters and configuration settings. By default, the images will be preprocessed as follows: An Image object is created for each image. The config parameter will have the \u2018nostore\u2019 setting, which instructs each image object to not separately store the generated preprocessed machine learning ready data. Upon completion, the preprocessed machine learning data for each image is stored as a single HDF5 file in the current working directory. The root name of the file will be the root name of the first image. If either or both the dir and config options are not None, they are passed down to each Image object. If the name parameter is specified, the value will be the root name of the HDF5 stored file. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Images object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location.","title":"Usage"},{"location":"specs/vision_spec/#exceptions","text":"A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A IndexError is raised if the size of the labels list does not match the size of the images list.","title":"Exceptions"},{"location":"specs/vision_spec/#13-images-properties","text":"","title":"1.3  Images Properties"},{"location":"specs/vision_spec/#131-dir","text":"","title":"1.3.1  dir"},{"location":"specs/vision_spec/#synopsis_1","text":"# Getter path = images.dir # Setter images.dir = path","title":"Synopsis"},{"location":"specs/vision_spec/#usage_1","text":"When used as a getter, the property returns the path where the HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the HDF5 file is found. Otherwise, it is ignored.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_1","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist.","title":"Exceptions"},{"location":"specs/vision_spec/#132-name","text":"","title":"1.3.2  name"},{"location":"specs/vision_spec/#synopsis_2","text":"# Getter collection = images.name","title":"Synopsis"},{"location":"specs/vision_spec/#usage_2","text":"When used as a getter the property returns the root name of the HDF5 stored file (also referred to as the name of the collection).","title":"Usage"},{"location":"specs/vision_spec/#133-images","text":"","title":"1.3.3  images"},{"location":"specs/vision_spec/#synopsis_3","text":"# Getter images = images.images","title":"Synopsis"},{"location":"specs/vision_spec/#usage_3","text":"When used as a getter the property returns the list of Image objects generated for the collection.","title":"Usage"},{"location":"specs/vision_spec/#134-labels","text":"","title":"1.3.4  labels"},{"location":"specs/vision_spec/#synopsis_4","text":"# Getter labels = images.labels","title":"Synopsis"},{"location":"specs/vision_spec/#usage_4","text":"When used as a getter the property returns the label or list of labels for the collection.","title":"Usage"},{"location":"specs/vision_spec/#135-time","text":"","title":"1.3.5  time"},{"location":"specs/vision_spec/#synopsis_5","text":"# Getter secs = images.time","title":"Synopsis"},{"location":"specs/vision_spec/#usage_5","text":"When used as a getter the property returns the amount of time (in seconds) it took to preprocess the collection into machine learning ready data.","title":"Usage"},{"location":"specs/vision_spec/#136-split","text":"","title":"1.3.6  split"},{"location":"specs/vision_spec/#synopsis_6","text":"# Getter x_train, x_test, y_train, y_test = images.split # Setter images.split = percent [,seed]","title":"Synopsis"},{"location":"specs/vision_spec/#usage_6","text":"When used as a setter, a training and test set is generated. The percent parameter specifies the percent that is training data. The data is first randomized before the split. By default, the seed for the split is 0. A seed may be optional specified as a second value. When repeated, the property will re-split the data and re-randomize it. When used as a getter, the split training, test, and corresponding labels are returned as lists. This is typically used in conjunction with next() operator or minibatch property. When the percent is 0, the data is not split. All the data will be returned in x_train and y_train , but will still be randomized; x_test and y_test will be None .","title":"Usage"},{"location":"specs/vision_spec/#exceptions_2","text":"A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if a parameter is out of range. A AttributeError is raised if the number of parameters passed to the setter property is incorrect.","title":"Exceptions"},{"location":"specs/vision_spec/#137-minibatch","text":"","title":"1.3.7  minibatch"},{"location":"specs/vision_spec/#synopsis_7","text":"# Getter generator = images.minibatch # Setter images.minibatch = batch_size","title":"Synopsis"},{"location":"specs/vision_spec/#usage_7","text":"When used as a setter, the [mini] batch size is set. When used as a getter, a generator is returned. The generator will iterate sequentially through the minibatches of the training set. If the property augment is set to True, for each image in the training set, an additional image is generated by rotating the image a random value between -90 and 90 degrees. Thus, if the mnibatch size is 100 images, the minibatch getter will build a generator for 200 images.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_3","text":"A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the batch_size is out of range.","title":"Exceptions"},{"location":"specs/vision_spec/#138-augment","text":"","title":"1.3.8  augment"},{"location":"specs/vision_spec/#synopsis_8","text":"# Getter augment = images.augment # Setter images.augment = augment","title":"Synopsis"},{"location":"specs/vision_spec/#usage_8","text":"When used as a setter and set to True, image augmentation is enabled during batch generation (see minibatch and next() ). When used as a getter, the property returns whether image augmentation is enabled.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_4","text":"A TypeError is raised if the type of the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/vision_spec/#14-images-overridden-operators","text":"","title":"1.4  Images Overridden Operators"},{"location":"specs/vision_spec/#141-len","text":"","title":"1.4.1  len()"},{"location":"specs/vision_spec/#synopsis_9","text":"n_images = len(images)","title":"Synopsis"},{"location":"specs/vision_spec/#usage_9","text":"The len() (__len__) operator is overridden to return the number of Image objects in the collection.","title":"Usage"},{"location":"specs/vision_spec/#142","text":"","title":"1.4.2  []"},{"location":"specs/vision_spec/#synopsis_10","text":"image = images[n]","title":"Synopsis"},{"location":"specs/vision_spec/#usage_10","text":"The [] (__getitem__) operator is overridden to return the Image object at the specified index.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_5","text":"A IndexError is raised if the index is out of range.","title":"Exceptions"},{"location":"specs/vision_spec/#143-next","text":"","title":"1.4.3  next()"},{"location":"specs/vision_spec/#synopsis_11","text":"data, label = next(images)","title":"Synopsis"},{"location":"specs/vision_spec/#usage_11","text":"The next() operator is overridden and is used in conjunction with the split property. Once the collection has been split in training and test data, the next() operator will iterate through the training dataset one image, and corresponding label at a time. Once the training set has been fully iterated, the next() operator returns None , None and will reset and start with the first element. Additionally, the training set will be randomly reshuffled. If the augment property is set, for each image in the training set, an additional image is generated by rotating the image a random value between -90 and 90 degrees. Thus, if the training set is 1000 images, the next() operator will iterate through 2000 images.","title":"Usage"},{"location":"specs/vision_spec/#15-images-public-methods","text":"","title":"1.5  Images Public Methods"},{"location":"specs/vision_spec/#synopsis_12","text":"images.load(name, dir=None)","title":"Synopsis"},{"location":"specs/vision_spec/#parameters_1","text":"name: The name of the collection.","title":"Parameters"},{"location":"specs/vision_spec/#usage_12","text":"This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the collection name. The method will load the HDF5 by the filename .h5. If dir is None, then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Images object will have the same characteristics as when the Images object was created.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_6","text":"A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None.","title":"Exceptions"},{"location":"specs/vision_spec/#2-image","text":"","title":"2  Image"},{"location":"specs/vision_spec/#21-image-overview","text":"The Image CV preprocessor contains the following primary classes, and their relationships: Image - This is the base class for the representation of a Computer Vision (CV) preprocessed image. The constructor optionally takes as parameters an image (path), and corresponding label, and flags for CV preprocessing of the image. image = Image(<image_path>, <label>, flags \u2026) The image path maybe a local path or an URL to a remote location, or raw pixel data as a numpy array. For remote location, a HTTP request is made to obtain the image data.","title":"2.1  Image Overview"},{"location":"specs/vision_spec/#22-image-initializer-constructor","text":"","title":"2.2  Image Initializer (Constructor)"},{"location":"specs/vision_spec/#synopsis_13","text":"Image( image=None, label= 0, dir=\u2019./\u2019, ehandler=None, config=None)","title":"Synopsis"},{"location":"specs/vision_spec/#parameters_2","text":"image: If not None, a string of either: 1. local path to an image file 2. remote location of an image file (i.e., http[s]://\u2026.) 3. raw pixel data as a numpy array label: An integer value which is the label corresponding to the image. dir: The directory where to store the machine learning ready data. ehandler: If not None, the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(image, dir): # Where image is the Image object that was preprocessed. config: If not None, a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width nostore","title":"Parameters"},{"location":"specs/vision_spec/#usage_13","text":"When specified with no parameters, an empty Image object is created. The Image object may then be used to subsequent load previously stored preprocessed machine learning ready data (see load() ). Otherwise, both image and label parameters must be specified. The label parameter corresponds to the label of the image. The image specified by the image option will be preprocessed according to the optional parameters and configuration settings. By default, the image will be preprocessed as follows: Decompressed into raw pixel data. Converted to RGB, if not already. The pixel values are normalized (i.e., pixel integer values 0..255 converted to floating point values between 0 and 1). Upon completion, the raw pixel data and the preprocessed machine learning data for the image is stored as a single HDF5 file in the current working directory. The root name of the file will be the root name of the image. Attributes of the raw and preprocessed image are stored in the HDF5 file. If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. If the parameter dir is specified, then the generated HDF5 file is stored in the specified directory. If the directory does not exist, it is created. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Image object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. If the configuration setting grayscale (may be shortened to gray) is specified, then the image is converted to a single channel grayscale image, if not already. If the configuration setting resize is specified, then the image is resized to the specified height and width. If the configuration setting flatten (may be shortened to flat) is specified, the image is flattened into a single 1D vector (i.e., for input to a ANN). If the configuration setting thumb is specified, then a thumbnail of the raw pixel data is generated to the specified height and width and stored in the HDF5 file.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_7","text":"A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A FileNotFoundError is raised if the image file does not exist. A IOError is raised if an error occurs reading in the image file.","title":"Exceptions"},{"location":"specs/vision_spec/#23-image-properties","text":"","title":"2.3  Image Properties"},{"location":"specs/vision_spec/#231-image","text":"","title":"2.3.1  image"},{"location":"specs/vision_spec/#synopsis_14","text":"# Getter path = image.image # Setter image.image = path","title":"Synopsis"},{"location":"specs/vision_spec/#usage_14","text":"When used as a getter the property returns the path to the image file. When used as a setter the property specifies the path of the image file to preprocess into machine learning ready data (see initializer).","title":"Usage"},{"location":"specs/vision_spec/#exceptions_8","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the image file does not exist. A IOError is raised if an error occurs reading in the image file.","title":"Exceptions"},{"location":"specs/vision_spec/#232-name","text":"","title":"2.3.2  name"},{"location":"specs/vision_spec/#synopsis_15","text":"# Getter root = image.name","title":"Synopsis"},{"location":"specs/vision_spec/#usage_15","text":"When used as a getter the property returns the root name of the image file(e.g., /mydir/myimage.jpg -> myimage). If the input was raw pixel data, the name property will return \u2018untitled\u2019.","title":"Usage"},{"location":"specs/vision_spec/#233-type","text":"","title":"2.3.3  type"},{"location":"specs/vision_spec/#synopsis_16","text":"# Getter suffix = image.type","title":"Synopsis"},{"location":"specs/vision_spec/#usage_16","text":"When used as a getter the property returns the file suffix of the image file (e.g., jpg). If the input was raw pixel data, the property will return \u2018raw\u2019.","title":"Usage"},{"location":"specs/vision_spec/#234-size","text":"","title":"2.3.4 size"},{"location":"specs/vision_spec/#synopsis_17","text":"# Getter size = image.size","title":"Synopsis"},{"location":"specs/vision_spec/#usage_17","text":"When used as a getter the property returns the file size of the image file in bytes.","title":"Usage"},{"location":"specs/vision_spec/#235-raw","text":"","title":"2.3.5 raw"},{"location":"specs/vision_spec/#synopsis_18","text":"# Getter pixels = image.raw","title":"Synopsis"},{"location":"specs/vision_spec/#usage_18","text":"When used as a getter the property returns the raw pixel data of the uncompressed image.","title":"Usage"},{"location":"specs/vision_spec/#236-thumb","text":"","title":"2.3.6  thumb"},{"location":"specs/vision_spec/#synopsis_19","text":"# Getter pixels = image.thumb ###### Usage When used as a getter the property returns the pixel data for the thumbnail image.","title":"Synopsis"},{"location":"specs/vision_spec/#237-label","text":"","title":"2.3.7  label"},{"location":"specs/vision_spec/#synopsis_20","text":"# Getter label = image.label # Setter image.label = label","title":"Synopsis"},{"location":"specs/vision_spec/#usage_19","text":"When used as a getter the property returns the (integer) label specified for the image. When used as a setter the property sets the label of the image to the specified integer value.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_9","text":"A TypeError is raised if the type of the parameter is not the expected type.","title":"Exceptions"},{"location":"specs/vision_spec/#237-dir","text":"","title":"2.3.7  dir"},{"location":"specs/vision_spec/#synopsis_21","text":"# Getter subfolder = image.dir # Setter image.dir = subfolder","title":"Synopsis"},{"location":"specs/vision_spec/#usage_20","text":"When used as a getter the property returns the directory path where the corresponding HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the HDF5 file is found. Otherwise, it is ignored.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_10","text":"A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist.","title":"Exceptions"},{"location":"specs/vision_spec/#238-data","text":"","title":"2.3.8  data"},{"location":"specs/vision_spec/#synopsis_22","text":"# Getter data = image.data","title":"Synopsis"},{"location":"specs/vision_spec/#usage_21","text":"When used as a getter the property returns the preprocessed machine learning ready data.","title":"Usage"},{"location":"specs/vision_spec/#239-shape","text":"","title":"2.3.9  shape"},{"location":"specs/vision_spec/#synopsis_23","text":"# Getter shape = image.shape","title":"Synopsis"},{"location":"specs/vision_spec/#usage_22","text":"When used as a getter the property returns the shape of the preprocessed machine learning ready data (e.g., (50, 50, 3)).","title":"Usage"},{"location":"specs/vision_spec/#2310-time","text":"","title":"2.3.10  time"},{"location":"specs/vision_spec/#synopsis_24","text":"# Getter secs = image.time","title":"Synopsis"},{"location":"specs/vision_spec/#usage_23","text":"When used as a getter the property returns the amount of time (in seconds) it took to preprocess the image into machine learning ready data.","title":"Usage"},{"location":"specs/vision_spec/#24-image-overridden-operators","text":"","title":"2.4  Image Overridden Operators"},{"location":"specs/vision_spec/#241-str","text":"","title":"2.4.1  str()"},{"location":"specs/vision_spec/#synopsis_25","text":"label = str(image)","title":"Synopsis"},{"location":"specs/vision_spec/#usage_24","text":"The str() (__str__) operator is overridden to return the label of the image as a string.","title":"Usage"},{"location":"specs/vision_spec/#25-image-public-methods","text":"","title":"2.5  Image Public Methods"},{"location":"specs/vision_spec/#251-load","text":"","title":"2.5.1  load()"},{"location":"specs/vision_spec/#synopsis_26","text":"image.load(name, dir=None)","title":"Synopsis"},{"location":"specs/vision_spec/#parameters_3","text":"name: The filename of the stored HDF5 file. dir: The directory where the HDF5 file is located.","title":"Parameters"},{"location":"specs/vision_spec/#usage_25","text":"This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the parameter name. The method will load the HDF5 by the filename .h5. If dir is None, then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Image object will have the same characteristics as when the Image object was created.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_11","text":"A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None.","title":"Exceptions"},{"location":"specs/vision_spec/#252-rotate","text":"","title":"2.5.2 rotate()"},{"location":"specs/vision_spec/#synopsis_27","text":"image.rotate(degree)","title":"Synopsis"},{"location":"specs/vision_spec/#parameters_4","text":"degree: The degree (angle) to rotate the image data.","title":"Parameters"},{"location":"specs/vision_spec/#usage_26","text":"This method generates a rotated copy of the raw image data. The parameter degree specifies the degree (angle) to rotate the image. The method uses the imutils module which will resize the image to prevent clipping prior to the rotation. Once rotated, the image is resized back to the target size.","title":"Usage"},{"location":"specs/vision_spec/#exceptions_12","text":"A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the degree is not between 0 and 360.","title":"Exceptions"},{"location":"specs/vision_spec/#appendix-i-updates","text":"Pre-Gap (Epipog) v1.5 1. Created first instance of module Gap v0.9 (alpha) 1. Added splitting collection into training and test data 2. Added iterating (next) through the training set 3. Added support for minibatch Gap v0.91 (alpha) 1. Added support for Images to take list of directories of images. 2. Added support for Image for image path is an URL (http request). 3. Added image rotation. 4. Rewrote Specification. 5. Added support for Images for image parameters to be folders of images. 6. Added support for GIF. 7. Added support for image augmentation in next() /minibatch. 8. Added support for raw pixel input to Image class.","title":"APPENDIX I: Updates"},{"location":"specs/vision_spec/#appendix-ii-anticipated-engineering","text":"The following has been identified as enhancement/issues to be addressed in subsequent update: 1. Add transformations Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"APPENDIX II: Anticipated Engineering"},{"location":"tutorials/computer_vision/","text":"Computer Vision Introduction Welcome to the labs.earth collaborative laboratory tutorials on machine learning. The computer vision (CV) tutorials will start with the basics and progress to advanced real world applications. The tutorials go beyond explaining the code and steps, to include the answers to the anticipated what and why questions. Before the advent of machine learning with computer vision and today's modern ML/CV frameworks, working with and building real world applications was once the exclusive domain of imaging scientists. The Gap framework extends modern computer vision to software developers, whom are familar with object oriented programming (OOP), object relational models (ORM), design patterns (e.g., MVC), asynchronous programming (AJAX), and microservice architectures. For the data analyst and statisticians whom feel they don't have the necessary software development background, we encourage you to visit the collaborative lab's training site for fundamentials in modern software programming. Likewise, for those software developers whom feel they don't have the necessary background in statistics and machine learning, we encourage you to visit the collaborative lab's training site for fundamentials in modern statistics and machine learning . As far as our team and contributers, they keep a single phrase in mind when designing, coding and building tutorials. They like to say that Gap is: Machine Learning for Humans The First Steps in using Gap for Computer Vision (CV) The first step in using Gap for machine learning (ML) of computer vision (CV) is learning to classify a single object in an image. Is it a dog, a cat, what digit is it, what sign language digit is it, etc... To do single object classification, depending on the images, one will use either a artificial neural network ( ANN ) or a convolutional neural network ( CNN ). In either case, the raw pixel data is not directly inputted into a neural network. Instead, it has to be prepared into machine learning (ML) ready data. How it is prepared/transformed is dependent on the image source, the type and configuration of the neural network, and the target application. Images can come from a variety of sources, such as by your cell phone, images found on the Internet, a facsimile image (FAX) , a video frame from a video stream , a digitized medical/dental x-ray , a magnetic resonance imaging (MRI) , a electron microscopy (TEM) , etc. Images go from very basic like 1-bit BW single channel (color plane) , to 8-bit gray scale single channel , to 8-bit color three channel (RGB) , to 8-bit color four channel (+alpha channel) , to 16-bit high tone (CMYK) , to infrared , to stereoscopic images (3D) , sound navigation and ranging (SONAR) , to RADAR , and more. Fundamentals in Preparing an Image for Machine Learning Neural networks take as input numbers, specifically numbers that are continous real numbers and have been normalized . For images, pixel values are proportionally squashed between 0 and 1. For ANN networks, the inputs need to be a 1D vector, in which case the input data needs to be flatten, while in a CNN, the input is a 2D vector. Neural networks for computer vision take input of fixed sizes, so there is a transformation step to transform the pixel data to the input size and shape of the neural network, and finally assigning a label to the image (e.g., it's a cat). Again, for labels, neural networks use integer numbers; for example a cat must be assigned to a unique integer value and a dog to a different unique integer value. These are the basic steps for all computer vision based neural networks: Transformation Normalization Shaping (e.g., flattening) Labeling Importing Vision module The Vision module of the Gap framework implements the classes and methods for computer vision. Within the Vision module are two primary class objects for data management of images. The Image class manages individual images, while the Images class manages collections of images. As a first step, in your Python script or program you want to import from the Vision module the Image and Images class objects. from vision import Image, Images Preprocessing (Preparing) an image with Gap Relative to the location of this tutorial are a number of test images used in verifying releases of Gap. For the purpose of these tutorials, the images that are part of the Gap release verification will be used for examples. The test file 1_100.jpg is a simple 100x100 96 dpi color image (RGB/8bit) from the Kaggle Fruit360 dataset. This dataset was part of a Kaggle contents to classify different types of fruits and their variety. It was a fairly simple dataset in that all the images were of the same size, type and number of channels. Further, each image contained only the object to classify (i.e., fruit) and was centered in the image. The first step is to instantiate an Image class object and load the image into it, and its corresponding label. In the example below, an Image object is created where the first two positional parameters are the path to the image and the corresponding label (i.e., 1). image = Image(\"../tests/files/1_100.jpg\", 1) While Python does not have OOP polymorphism builtin, the class objects in Gap have been constructed to emulate polymorphism in a variety of ways. The first positional parameter (image path) to the Image class can either be a local path or a remote path. In the latter case, a path starting with http or https is a remote path. In this case, a HTTP request to fetch the image from the remote location is made. image = Image(\"https://en.wikipedia.org/wiki/File:Example.jpg\", 1) Alternately, raw pixel data can be specified as the first (image) positional parameter, as a numpy array. raw = cv2.imread(\"../tests/files/1_100.jpg\") image = Image(raw, 1) Preprocessing of the image in the above examples is synchronous. The initializer (i.e., constructor) returns an image object once the image file has been preprocessed. Alternately, preprocessing of an image can be done asynchronously, where the preprocessing is performed by a background thread. Asynchronous processing occurs if the keyword parameter ehandler is specified. The value of the parameter is set to a function or method, which is invoked with the image object as a parameter when preprocessing of the image is complete. image = Image(\"../tests/files/1_100.jpg\", 1, ehandler=myfunc) def myfunc(image): print(\"done\") The Image class has a number of attributes which are accessed using OOP properties (i.e., getters and setters). The attributes below provide information on the source image: print(image.name) # the name of the image (e.g., 1_100) print(image.type) # the type of the image (e.g., jpg) print(image.size) # the size of the image in bytes (e.g., 3574) print(image.label) # the label assigned to the image (e.g., 1) The raw pixel data of the source image is accessed with the raw property, where property returns the uncompressed pixel data of the source image as a numpy array. raw = image.raw print(type(raw)) # outputs <class 'numpy.ndarry'> print(raw.shape) # outputs the shape of the source image (e.g., (100, 100, 3)) The preprocessed machine learning ready data is accessed with the data property, where the property returns the data as a numpy array. data = image.data print(type(data)) # outputs <class 'numpy.ndarry'> print(data.shape) # outputs the shape of the machine learning data (e.g., (100, 100, 3)) By default, the shape and number of channels of the source image are maintained in the preprocessed machine learning ready data, and the pixel values are normalized to values between 0 and 1. print(raw[0][80]) # outputs pixel values (e.g., [250, 255, 255]) print(data[0][80]) # outputs machine learning ready data values (e.g., [0.98039216, 1.0, 1.0]) When processing of the image is completed, the raw pixel data, machine learning ready data, and attributes are stored in a HDF5 (Hierarchical Data Format) formatted file. By default, the file is stored in the current local directory, where the rootname of the file is the rootname of the image. Storage provides the means to latter retrieval the machine learning ready data for feeding into a neural network, and/or retransforming the machine learning ready data. In the above example, the file would be stored as: ./1_100.hd5 The path location of the stored HDF5 can be specified with the keyword parameter dir . image = Image(\"../tests/files/1_100.jpg\", 1, dir=\"tmp\") In the above example, the HDF5 file will be stored under the subdirectory tmp . If the subdirectory path does not exist, the Image object will attempt to create the folder. The Image class optionally takes the keyword parameter config . This parameter takes a list of one or more settings, which alter how the image is preprocessed. For example, one can choose to use disable storing the HDF5 file using the keyword parameter config with the setting nostore . image = Image(\"../tests/files/1_100.jpg\", 1, config=['nostore']) Example: Cloud-based Image Processing Pipeline For a real-world example, let's assume one is developing a cloud based system that takes images uploaded from users, with the following requirements. Handles multiple users uploading at the same time. Preprocessing of the images is concurrent. The machine learning ready data is passed to another step in a data (e.g., workflow) pipeline. Below is a bare-bones implementation. def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=next_step, config=['nostore']) def next_step(step): \"\"\" Do something with the Image object as the next step in the data pipeline \"\"\" data = image.data Preprocessing Transformations: Resizing, Reshaping, Flattening The keyword parameter config has a number of settings for specifying how the raw pixel data is preprocessed. The Gap framework is designed to eliminate the use of large numbers of keyword parameters, and instead uses a modern convention of passing in a configuration parameter. Here are some of the configuration settings: nostore # do not store in a HDF5 file grayscale | gray # convert to a grayscale image with a single channel (i.e., color plane) flatten | flat # flatten the machine learning ready data into a 1D vector resize=(height, width) # resize the raw pixel data thumb=(height, width) # create (and store) a thumbnail of the raw pixel data Let's look how you can use these settings for something like neural network's equivalent of the hello world example ~ training the MNIST dataset . The MNIST dataset consists of 28x28 grayscale images. Do to its size, grayscale and simplicity, it can be trained with just a ANN (vs. CNN). Since ANN take as input a 1D vector, the machine learning ready data would need to be reshaped (i.e., flatten) into a 1D vector. # An example of how one might use the Image object to preprocess an image from the MNIST dataset for a ANN image = Image(\"mnist_example.jpg\", digit, config=[\"gray\", \"flatten\"]) print(image.shape) # would output (784,) In the above, the preprocessed machine learning ready data will be in a vector of size 784 (i.e., 28x28) with data type float. Let's look at another publicly accessible training set, the Fruits360 Kaggle competition. In this training set, the images are 100x100 RGB images (i.e., 3 channels). If one used a CNN for this training set, one would preserve the number of channels. But the input vector may be unneccessarily large for training purposes (30000 ~ 100x100x3). Let's reduce the size using the resize setting by 1/4. image = Image(\"../tests/files/1_100.jpg\", config=['resize=(50,50)']) print(image.shape) # would output (50, 50, 3) Example: Image Processing Dashboard Let's expand on the real-word cloud example from earlier. In this case, let's assume that one wants to have a dashboard for a DevOps person to monitor the preprocessing of images from a user, with the requirements: Each time an image is preprocessed, the following is displayed on the dashboard: A thumbnail of the source image. The amount of time to preprocess the image. Progress count of number of images preprocessed and accumulated time. Here's the updated code: def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=second_step, config=['nostore', 'thumb=(16,16)']) nimages = 0 nsecs = 0 def second_step(image): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation nimages += 1 nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image) Okay, there is still some problem with this example in that nimages and nsecs are global and would be trashed by concurrent processing of different users. The ehandler parameter can be passed a tuple instead of a single value. In this case, the Image object emulates polymorphism. When specified as a tuple, the first item in the tuple is the event handler and the remaining items are additional arguments to the event handler. Let's now solve the above problem by adding a new object user which is passed to the first function first_step() . The user object will have fields for accumulating the number of times an image was processed for the user and the accumulated time. The ehandler parameter is then modified to pass the user object to the event handler second_step() . def first_step(uploaded_image, label, user): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=(second_step, user), config=['nostore', 'thumb=(16,16)']) def second_step(image, user): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation user.nimages += 1 user.nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image, user) Image Retrieval By default, the Image class will store the generated HDF5 in the current working directory (i.e., ./). The keyword parameter dir tells the Image class where to store the generated HDF5 file. image = Image(\"../tests/files/1_100.jpg\", dir='tmp') # stored as tmp/1_100.h5 Once stored, the Image object subsequently can be retrieved (i.e., reused) from the HDF5 file. In the example below, an empty Image object is first instantiated, and then the method load() is invoked passing it the name (rootname) of the image and the directory where the HDF5 file is stored, if not in the current working directory. image = Image() image.load('1_100', dir='tmp') # retrieve the machine learning ready data from the loaded Image object data = image.data Image Reference For a complete reference on all methods and properties for the Image class, see reference . Image Collections The Images class provides preprocessing of a collections of images (vs. a single image). The parameters and emulated polymorphism are identical to the Image class, except the images and labels parameter refer to a plurality of images, which comprise the collection. The positional parameter images can be specified as: A list of local or remote images (e.g., [ '1_100.jpg', '2_100.jpg', '3_100.jpg']) A single directory path of images (e.g., 'apple') A list of directory paths of images (e.g., ['apple', 'pear', 'banana']) The corresponding positional parameter labels must match the number of images as follows: A single value, applies to all the images (e.g., 1) A list of values which are the same length as the list of images or directory paths (e.g., [1, 2, 3]). The example below creates an Images objects consisting of three images with corresponding labels 1, 2 and 3. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) For each image specified, the Images class creates an Image object, which are maintained in the images objects as a list. The list of corresonding Image objects can be accessed from the property images . In the example below, a collection of three images is created, and then the images property is accessed as a list iterator in a for loop. On each loop, the next Image object is accessed and inside the loop the code prints the name and label of the corresponding Image object. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) for image in images.images: print(image.name, image.label) will output: 1_100 1 2_100 2 3_100 3 The builtin operators len() and [] are overridden in the Images class. The len() operator will return the number of images, and the list (array) index operator [] will return the Image object at the corresponding index. Using the builtin operators, the above example can be alternately coded as: for i in range(len(images)): print(images[i].name, images[i].label) Collection Storage & Retrieval The Images class, disables the Image objects from storing the machine learning ready data as individual HDF5 files per image, and insteads stores a single HDF5 for the entire collection. By default, the file name combines the prefix collection. with the root name of the first image in the collection, and is stored in the current working directory. In the above example, the machine learning ready data for the entire collection would be stored as: ./collection.1_100.h5 The directory where the HDF5 file is stored can be changed with the keyword paramater dir , and the root name of the file can be set with the keyword parameter name . images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3], dir='tmp', name='apples') In the above example, the machine learning ready data is stored as: ./tmp/apples.h5 A stored collection can the be subsequently retrieved from storage by instantiating an empty Images object and invoking the load() method with the corresponding collection name. For the above example, the apples collection would be retrieved and Images and corresponding Image objects reconstructed in memory: # Instantiate an empty Images object images = Images() # Set the directory where the collection is images.dir = 'tmp' # Load the collection into memory images.load('apples') Alternately, the load() method can be passed the keyword parameter dir to specify the directory where the collection is stored. For the above, this can be specified as: images.load('apples', dir='tmp') Example: Data Preparation for a Fruits Dataset: As Individual Collections In this example, a dataset of images of fruit are preprocessed into machine learning ready data, as follows: The images for each type of fruit are in separate directories (i.e., apple, pear, banana). The labels for the fruit will are sequentially numbered (i.e., 1, 2, 3). The images will be preserved as color images, but resized to (50,50). The shape of the preprocessed machine learning data will be (50, 50, 3) for input to a CNN. In the example below, a separate collection is created for each type of fruit: apples = Images( 'apple', 1, name='apples', config=['resize=(50,50)'] ) pears = Images( 'pear' , 2, name='pears', config=['resize=(50,50)'] ) bananas = Images( 'banana', 3, name='bananas', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./apples.h5 ./pears.h5 ./bananas.h5 In the above example, the preprocessing of each type of fruit was sequentially. Since conventional CPUs today are multi-core, we can take advantage of concurrency and speed up the processing in many computers by using the keyword parameter ehandler to process each collection asynchronously in parallel. accumulated = 0 label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) Let's describe some of the aspects of the above example. For the directories, we created a list of the directory names and then iterated through it. For each iteration, we: Instantiate an Images object for the current fruit. Set the collection name to the plural of the fruit name (i.e., fruit + 's'). Use an incrementer, starting at 1, for the label. Use the ehandler parameter to process the collection asynchronously. When each collection is completed, the function collectionHandler is called. This function will print the number of images processed in the collection, the time (in seconds) to process the collection, and the accumulated processing time for all the collections. Example: Data Preparation for a Fruits Dataset: As a Combined Collection Alternatively, we can process a group of collections as a single combined collection. The example below does the same as the aforementioned example, but produces a single (combined) dataset (vs. three individual datasets). fruits = Images( ['apples', 'pears', 'bananas'], [1, 2, 3], name='fruits', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./fruits.h5 Can we improve on the above? We got the benefit of a combined collection, but lost the benefit of concurrently preprocessing each collection. That's not overlooked. The += operator for the Images collection is overridden to combine collections. Let's update the earlier example to preprocess each collection asynchronously and combine them into a single collection. dataset = None accumulated = 0 lock = Lock() label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) # these steps need to be atomic lock.acquire() if dataset is None: dataset = images else: dataset += images lock.release() In the above example, we used the variable dataset for the combined collection. After the first collection is preprocessed, we set the variable dataset to the first Images object, and afterwards we combine it with subsequent collections using the += operator. Because the processing and invoking the event handler happen concurrently, there are possible problems including a race condition (i.e., two threads access dataset at the same time), and trashing the internal data (i.e., two threads are combining data at the same time). We solve this by making this operation atomic using Python's thread lock mechanism. *Above feature anticipated for v0.9.5 (beta) Splitting the Collection into Training and Test Data The first step to training a neural network is to split the collection into training and test data. We will cover some basic cases here. One uses the property split as a setter to split the collection into training and test data. This property will randomized the order of the Image objects in the collection, create a partition between the train and test data and create a corresponding internal index. The split property is implemented using emulated polymorphism, whereby the property can be given a single value or a tuple. The first value (parameter) is the percentage of the collection that will be set aside as testing data, and must be between 0 and 1. Below is an example: # 20% of the collection is test, and 80% is training images.split = 0.2 In another case, one might have separate collections for train and test. In this case, for both collections set the split to 0, which means use the entire collection, but otherwises randomizes the order of the Image objects. train = Images( ['train/apple', 'train/pear', 'train/banana'], [1, 2, 3], config=['resize=(50,50)']) test = Images( ['test/apple', 'test/pear', 'test/banana'], [1, 2, 3], config=['resize=(50,50)']) train.split = 0 test.split = 0 The random number generation by default will start at a different seed each time. If you need (desire) consistency between training on the results for comparison or demo'ing, then one specifies a seed value for the random number generation. The seed value is an integer value and is specified as a second parameter (i.e., tuple) to the split property. In the example below, the split is set to 20% test, and the random seed set to 42. images.split = 0.2, 42 One can see the index of the randomized distribution by displaying the internal member _train . This member is a list of integers which correspond to the index in the images list. While Python does not support the OOP concept of data encapsulation using private members, the Gap framework follows the convention that any member beginning with an underscore should be treated by developers as private. While not enforced by Python, members like _train should only be read and not written. The example below accesses (read) the randomized index for the training data and then prints it. indexes = images._train print(indexes) Forward Feeding a Neural Network The Images class provides methods for batch, stochastic and mini-batch feeding for training and evaluating a neural network. The feeders produce full batch samples, single samples and mini-batch samples as numpy matrixes, which are compatible for input with all Python machine learning frameworks that support numpy arrays, such as Tensorflow, Keras and Pytorch, for example. Forward feeding is randomized, and the entire collection(s) can be continuously re-feed (i.e., epoch), where each time they are re-randomized. The split , minibatch , and overriden next() operator support forward feeding. Batch Feeding In batch mode, the entire training set can be ran through the neural network as a single pass, prior to backward probagation and updating the weights using gradient descent. This is known as 'batch gradient descent'. When the split property is used as a getter, it returns the image data and corresponding labels for the training and test set similar to using sci-learn's train_test_split() function. In the example below: The dataset is split into 20% test and 80% training. The X_train and X_test is the list of machine learning ready data, as numpy arrays, of the corresponding training and test images. The Y_train and Y_test is the list of the corresponding labels. The variable epochs is the number of times the X_train dataset will be forward feed through the neural network. The optimizer performs backward probagation to update the weights. At the end of each epoch, The training data is re-randomized by calling the split method again as a getter. When training is done, the X_test and corresponding Y_test are forward feed to evaluate the accuracy of the trained model. # Get the first randomized split of the dataset images.split = 0.2, 42 X_train, X_test, Y_train, Y_test = images.split nepochs = 200 # the number of times to feed the entire training set while training the neural network for _ in range(nepochs): # Feed the entire training set per epoch (i.e., X_train, Y_train) and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Re-randomize the training set X_train, _, Y_train, _ = images.split # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass Stochastic Feeding Another way of feeding a neural network is to feed one image at a time and do backward probagation, using gradient descent. This is known as 'stochastic gradient descent'. The next() operator supports iterating through the training list one image object at a time. Once all of the entire training set has been iterated through, the next() operator returns None, and the training set is randomly re-shuffled for the next epoch. # Split the data into test and training datasets images.split = 0.2, 42 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass Mini-Batch Feeding Another way of feeding a neural network is through mini-batches. A mini-batch is a subset of the training set, that is greater than one. After each mini-batch is feed, then backward probagation, using gradient descent, is done. Typically, mini-batches are set to sizes like 30, 50, 100, or 200. The minibatch property when used as a setter, will set the size of the mini-batches. In the example below, the mini-batch size is set to 100. images.minibatch = 100 When the minibatch property is used as a getter, it will produce a generator, which will generate a batch from the training set of the size specified when used as a setter. Each time the minibatch property is called as a getter, it will sequentially move through the randomized set of training data. Upon completion of an epoch, the training set is re-randomized, and the minibatch property will reset to the begining of the training set. In the example below: The minibatch size is set to 100. The total number of batches for the training set is calculated. The training set is forward feed through the neural network 200 times (epochs). On each epoch, the training set is partitioned into mini-batches. After each mini-batch is feed, run the optimizer to update the weights. # Set the minibatch size images.minibatch = 100 # Calculate the number of batches nbatches = len(images) // 100 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Process each mini-batch for _ in range(nbatches): # Create a generator for the next minibatch g = images.minibatch # Get the data, labels for each item in the minibatch for data, label in g: # Forward Feed the image data and label pass # Run the optimizer (backward probagation) to update the weights after each mini-batch pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass Image (Data) Augmentation Image Augmentation is the process of generating (synthesizing) new images from existing images, which can then be used to augment the training process. Synthesis can include, rotation, skew, sharpending and blur of existing images. These new images are then feed into the neural network during training to augment the training set. Rotating and skew aid in recognizing images from different angles, and sharpening and blur help generalize recognition (offset overfitting), as well as recognition under different lightening and time of day conditions. When the augment property is used as a setter, it will either enable or disable image augmentation when forward feeding the neural network when used in conjunction with the split property, minibatch property or next() operator. The augment property uses emulated polymorphism for the paramters. When the parameter is True, the feed forward process (e.g., next() ) will generate an additional augmented image for each image in the training set, where the augmented image is a random rotation between -90 and 90 degress of the original image. The augmentation process adjusts the height and width of the image prior to rotation, as to prevent cropping, and then resizes back to the target size. # Split the data into test and training datasets images.split = 0.2, 42 # Enable image augmentation images.augmentation = True # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: # Twice as many images as size of the training set will be generated, where every other image # is a random rotation between -90 and 90 degrees of the last image. data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass If parameter to the augment property may also be a tuple. The tuple specifies the rotation range and optionally the number of agumented images to generate per image; otherwise defaults to one. In the example below: Augmented images will be a random rotation between -45 and 120. For each image, three augmented images will be generated. The mini-batch size is set to 100, so with the augmentation each mini-batch will produce 400 images. images.augment = -45, 120, 3 images.minibatch = 100 Specifying the parameter as a tuple is anticipated for v0.9.5 (beta) Transformation The transformation methods provide the ability to transform the existing stored machine learning ready data into another shape without reprocessing the image data. This feature is particularly useful if the existing machine learning ready data is repurposed for another neural network whose input is a different shape. The above feature is anticipated for v0.9.5 (beta)","title":"Computer Vision"},{"location":"tutorials/computer_vision/#computer-vision","text":"","title":"Computer Vision"},{"location":"tutorials/computer_vision/#introduction","text":"Welcome to the labs.earth collaborative laboratory tutorials on machine learning. The computer vision (CV) tutorials will start with the basics and progress to advanced real world applications. The tutorials go beyond explaining the code and steps, to include the answers to the anticipated what and why questions. Before the advent of machine learning with computer vision and today's modern ML/CV frameworks, working with and building real world applications was once the exclusive domain of imaging scientists. The Gap framework extends modern computer vision to software developers, whom are familar with object oriented programming (OOP), object relational models (ORM), design patterns (e.g., MVC), asynchronous programming (AJAX), and microservice architectures. For the data analyst and statisticians whom feel they don't have the necessary software development background, we encourage you to visit the collaborative lab's training site for fundamentials in modern software programming. Likewise, for those software developers whom feel they don't have the necessary background in statistics and machine learning, we encourage you to visit the collaborative lab's training site for fundamentials in modern statistics and machine learning . As far as our team and contributers, they keep a single phrase in mind when designing, coding and building tutorials. They like to say that Gap is: Machine Learning for Humans","title":"Introduction"},{"location":"tutorials/computer_vision/#the-first-steps-in-using-gap-for-computer-vision-cv","text":"The first step in using Gap for machine learning (ML) of computer vision (CV) is learning to classify a single object in an image. Is it a dog, a cat, what digit is it, what sign language digit is it, etc... To do single object classification, depending on the images, one will use either a artificial neural network ( ANN ) or a convolutional neural network ( CNN ). In either case, the raw pixel data is not directly inputted into a neural network. Instead, it has to be prepared into machine learning (ML) ready data. How it is prepared/transformed is dependent on the image source, the type and configuration of the neural network, and the target application. Images can come from a variety of sources, such as by your cell phone, images found on the Internet, a facsimile image (FAX) , a video frame from a video stream , a digitized medical/dental x-ray , a magnetic resonance imaging (MRI) , a electron microscopy (TEM) , etc. Images go from very basic like 1-bit BW single channel (color plane) , to 8-bit gray scale single channel , to 8-bit color three channel (RGB) , to 8-bit color four channel (+alpha channel) , to 16-bit high tone (CMYK) , to infrared , to stereoscopic images (3D) , sound navigation and ranging (SONAR) , to RADAR , and more.","title":"The First Steps in using Gap for Computer Vision (CV)"},{"location":"tutorials/computer_vision/#fundamentals-in-preparing-an-image-for-machine-learning","text":"Neural networks take as input numbers, specifically numbers that are continous real numbers and have been normalized . For images, pixel values are proportionally squashed between 0 and 1. For ANN networks, the inputs need to be a 1D vector, in which case the input data needs to be flatten, while in a CNN, the input is a 2D vector. Neural networks for computer vision take input of fixed sizes, so there is a transformation step to transform the pixel data to the input size and shape of the neural network, and finally assigning a label to the image (e.g., it's a cat). Again, for labels, neural networks use integer numbers; for example a cat must be assigned to a unique integer value and a dog to a different unique integer value. These are the basic steps for all computer vision based neural networks: Transformation Normalization Shaping (e.g., flattening) Labeling","title":"Fundamentals in Preparing an Image for Machine Learning"},{"location":"tutorials/computer_vision/#importing-vision-module","text":"The Vision module of the Gap framework implements the classes and methods for computer vision. Within the Vision module are two primary class objects for data management of images. The Image class manages individual images, while the Images class manages collections of images. As a first step, in your Python script or program you want to import from the Vision module the Image and Images class objects. from vision import Image, Images","title":"Importing Vision module"},{"location":"tutorials/computer_vision/#preprocessing-preparing-an-image-with-gap","text":"Relative to the location of this tutorial are a number of test images used in verifying releases of Gap. For the purpose of these tutorials, the images that are part of the Gap release verification will be used for examples. The test file 1_100.jpg is a simple 100x100 96 dpi color image (RGB/8bit) from the Kaggle Fruit360 dataset. This dataset was part of a Kaggle contents to classify different types of fruits and their variety. It was a fairly simple dataset in that all the images were of the same size, type and number of channels. Further, each image contained only the object to classify (i.e., fruit) and was centered in the image. The first step is to instantiate an Image class object and load the image into it, and its corresponding label. In the example below, an Image object is created where the first two positional parameters are the path to the image and the corresponding label (i.e., 1). image = Image(\"../tests/files/1_100.jpg\", 1) While Python does not have OOP polymorphism builtin, the class objects in Gap have been constructed to emulate polymorphism in a variety of ways. The first positional parameter (image path) to the Image class can either be a local path or a remote path. In the latter case, a path starting with http or https is a remote path. In this case, a HTTP request to fetch the image from the remote location is made. image = Image(\"https://en.wikipedia.org/wiki/File:Example.jpg\", 1) Alternately, raw pixel data can be specified as the first (image) positional parameter, as a numpy array. raw = cv2.imread(\"../tests/files/1_100.jpg\") image = Image(raw, 1) Preprocessing of the image in the above examples is synchronous. The initializer (i.e., constructor) returns an image object once the image file has been preprocessed. Alternately, preprocessing of an image can be done asynchronously, where the preprocessing is performed by a background thread. Asynchronous processing occurs if the keyword parameter ehandler is specified. The value of the parameter is set to a function or method, which is invoked with the image object as a parameter when preprocessing of the image is complete. image = Image(\"../tests/files/1_100.jpg\", 1, ehandler=myfunc) def myfunc(image): print(\"done\") The Image class has a number of attributes which are accessed using OOP properties (i.e., getters and setters). The attributes below provide information on the source image: print(image.name) # the name of the image (e.g., 1_100) print(image.type) # the type of the image (e.g., jpg) print(image.size) # the size of the image in bytes (e.g., 3574) print(image.label) # the label assigned to the image (e.g., 1) The raw pixel data of the source image is accessed with the raw property, where property returns the uncompressed pixel data of the source image as a numpy array. raw = image.raw print(type(raw)) # outputs <class 'numpy.ndarry'> print(raw.shape) # outputs the shape of the source image (e.g., (100, 100, 3)) The preprocessed machine learning ready data is accessed with the data property, where the property returns the data as a numpy array. data = image.data print(type(data)) # outputs <class 'numpy.ndarry'> print(data.shape) # outputs the shape of the machine learning data (e.g., (100, 100, 3)) By default, the shape and number of channels of the source image are maintained in the preprocessed machine learning ready data, and the pixel values are normalized to values between 0 and 1. print(raw[0][80]) # outputs pixel values (e.g., [250, 255, 255]) print(data[0][80]) # outputs machine learning ready data values (e.g., [0.98039216, 1.0, 1.0]) When processing of the image is completed, the raw pixel data, machine learning ready data, and attributes are stored in a HDF5 (Hierarchical Data Format) formatted file. By default, the file is stored in the current local directory, where the rootname of the file is the rootname of the image. Storage provides the means to latter retrieval the machine learning ready data for feeding into a neural network, and/or retransforming the machine learning ready data. In the above example, the file would be stored as: ./1_100.hd5 The path location of the stored HDF5 can be specified with the keyword parameter dir . image = Image(\"../tests/files/1_100.jpg\", 1, dir=\"tmp\") In the above example, the HDF5 file will be stored under the subdirectory tmp . If the subdirectory path does not exist, the Image object will attempt to create the folder. The Image class optionally takes the keyword parameter config . This parameter takes a list of one or more settings, which alter how the image is preprocessed. For example, one can choose to use disable storing the HDF5 file using the keyword parameter config with the setting nostore . image = Image(\"../tests/files/1_100.jpg\", 1, config=['nostore'])","title":"Preprocessing (Preparing) an image with Gap"},{"location":"tutorials/computer_vision/#example-cloud-based-image-processing-pipeline","text":"For a real-world example, let's assume one is developing a cloud based system that takes images uploaded from users, with the following requirements. Handles multiple users uploading at the same time. Preprocessing of the images is concurrent. The machine learning ready data is passed to another step in a data (e.g., workflow) pipeline. Below is a bare-bones implementation. def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=next_step, config=['nostore']) def next_step(step): \"\"\" Do something with the Image object as the next step in the data pipeline \"\"\" data = image.data","title":"Example: Cloud-based Image Processing Pipeline"},{"location":"tutorials/computer_vision/#preprocessing-transformations-resizing-reshaping-flattening","text":"The keyword parameter config has a number of settings for specifying how the raw pixel data is preprocessed. The Gap framework is designed to eliminate the use of large numbers of keyword parameters, and instead uses a modern convention of passing in a configuration parameter. Here are some of the configuration settings: nostore # do not store in a HDF5 file grayscale | gray # convert to a grayscale image with a single channel (i.e., color plane) flatten | flat # flatten the machine learning ready data into a 1D vector resize=(height, width) # resize the raw pixel data thumb=(height, width) # create (and store) a thumbnail of the raw pixel data Let's look how you can use these settings for something like neural network's equivalent of the hello world example ~ training the MNIST dataset . The MNIST dataset consists of 28x28 grayscale images. Do to its size, grayscale and simplicity, it can be trained with just a ANN (vs. CNN). Since ANN take as input a 1D vector, the machine learning ready data would need to be reshaped (i.e., flatten) into a 1D vector. # An example of how one might use the Image object to preprocess an image from the MNIST dataset for a ANN image = Image(\"mnist_example.jpg\", digit, config=[\"gray\", \"flatten\"]) print(image.shape) # would output (784,) In the above, the preprocessed machine learning ready data will be in a vector of size 784 (i.e., 28x28) with data type float. Let's look at another publicly accessible training set, the Fruits360 Kaggle competition. In this training set, the images are 100x100 RGB images (i.e., 3 channels). If one used a CNN for this training set, one would preserve the number of channels. But the input vector may be unneccessarily large for training purposes (30000 ~ 100x100x3). Let's reduce the size using the resize setting by 1/4. image = Image(\"../tests/files/1_100.jpg\", config=['resize=(50,50)']) print(image.shape) # would output (50, 50, 3)","title":"Preprocessing Transformations: Resizing, Reshaping, Flattening"},{"location":"tutorials/computer_vision/#example-image-processing-dashboard","text":"Let's expand on the real-word cloud example from earlier. In this case, let's assume that one wants to have a dashboard for a DevOps person to monitor the preprocessing of images from a user, with the requirements: Each time an image is preprocessed, the following is displayed on the dashboard: A thumbnail of the source image. The amount of time to preprocess the image. Progress count of number of images preprocessed and accumulated time. Here's the updated code: def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=second_step, config=['nostore', 'thumb=(16,16)']) nimages = 0 nsecs = 0 def second_step(image): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation nimages += 1 nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image) Okay, there is still some problem with this example in that nimages and nsecs are global and would be trashed by concurrent processing of different users. The ehandler parameter can be passed a tuple instead of a single value. In this case, the Image object emulates polymorphism. When specified as a tuple, the first item in the tuple is the event handler and the remaining items are additional arguments to the event handler. Let's now solve the above problem by adding a new object user which is passed to the first function first_step() . The user object will have fields for accumulating the number of times an image was processed for the user and the accumulated time. The ehandler parameter is then modified to pass the user object to the event handler second_step() . def first_step(uploaded_image, label, user): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=(second_step, user), config=['nostore', 'thumb=(16,16)']) def second_step(image, user): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation user.nimages += 1 user.nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image, user)","title":"Example: Image Processing Dashboard"},{"location":"tutorials/computer_vision/#image-retrieval","text":"By default, the Image class will store the generated HDF5 in the current working directory (i.e., ./). The keyword parameter dir tells the Image class where to store the generated HDF5 file. image = Image(\"../tests/files/1_100.jpg\", dir='tmp') # stored as tmp/1_100.h5 Once stored, the Image object subsequently can be retrieved (i.e., reused) from the HDF5 file. In the example below, an empty Image object is first instantiated, and then the method load() is invoked passing it the name (rootname) of the image and the directory where the HDF5 file is stored, if not in the current working directory. image = Image() image.load('1_100', dir='tmp') # retrieve the machine learning ready data from the loaded Image object data = image.data","title":"Image Retrieval"},{"location":"tutorials/computer_vision/#image-reference","text":"For a complete reference on all methods and properties for the Image class, see reference .","title":"Image Reference"},{"location":"tutorials/computer_vision/#image-collections","text":"The Images class provides preprocessing of a collections of images (vs. a single image). The parameters and emulated polymorphism are identical to the Image class, except the images and labels parameter refer to a plurality of images, which comprise the collection. The positional parameter images can be specified as: A list of local or remote images (e.g., [ '1_100.jpg', '2_100.jpg', '3_100.jpg']) A single directory path of images (e.g., 'apple') A list of directory paths of images (e.g., ['apple', 'pear', 'banana']) The corresponding positional parameter labels must match the number of images as follows: A single value, applies to all the images (e.g., 1) A list of values which are the same length as the list of images or directory paths (e.g., [1, 2, 3]). The example below creates an Images objects consisting of three images with corresponding labels 1, 2 and 3. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) For each image specified, the Images class creates an Image object, which are maintained in the images objects as a list. The list of corresonding Image objects can be accessed from the property images . In the example below, a collection of three images is created, and then the images property is accessed as a list iterator in a for loop. On each loop, the next Image object is accessed and inside the loop the code prints the name and label of the corresponding Image object. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) for image in images.images: print(image.name, image.label) will output: 1_100 1 2_100 2 3_100 3 The builtin operators len() and [] are overridden in the Images class. The len() operator will return the number of images, and the list (array) index operator [] will return the Image object at the corresponding index. Using the builtin operators, the above example can be alternately coded as: for i in range(len(images)): print(images[i].name, images[i].label)","title":"Image Collections"},{"location":"tutorials/computer_vision/#collection-storage-retrieval","text":"The Images class, disables the Image objects from storing the machine learning ready data as individual HDF5 files per image, and insteads stores a single HDF5 for the entire collection. By default, the file name combines the prefix collection. with the root name of the first image in the collection, and is stored in the current working directory. In the above example, the machine learning ready data for the entire collection would be stored as: ./collection.1_100.h5 The directory where the HDF5 file is stored can be changed with the keyword paramater dir , and the root name of the file can be set with the keyword parameter name . images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3], dir='tmp', name='apples') In the above example, the machine learning ready data is stored as: ./tmp/apples.h5 A stored collection can the be subsequently retrieved from storage by instantiating an empty Images object and invoking the load() method with the corresponding collection name. For the above example, the apples collection would be retrieved and Images and corresponding Image objects reconstructed in memory: # Instantiate an empty Images object images = Images() # Set the directory where the collection is images.dir = 'tmp' # Load the collection into memory images.load('apples') Alternately, the load() method can be passed the keyword parameter dir to specify the directory where the collection is stored. For the above, this can be specified as: images.load('apples', dir='tmp')","title":"Collection Storage &amp; Retrieval"},{"location":"tutorials/computer_vision/#example-data-preparation-for-a-fruits-dataset-as-individual-collections","text":"In this example, a dataset of images of fruit are preprocessed into machine learning ready data, as follows: The images for each type of fruit are in separate directories (i.e., apple, pear, banana). The labels for the fruit will are sequentially numbered (i.e., 1, 2, 3). The images will be preserved as color images, but resized to (50,50). The shape of the preprocessed machine learning data will be (50, 50, 3) for input to a CNN. In the example below, a separate collection is created for each type of fruit: apples = Images( 'apple', 1, name='apples', config=['resize=(50,50)'] ) pears = Images( 'pear' , 2, name='pears', config=['resize=(50,50)'] ) bananas = Images( 'banana', 3, name='bananas', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./apples.h5 ./pears.h5 ./bananas.h5 In the above example, the preprocessing of each type of fruit was sequentially. Since conventional CPUs today are multi-core, we can take advantage of concurrency and speed up the processing in many computers by using the keyword parameter ehandler to process each collection asynchronously in parallel. accumulated = 0 label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) Let's describe some of the aspects of the above example. For the directories, we created a list of the directory names and then iterated through it. For each iteration, we: Instantiate an Images object for the current fruit. Set the collection name to the plural of the fruit name (i.e., fruit + 's'). Use an incrementer, starting at 1, for the label. Use the ehandler parameter to process the collection asynchronously. When each collection is completed, the function collectionHandler is called. This function will print the number of images processed in the collection, the time (in seconds) to process the collection, and the accumulated processing time for all the collections.","title":"Example: Data Preparation for a Fruits Dataset: As Individual Collections"},{"location":"tutorials/computer_vision/#example-data-preparation-for-a-fruits-dataset-as-a-combined-collection","text":"Alternatively, we can process a group of collections as a single combined collection. The example below does the same as the aforementioned example, but produces a single (combined) dataset (vs. three individual datasets). fruits = Images( ['apples', 'pears', 'bananas'], [1, 2, 3], name='fruits', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./fruits.h5 Can we improve on the above? We got the benefit of a combined collection, but lost the benefit of concurrently preprocessing each collection. That's not overlooked. The += operator for the Images collection is overridden to combine collections. Let's update the earlier example to preprocess each collection asynchronously and combine them into a single collection. dataset = None accumulated = 0 lock = Lock() label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) # these steps need to be atomic lock.acquire() if dataset is None: dataset = images else: dataset += images lock.release() In the above example, we used the variable dataset for the combined collection. After the first collection is preprocessed, we set the variable dataset to the first Images object, and afterwards we combine it with subsequent collections using the += operator. Because the processing and invoking the event handler happen concurrently, there are possible problems including a race condition (i.e., two threads access dataset at the same time), and trashing the internal data (i.e., two threads are combining data at the same time). We solve this by making this operation atomic using Python's thread lock mechanism. *Above feature anticipated for v0.9.5 (beta)","title":"Example: Data Preparation for a Fruits Dataset: As a Combined Collection"},{"location":"tutorials/computer_vision/#splitting-the-collection-into-training-and-test-data","text":"The first step to training a neural network is to split the collection into training and test data. We will cover some basic cases here. One uses the property split as a setter to split the collection into training and test data. This property will randomized the order of the Image objects in the collection, create a partition between the train and test data and create a corresponding internal index. The split property is implemented using emulated polymorphism, whereby the property can be given a single value or a tuple. The first value (parameter) is the percentage of the collection that will be set aside as testing data, and must be between 0 and 1. Below is an example: # 20% of the collection is test, and 80% is training images.split = 0.2 In another case, one might have separate collections for train and test. In this case, for both collections set the split to 0, which means use the entire collection, but otherwises randomizes the order of the Image objects. train = Images( ['train/apple', 'train/pear', 'train/banana'], [1, 2, 3], config=['resize=(50,50)']) test = Images( ['test/apple', 'test/pear', 'test/banana'], [1, 2, 3], config=['resize=(50,50)']) train.split = 0 test.split = 0 The random number generation by default will start at a different seed each time. If you need (desire) consistency between training on the results for comparison or demo'ing, then one specifies a seed value for the random number generation. The seed value is an integer value and is specified as a second parameter (i.e., tuple) to the split property. In the example below, the split is set to 20% test, and the random seed set to 42. images.split = 0.2, 42 One can see the index of the randomized distribution by displaying the internal member _train . This member is a list of integers which correspond to the index in the images list. While Python does not support the OOP concept of data encapsulation using private members, the Gap framework follows the convention that any member beginning with an underscore should be treated by developers as private. While not enforced by Python, members like _train should only be read and not written. The example below accesses (read) the randomized index for the training data and then prints it. indexes = images._train print(indexes)","title":"Splitting the Collection into Training and Test Data"},{"location":"tutorials/computer_vision/#forward-feeding-a-neural-network","text":"The Images class provides methods for batch, stochastic and mini-batch feeding for training and evaluating a neural network. The feeders produce full batch samples, single samples and mini-batch samples as numpy matrixes, which are compatible for input with all Python machine learning frameworks that support numpy arrays, such as Tensorflow, Keras and Pytorch, for example. Forward feeding is randomized, and the entire collection(s) can be continuously re-feed (i.e., epoch), where each time they are re-randomized. The split , minibatch , and overriden next() operator support forward feeding.","title":"Forward Feeding a Neural Network"},{"location":"tutorials/computer_vision/#batch-feeding","text":"In batch mode, the entire training set can be ran through the neural network as a single pass, prior to backward probagation and updating the weights using gradient descent. This is known as 'batch gradient descent'. When the split property is used as a getter, it returns the image data and corresponding labels for the training and test set similar to using sci-learn's train_test_split() function. In the example below: The dataset is split into 20% test and 80% training. The X_train and X_test is the list of machine learning ready data, as numpy arrays, of the corresponding training and test images. The Y_train and Y_test is the list of the corresponding labels. The variable epochs is the number of times the X_train dataset will be forward feed through the neural network. The optimizer performs backward probagation to update the weights. At the end of each epoch, The training data is re-randomized by calling the split method again as a getter. When training is done, the X_test and corresponding Y_test are forward feed to evaluate the accuracy of the trained model. # Get the first randomized split of the dataset images.split = 0.2, 42 X_train, X_test, Y_train, Y_test = images.split nepochs = 200 # the number of times to feed the entire training set while training the neural network for _ in range(nepochs): # Feed the entire training set per epoch (i.e., X_train, Y_train) and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Re-randomize the training set X_train, _, Y_train, _ = images.split # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass","title":"Batch Feeding"},{"location":"tutorials/computer_vision/#stochastic-feeding","text":"Another way of feeding a neural network is to feed one image at a time and do backward probagation, using gradient descent. This is known as 'stochastic gradient descent'. The next() operator supports iterating through the training list one image object at a time. Once all of the entire training set has been iterated through, the next() operator returns None, and the training set is randomly re-shuffled for the next epoch. # Split the data into test and training datasets images.split = 0.2, 42 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass","title":"Stochastic Feeding"},{"location":"tutorials/computer_vision/#mini-batch-feeding","text":"Another way of feeding a neural network is through mini-batches. A mini-batch is a subset of the training set, that is greater than one. After each mini-batch is feed, then backward probagation, using gradient descent, is done. Typically, mini-batches are set to sizes like 30, 50, 100, or 200. The minibatch property when used as a setter, will set the size of the mini-batches. In the example below, the mini-batch size is set to 100. images.minibatch = 100 When the minibatch property is used as a getter, it will produce a generator, which will generate a batch from the training set of the size specified when used as a setter. Each time the minibatch property is called as a getter, it will sequentially move through the randomized set of training data. Upon completion of an epoch, the training set is re-randomized, and the minibatch property will reset to the begining of the training set. In the example below: The minibatch size is set to 100. The total number of batches for the training set is calculated. The training set is forward feed through the neural network 200 times (epochs). On each epoch, the training set is partitioned into mini-batches. After each mini-batch is feed, run the optimizer to update the weights. # Set the minibatch size images.minibatch = 100 # Calculate the number of batches nbatches = len(images) // 100 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Process each mini-batch for _ in range(nbatches): # Create a generator for the next minibatch g = images.minibatch # Get the data, labels for each item in the minibatch for data, label in g: # Forward Feed the image data and label pass # Run the optimizer (backward probagation) to update the weights after each mini-batch pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass","title":"Mini-Batch Feeding"},{"location":"tutorials/computer_vision/#image-data-augmentation","text":"Image Augmentation is the process of generating (synthesizing) new images from existing images, which can then be used to augment the training process. Synthesis can include, rotation, skew, sharpending and blur of existing images. These new images are then feed into the neural network during training to augment the training set. Rotating and skew aid in recognizing images from different angles, and sharpening and blur help generalize recognition (offset overfitting), as well as recognition under different lightening and time of day conditions. When the augment property is used as a setter, it will either enable or disable image augmentation when forward feeding the neural network when used in conjunction with the split property, minibatch property or next() operator. The augment property uses emulated polymorphism for the paramters. When the parameter is True, the feed forward process (e.g., next() ) will generate an additional augmented image for each image in the training set, where the augmented image is a random rotation between -90 and 90 degress of the original image. The augmentation process adjusts the height and width of the image prior to rotation, as to prevent cropping, and then resizes back to the target size. # Split the data into test and training datasets images.split = 0.2, 42 # Enable image augmentation images.augmentation = True # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: # Twice as many images as size of the training set will be generated, where every other image # is a random rotation between -90 and 90 degrees of the last image. data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass If parameter to the augment property may also be a tuple. The tuple specifies the rotation range and optionally the number of agumented images to generate per image; otherwise defaults to one. In the example below: Augmented images will be a random rotation between -45 and 120. For each image, three augmented images will be generated. The mini-batch size is set to 100, so with the augmentation each mini-batch will produce 400 images. images.augment = -45, 120, 3 images.minibatch = 100 Specifying the parameter as a tuple is anticipated for v0.9.5 (beta)","title":"Image (Data) Augmentation"},{"location":"tutorials/computer_vision/#transformation","text":"The transformation methods provide the ability to transform the existing stored machine learning ready data into another shape without reprocessing the image data. This feature is particularly useful if the existing machine learning ready data is repurposed for another neural network whose input is a different shape. The above feature is anticipated for v0.9.5 (beta)","title":"Transformation"},{"location":"tutorials/natural_language_processing/","text":"Natural Language Processing Introduction Welcome to the labs.earth collaborative laboratory tutorials on machine learning. The natural language processing (NLP) tutorials will start with the basics and progress to advanced real world applications. The tutorials go beyond explaining the code and steps, to include the answers to the anticipated what and why questions. Before the advent of machine learning with computer vision and today's modern ML/CV frameworks, working with and building real world applications was once the exclusive domain of imaging scientists. The Gap framework extends modern natural language processing to software developers, whom are familar with object oriented programming (OOP), object relational models (ORM), design patterns (e.g., MVC), asynchronous programming (AJAX), and microservice architectures. For the data analyst and statisticians whom feel they don't have the necessary software development background, we encourage you to visit the collaborative lab's training site for fundamentials in modern software programming. Likewise, for those software developers whom feel they don't have the necessary background in statistics and machine learning, we encourage you to visit the collaborative lab's training site for fundamentials in modern statistics and machine learning . As far as our team and contributers, they keep a single phrase in mind when designing, coding and building tutorials. They like to say that Gap is: Machine Learning for Humans The First Steps in using Gap for Natural Language Processing (NLP)","title":"Natural Language Processing"},{"location":"tutorials/natural_language_processing/#natural-language-processing","text":"","title":"Natural Language Processing"},{"location":"tutorials/natural_language_processing/#introduction","text":"Welcome to the labs.earth collaborative laboratory tutorials on machine learning. The natural language processing (NLP) tutorials will start with the basics and progress to advanced real world applications. The tutorials go beyond explaining the code and steps, to include the answers to the anticipated what and why questions. Before the advent of machine learning with computer vision and today's modern ML/CV frameworks, working with and building real world applications was once the exclusive domain of imaging scientists. The Gap framework extends modern natural language processing to software developers, whom are familar with object oriented programming (OOP), object relational models (ORM), design patterns (e.g., MVC), asynchronous programming (AJAX), and microservice architectures. For the data analyst and statisticians whom feel they don't have the necessary software development background, we encourage you to visit the collaborative lab's training site for fundamentials in modern software programming. Likewise, for those software developers whom feel they don't have the necessary background in statistics and machine learning, we encourage you to visit the collaborative lab's training site for fundamentials in modern statistics and machine learning . As far as our team and contributers, they keep a single phrase in mind when designing, coding and building tutorials. They like to say that Gap is: Machine Learning for Humans","title":"Introduction"},{"location":"tutorials/natural_language_processing/#the-first-steps-in-using-gap-for-natural-language-processing-nlp","text":"","title":"The First Steps in using Gap for Natural Language Processing (NLP)"}]}