{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3bc4854-2787-eae1-950d-2742ad3d7db2",
    "_uuid": "d507f816cc74a88c9afefd02cf225d1a7dd6461f"
   },
   "source": [
    "# COMPREHENSIVE DATA EXPLORATION WITH PYTHON\n",
    "[Pedro Marcelino](http://pmarcelino.com) - February 2017\n",
    "\n",
    "Other Kernels: [Data analysis and feature extraction with Python\n",
    "](https://www.kaggle.com/pmarcelino/data-analysis-and-feature-extraction-with-python)\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8ca352d7-08aa-36b4-fb2d-3c9854a8d86a",
    "_uuid": "014f5c099a26d9232f9d0d6ca85d5c02b812c98a"
   },
   "source": [
    "<b>'The most difficult thing in life is to know yourself'</b>\n",
    "\n",
    "This quote belongs to Thales of Miletus. Thales was a Greek/Phonecian philosopher, mathematician and astronomer, which is recognised as the first individual in Western civilisation known to have entertained and engaged in scientific thought (source: https://en.wikipedia.org/wiki/Thales)\n",
    "\n",
    "I wouldn't say that knowing your data is the most difficult thing in data science, but it is time-consuming. Therefore, it's easy to overlook this initial step and jump too soon into the water.\n",
    "\n",
    "So I tried to learn how to swim before jumping into the water. Based on [Hair et al. (2013)](https://amzn.to/2JuDmvo), chapter 'Examining your data', I did my best to follow a comprehensive, but not exhaustive, analysis of the data. I'm far from reporting a rigorous study in this kernel, but I hope that it can be useful for the community, so I'm sharing how I applied some of those data analysis principles to this problem.\n",
    "\n",
    "Despite the strange names I gave to the chapters, what we are doing in this kernel is something like:\n",
    "\n",
    "1. <b>Understand the problem</b>. We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n",
    "2. <b>Univariable study</b>. We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\n",
    "3. <b>Multivariate study</b>. We'll try to understand how the dependent variable and independent variables relate.\n",
    "4. <b>Basic cleaning</b>. We'll clean the dataset and handle the missing data, outliers and categorical variables.\n",
    "5. <b>Test assumptions</b>. We'll check if our data meets the assumptions required by most multivariate techniques.\n",
    "\n",
    "Now, it's time to have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2df621e0-e03c-7aaa-6e08-40ed1d7dfecc",
    "_execution_state": "idle",
    "_uuid": "d581f6797b9fde1580271358d484df67bf6b14a1"
   },
   "outputs": [],
   "source": [
    "#invite people for the Kaggle party\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d56d5e71-4277-7a74-5306-7d5af4c7f263",
    "_execution_state": "idle",
    "_uuid": "827a72128cd211cf6af16b003e7c09951e3f2b1e"
   },
   "outputs": [],
   "source": [
    "#bring in the six packs\n",
    "df_train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "02250c81-7e15-c195-2e86-5adbd15c9d30",
    "_execution_state": "idle",
    "_uuid": "10814ba44786b5fea5e333324c6fe54729cabf33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'rle_mask'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the decoration\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79d22981-dfd7-a25f-9a78-5436213207e2",
    "_uuid": "ba13c267e3aacbef23dfbcc0ab05f002d7835e0c"
   },
   "source": [
    "# 1. So... What can we expect?\n",
    "\n",
    "In order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset.\n",
    "\n",
    "In order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:\n",
    "* <b>Variable</b> - Variable name.\n",
    "* <b>Type</b> - Identification of the variables' type. There are two possible values for this field: 'numerical' or 'categorical'. By 'numerical' we mean variables for which the values are numbers, and by 'categorical' we mean variables for which the values are categories.\n",
    "* <b>Segment</b> - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood').\n",
    "* <b>Expectation</b> - Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values.\n",
    "* <b>Conclusion</b> - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'.\n",
    "* <b>Comments</b> - Any general comments that occured to us.\n",
    "\n",
    "While 'Type' and 'Segment' is just for possible future reference, the column 'Expectation' is important because it will help us develop a 'sixth sense'. To fill this column, we should read the description of all the variables and, one by one, ask ourselves:\n",
    "\n",
    "* Do we think about this variable when we are buying a house? (e.g. When we think about the house of our dreams, do we care about its 'Masonry veneer type'?).\n",
    "* If so, how important would this variable be? (e.g. What is the impact of having 'Excellent' material on the exterior instead of 'Poor'? And of having 'Excellent' instead of 'Good'?).\n",
    "* Is this information already described in any other variable? (e.g. If 'LandContour' gives the flatness of the property, do we really need to know the 'LandSlope'?).\n",
    "\n",
    "After this daunting exercise, we can filter the spreadsheet and look carefully to the variables with 'High' 'Expectation'. Then, we can rush into some scatter plots between those variables and 'SalePrice', filling in the 'Conclusion' column which is just the correction of our expectations.\n",
    "\n",
    "I went through this process and concluded that the following variables can play an important role in this problem:\n",
    "\n",
    "* OverallQual (which is a variable that I don't like because I don't know how it was computed; a funny exercise would be to predict 'OverallQual' using all the other variables available).\n",
    "* YearBuilt.\n",
    "* TotalBsmtSF.\n",
    "* GrLivArea.\n",
    "\n",
    "I ended up with two 'building' variables ('OverallQual' and 'YearBuilt') and two 'space' variables ('TotalBsmtSF' and 'GrLivArea'). This might be a little bit unexpected as it goes against the real estate mantra that all that matters is 'location, location and location'. It is possible that this quick data examination process was a bit harsh for categorical variables. For example, I expected the 'Neigborhood' variable to be more relevant, but after the data examination I ended up excluding it. Maybe this is related to the use of scatter plots instead of boxplots, which are more suitable for categorical variables visualization. The way we visualize data often influences our conclusions.\n",
    "\n",
    "However, the main point of this exercise was to think a little about our data and expectactions, so I think we achieved our goal. Now it's time for 'a little less conversation, a little more action please'. Let's <b>shake it!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3ef87d93-0ea6-8cb2-aa2d-90d5b56a1ca1",
    "_uuid": "39d104c7e40b3f66a0f6e2330119332b301be7bf"
   },
   "source": [
    "# 2. First things first: analysing 'SalePrice'\n",
    "\n",
    "'SalePrice' is the reason of our quest. It's like when we're going to a party. We always have a reason to be there. Usually, women are that reason. (disclaimer: adapt it to men, dancing or alcohol, according to your preferences)\n",
    "\n",
    "Using the women analogy, let's build a little story, the story of 'How we met 'SalePrice''.\n",
    "\n",
    "*Everything started in our Kaggle party, when we were looking for a dance partner. After a while searching in the dance floor, we saw a girl, near the bar, using dance shoes. That's a sign that she's there to dance. We spend much time doing predictive modelling and participating in analytics competitions, so talking with girls is not one of our super powers. Even so, we gave it a try:*\n",
    "\n",
    "*'Hi, I'm Kaggly! And you? 'SalePrice'? What a beautiful name! You know 'SalePrice', could you give me some data about you? I just developed a model to calculate the probability of a successful relationship between two people. I'd like to apply it to us!'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rle_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>575d24d81d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a266a2a9df</td>\n",
       "      <td>5051 5151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75efad62c1</td>\n",
       "      <td>9 93 109 94 210 94 310 95 411 95 511 96 612 96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34e51dba6a</td>\n",
       "      <td>48 54 149 54 251 53 353 52 455 51 557 50 659 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4875705fb0</td>\n",
       "      <td>1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>782ae9b7e7</td>\n",
       "      <td>1 1815 1819 90 1920 81 2021 73 2122 64 2223 55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9842f69f8d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aa94cfb806</td>\n",
       "      <td>1 28 102 28 203 29 304 30 405 32 506 33 607 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50d3073821</td>\n",
       "      <td>1 2121 9293 909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28f865caaa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b5e1371b3b</td>\n",
       "      <td>75 27 175 28 275 29 374 31 474 32 574 33 674 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>57e394bc67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b45ad3932e</td>\n",
       "      <td>49 11 149 11 249 11 351 10 452 9 553 9 654 8 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ef51bbcde7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>d4d34af4f7</td>\n",
       "      <td>8788 1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>302ea1ac81</td>\n",
       "      <td>6 96 108 95 210 94 311 94 413 93 515 92 615 93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40dcff68b3</td>\n",
       "      <td>3536 4 3637 20 3738 36 3839 53 3940 70 4041 86...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7845115d01</td>\n",
       "      <td>7677 2525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3da729cae9</td>\n",
       "      <td>1 54 102 54 203 53 304 53 405 53 506 53 607 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>d67e3a11d8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b3396387a6</td>\n",
       "      <td>42 60 141 62 241 63 341 64 441 65 542 65 642 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>a5471f53d8</td>\n",
       "      <td>9797 1 9891 8 9986 14 10081 20 10176 26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>d27831fef2</td>\n",
       "      <td>93 9 195 8 296 8 397 8 498 8 599 8 701 7 802 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7faea04242</td>\n",
       "      <td>1 6865 6869 92 6970 86 7071 81 7172 76 7273 71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9747413253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>b9614348f4</td>\n",
       "      <td>1 3434 3439 97 3544 93 3647 91 3749 90 3853 87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4696bb53e6</td>\n",
       "      <td>23 79 127 76 228 76 335 70 439 67 543 64 650 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fc250f574c</td>\n",
       "      <td>3435 1 3536 1 3637 2 3738 2 3839 3 3940 3 4041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>005b452274</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>02117a400e</td>\n",
       "      <td>4041 1 4142 4 4243 7 4344 10 4445 13 4546 15 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>16207869ba</td>\n",
       "      <td>3447 41 3528 71 3618 6584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>44d745a704</td>\n",
       "      <td>91 11 191 12 291 13 392 13 492 14 592 15 693 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>711c478c93</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>84e23dba29</td>\n",
       "      <td>304 1 405 2 506 3 607 4 708 5 809 6 910 7 1011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>e73532b450</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>8329ec9395</td>\n",
       "      <td>1 4039 4041 99 4142 98 4243 96 4344 95 4445 94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>7e7276d088</td>\n",
       "      <td>405 1 506 3 607 5 708 8 809 10 910 12 1011 14 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>0e63d9a8b4</td>\n",
       "      <td>7573 3 7674 3 7775 3 7876 3 7977 3 8078 3 8179...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>1d80de2ec9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>919bc0e2ba</td>\n",
       "      <td>1 5959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>5cada3d3f1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>0530b67189</td>\n",
       "      <td>3938 2 4032 9 4127 15 4221 22 4310 34 4398 47 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>fb44090bc7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>3c1ed5cc1f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3984</th>\n",
       "      <td>c7f54785b8</td>\n",
       "      <td>92 10 195 8 298 6 402 3 505 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>158765ed16</td>\n",
       "      <td>1 8585 8587 100 8690 98 8793 96 8896 94 8998 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>3ff3881428</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>e8d8b35601</td>\n",
       "      <td>79 11 177 13 276 14 375 15 474 16 573 18 672 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>a7cfa30d35</td>\n",
       "      <td>1 84 102 83 203 81 304 79 405 77 506 75 607 73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>917ef84e64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>633c7d5c80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>30082e87d9</td>\n",
       "      <td>59 43 163 40 267 37 371 34 472 34 573 34 674 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>f139be21a4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>ac931ace49</td>\n",
       "      <td>6464 1 6565 1 6666 1 6767 1 6868 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>cb36193e2f</td>\n",
       "      <td>74 28 173 30 270 34 367 38 467 39 567 40 668 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>9cbd5ddba4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>caa039b231</td>\n",
       "      <td>2398 7 2499 11 2600 16 2700 22 2801 26 2901 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1306fcee4c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>48d81e93d9</td>\n",
       "      <td>2828 1 2927 3 3026 5 3126 6 3225 8 3324 10 342...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>edf1e6ac00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           rle_mask\n",
       "0     575d24d81d                                                NaN\n",
       "1     a266a2a9df                                          5051 5151\n",
       "2     75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...\n",
       "3     34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...\n",
       "4     4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...\n",
       "5     782ae9b7e7  1 1815 1819 90 1920 81 2021 73 2122 64 2223 55...\n",
       "6     9842f69f8d                                                NaN\n",
       "7     aa94cfb806  1 28 102 28 203 29 304 30 405 32 506 33 607 34...\n",
       "8     50d3073821                                    1 2121 9293 909\n",
       "9     28f865caaa                                                NaN\n",
       "10    b5e1371b3b  75 27 175 28 275 29 374 31 474 32 574 33 674 3...\n",
       "11    57e394bc67                                                NaN\n",
       "12    b45ad3932e  49 11 149 11 249 11 351 10 452 9 553 9 654 8 7...\n",
       "13    ef51bbcde7                                                NaN\n",
       "14    d4d34af4f7                                          8788 1414\n",
       "15    302ea1ac81  6 96 108 95 210 94 311 94 413 93 515 92 615 93...\n",
       "16    40dcff68b3  3536 4 3637 20 3738 36 3839 53 3940 70 4041 86...\n",
       "17    7845115d01                                          7677 2525\n",
       "18    3da729cae9  1 54 102 54 203 53 304 53 405 53 506 53 607 53...\n",
       "19    d67e3a11d8                                                NaN\n",
       "20    b3396387a6  42 60 141 62 241 63 341 64 441 65 542 65 642 6...\n",
       "21    a5471f53d8            9797 1 9891 8 9986 14 10081 20 10176 26\n",
       "22    d27831fef2  93 9 195 8 296 8 397 8 498 8 599 8 701 7 802 7...\n",
       "23    7faea04242  1 6865 6869 92 6970 86 7071 81 7172 76 7273 71...\n",
       "24    9747413253                                                NaN\n",
       "25    b9614348f4  1 3434 3439 97 3544 93 3647 91 3749 90 3853 87...\n",
       "26    4696bb53e6  23 79 127 76 228 76 335 70 439 67 543 64 650 5...\n",
       "27    fc250f574c  3435 1 3536 1 3637 2 3738 2 3839 3 3940 3 4041...\n",
       "28    005b452274                                                NaN\n",
       "29    02117a400e  4041 1 4142 4 4243 7 4344 10 4445 13 4546 15 4...\n",
       "...          ...                                                ...\n",
       "3970  16207869ba                          3447 41 3528 71 3618 6584\n",
       "3971  44d745a704  91 11 191 12 291 13 392 13 492 14 592 15 693 1...\n",
       "3972  711c478c93                                                NaN\n",
       "3973  84e23dba29  304 1 405 2 506 3 607 4 708 5 809 6 910 7 1011...\n",
       "3974  e73532b450                                                NaN\n",
       "3975  8329ec9395  1 4039 4041 99 4142 98 4243 96 4344 95 4445 94...\n",
       "3976  7e7276d088  405 1 506 3 607 5 708 8 809 10 910 12 1011 14 ...\n",
       "3977  0e63d9a8b4  7573 3 7674 3 7775 3 7876 3 7977 3 8078 3 8179...\n",
       "3978  1d80de2ec9                                                NaN\n",
       "3979  919bc0e2ba                                             1 5959\n",
       "3980  5cada3d3f1                                                NaN\n",
       "3981  0530b67189  3938 2 4032 9 4127 15 4221 22 4310 34 4398 47 ...\n",
       "3982  fb44090bc7                                                NaN\n",
       "3983  3c1ed5cc1f                                                NaN\n",
       "3984  c7f54785b8                      92 10 195 8 298 6 402 3 505 1\n",
       "3985  158765ed16  1 8585 8587 100 8690 98 8793 96 8896 94 8998 9...\n",
       "3986  3ff3881428                                                NaN\n",
       "3987  e8d8b35601  79 11 177 13 276 14 375 15 474 16 573 18 672 1...\n",
       "3988  a7cfa30d35  1 84 102 83 203 81 304 79 405 77 506 75 607 73...\n",
       "3989  917ef84e64                                                NaN\n",
       "3990  633c7d5c80                                                NaN\n",
       "3991  30082e87d9  59 43 163 40 267 37 371 34 472 34 573 34 674 3...\n",
       "3992  f139be21a4                                                NaN\n",
       "3993  ac931ace49                 6464 1 6565 1 6666 1 6767 1 6868 1\n",
       "3994  cb36193e2f  74 28 173 30 270 34 367 38 467 39 567 40 668 4...\n",
       "3995  9cbd5ddba4                                                NaN\n",
       "3996  caa039b231  2398 7 2499 11 2600 16 2700 22 2801 26 2901 29...\n",
       "3997  1306fcee4c                                                NaN\n",
       "3998  48d81e93d9  2828 1 2927 3 3026 5 3126 6 3225 8 3324 10 342...\n",
       "3999  edf1e6ac00                                                NaN\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "54452e23-f4d3-919f-c734-80a35dc9ae08",
    "_execution_state": "idle",
    "_uuid": "5c15e1bd10b8e71c0b1d62bdb260882585a35579"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count          2438\n",
       "unique         2401\n",
       "top       1617 8585\n",
       "freq              4\n",
       "Name: rle_mask, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#descriptive statistics summary\n",
    "df_train['rle_mask'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6af460e5-1be2-6618-d624-2a4423bd501f",
    "_uuid": "bb3005e8025ea75b0b4d1ef624927e7e21833ea1"
   },
   "source": [
    "*'Very well... It seems that your minimum price is larger than zero. Excellent! You don't have one of those personal traits that would destroy my model! Do you have any picture that you can send me? I don't know... like, you in the beach... or maybe a selfie in the gym?'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "6bbea362-77b6-5385-f0a8-fb53afd088b7",
    "_execution_state": "idle",
    "_uuid": "2f78c77caa7290298138caf167672e62d3bc5a67"
   },
   "outputs": [],
   "source": [
    "\n",
    "#histogram\n",
    "#sns.distplot(df_train['rle_mask']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4e257f0-1dfd-0774-b346-f2a1b2a068cc",
    "_uuid": "f84e60c8b934615e53af10823558fe42753ac25d"
   },
   "source": [
    "*'Ah! I see you that you use seaborn makeup when you're going out... That's so elegant! I also see that you:*\n",
    "\n",
    "* *<b>Deviate from the normal distribution.</b>*\n",
    "* *<b>Have appreciable positive skewness.</b>*\n",
    "* *<b>Show peakedness.</b>*\n",
    "\n",
    "*This is getting interesting! 'SalePrice', could you give me your body measures?'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "36766737-f1a3-fe40-dbec-63c31be4d5e0",
    "_execution_state": "idle",
    "_uuid": "2cb253768dcd75b9a450ee264626ce69c808096a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "could not convert string to float: '2828 1 2927 3 3026 5 3126 6 3225 8 3324 10 3423 12 3522 14 3620 17 3719 19 3817 22 3916 24 4014 27 4113 29 4211 32 4310 34 4409 36 4508 38 4607 40 4707 41 4806 43 4906 44 5006 45 5106 46 5206 47 5305 49 5405 50 5504 52 5604 53 5703 55 5802 57 5901 59 5999 62 6098 64 6197 66 6295 69 6394 71 6493 73 6591 76 6690 78 6788 81 6886 84 6984 87 7082 90 7181 92 7279 95 7379 96 7478 98 7577 2625'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\GapPy36\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36m_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\GapPy36\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mnanskew\u001b[1;34m(values, axis, skipna)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_float_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '2828 1 2927 3 3026 5 3126 6 3225 8 3324 10 3423 12 3522 14 3620 17 3719 19 3817 22 3916 24 4014 27 4113 29 4211 32 4310 34 4409 36 4508 38 4607 40 4707 41 4806 43 4906 44 5006 45 5106 46 5206 47 5305 49 5405 50 5504 52 5604 53 5703 55 5802 57 5901 59 5999 62 6098 64 6197 66 6295 69 6394 71 6493 73 6591 76 6690 78 6788 81 6886 84 6984 87 7082 90 7181 92 7279 95 7379 96 7478 98 7577 2625'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2023d24870d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#skewness and kurtosis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Skewness: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rle_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Kurtosis: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rle_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkurt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\GapPy36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m   9611\u001b[0m                                       skipna=skipna)\n\u001b[0;32m   9612\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[1;32m-> 9613\u001b[1;33m                             numeric_only=numeric_only)\n\u001b[0m\u001b[0;32m   9614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9615\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\GapPy36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   3219\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[0;32m   3220\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3221\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\GapPy36\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36m_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;31m# object arrays that contain strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: could not convert string to float: '2828 1 2927 3 3026 5 3126 6 3225 8 3324 10 3423 12 3522 14 3620 17 3719 19 3817 22 3916 24 4014 27 4113 29 4211 32 4310 34 4409 36 4508 38 4607 40 4707 41 4806 43 4906 44 5006 45 5106 46 5206 47 5305 49 5405 50 5504 52 5604 53 5703 55 5802 57 5901 59 5999 62 6098 64 6197 66 6295 69 6394 71 6493 73 6591 76 6690 78 6788 81 6886 84 6984 87 7082 90 7181 92 7279 95 7379 96 7478 98 7577 2625'"
     ]
    }
   ],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df_train['rle_mask'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['rle_mask'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a3e43cc-5b75-0b49-1ad2-426d10d7fb42",
    "_uuid": "c984e32d4d10b1e6766c1b79ed8ade9a57529ffb"
   },
   "source": [
    "*'Amazing! If my love calculator is correct, our success probability is 97.834657%. I think we should meet again! Please, keep my number and give me a call if you're free next Friday. See you in a while, crocodile!'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c477edc1-b472-f55b-ba98-514159dbda2e",
    "_uuid": "e0368771327b15853b3e306f39eb93d209594e75"
   },
   "source": [
    "# 'SalePrice', her buddies and her interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b02d4be5-ef2c-bd5c-ff71-2c790b3637ce",
    "_uuid": "c6edbfb50a21f4c94052f86f6926cfdc35e7ca22"
   },
   "source": [
    "*It is military wisdom to choose the terrain where you will fight. As soon as 'SalePrice' walked away, we went to Facebook. Yes, now this is getting serious. Notice that this is not stalking. It's just an intense research of an individual, if you know what I mean.*\n",
    "\n",
    "*According to her profile, we have some common friends. Besides Chuck Norris, we both know 'GrLivArea' and 'TotalBsmtSF'. Moreover, we also have common interests such as 'OverallQual' and 'YearBuilt'. This looks promising!*\n",
    "\n",
    "*To take the most out of our research, we will start by looking carefully at the profiles of our common friends and later we will focus on our common interests.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73c1a953-aafd-96f6-c7bd-e2c79b3e5a0b",
    "_uuid": "08e034d70ed14ef0c4fb8cc1e9dd9e15f0d1c996"
   },
   "source": [
    "### Relationship with numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db040973-0adc-e126-e657-1d8934b5a5c8",
    "_execution_state": "idle",
    "_uuid": "91160363898f5caeee965a1aa81eb3abb7dcd760",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatter plot grlivarea/saleprice\n",
    "var = 'GrLivArea'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3dccb06-206a-20c0-6060-8d5b49f4df0b",
    "_uuid": "6775955416e9b2d4ac43e3fec5685c8577138455"
   },
   "source": [
    "*Hmmm... It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a <b>linear relationship.</b>*\n",
    "\n",
    "*And what about 'TotalBsmtSF'?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "353def35-0f26-998d-b9a4-7356f95e80ad",
    "_execution_state": "idle",
    "_uuid": "3ac3db51311338fcdc16014a7c506cf3d5315af7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatter plot totalbsmtsf/saleprice\n",
    "var = 'TotalBsmtSF'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7ea69b90-de6d-c104-ff92-a49943993930",
    "_uuid": "4496c7d3635636e8e19296f3b5b52fdba16f7cc5"
   },
   "source": [
    "*'TotalBsmtSF' is also a great friend of 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a <b>strong linear (exponential?)</b> reaction, everything changes. Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b31bc890-46bf-618c-e668-17879763ad23",
    "_uuid": "5550d4df3c0ad48d8b2b9b0905f17daa3fc8d244"
   },
   "source": [
    "### Relationship with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "26d0fddc-cb09-af7d-9f03-a07233fa6c9e",
    "_execution_state": "idle",
    "_uuid": "e2b7aaccc3486a74a09996289a833ffb6acd0764",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#box plot overallqual/saleprice\n",
    "var = 'OverallQual'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49c33fb5-fce0-e852-7f31-2d0da20f7b16",
    "_uuid": "463ae72c67dfa50524f9b2c89427dbcd4b7bd856"
   },
   "source": [
    "*Like all the pretty girls, 'SalePrice' enjoys 'OverallQual'. Note to self: consider whether McDonald's is suitable for the first date.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "590da500-3e48-7059-4f0b-1ef1801dd1db",
    "_execution_state": "idle",
    "_uuid": "a78c64aeed1e48aa453ae4b7deef172cb2060b39",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var = 'YearBuilt'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ad19b4d2-cb40-2e10-c6ee-913a0d5a6618",
    "_uuid": "22f458a1252d0eed8bbd6f0d0b56508033e228bd"
   },
   "source": [
    "*Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.*\n",
    "\n",
    "<b>Note</b>: we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2f4940-ee0c-edef-769d-65fead4f06f3",
    "_uuid": "1dc93d788b3cd2446afa41e7c06d05c6bbadb480"
   },
   "source": [
    "### In summary\n",
    "\n",
    "Stories aside, we can conclude that:\n",
    "\n",
    "* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n",
    "* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n",
    "\n",
    "We just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\n",
    "\n",
    "That said, let's separate the wheat from the chaff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b75e5f62-307b-f5f4-79ac-38815a7a6da4",
    "_uuid": "aa4ed71a6d1003513b4e31adacd0e7a491671f0f"
   },
   "source": [
    "# 3. Keep calm and work smart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ea2f896-48a6-db39-0684-6a029d8fda60",
    "_uuid": "d67e5bf6f2c4b6acb617c4cdcee0dc5c3f79d9b8"
   },
   "source": [
    "Until now we just followed our intuition and analysed the variables we thought were important. In spite of our efforts to give an objective character to our analysis, we must say that our starting point was subjective. \n",
    "\n",
    "As an engineer, I don't feel comfortable with this approach. All my education was about developing a disciplined mind, able to withstand the winds of subjectivity. There's a reason for that. Try to be subjective in structural engineering and you will see physics making things fall down. It can hurt.\n",
    "\n",
    "So, let's overcome inertia and do a more objective analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0eb27879-4f54-6fd7-e07a-156eae1aef7f",
    "_uuid": "4816779faf59b3eebf954c36bb3c85ff7dfc87a9"
   },
   "source": [
    "### The 'plasma soup'\n",
    "\n",
    "'In the very beginning there was nothing except for a plasma soup. What is known of these brief moments in time, at the start of our study of cosmology, is largely conjectural. However, science has devised some sketch of what probably happened, based on what is known about the universe today.' (source: http://umich.edu/~gs265/bigbang.htm) \n",
    "\n",
    "To explore the universe, we will start with some practical recipes to make sense of our 'plasma soup':\n",
    "* Correlation matrix (heatmap style).\n",
    "* 'SalePrice' correlation matrix (zoomed heatmap style).\n",
    "* Scatter plots between the most correlated variables (move like Jagger style)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06f8d02c-d779-f8fd-7f48-ba3c5166eda8",
    "_uuid": "bf469f1030a8768f73a18e5ad59db43c4241c603"
   },
   "source": [
    "#### Correlation matrix (heatmap style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4eb7a6ef-adf5-6abf-947d-c95afdc477b8",
    "_execution_state": "idle",
    "_uuid": "5dfee22210f5a126ea34ca6475bb4f365d41317b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "977fb7af-70e1-bfb8-e381-3176ec0321b0",
    "_uuid": "b6eb357b201b4d2a9e81f4373cadb9aeccc2482c"
   },
   "source": [
    "In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships. (Thank you @seaborn!)\n",
    "\n",
    "At first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'Garage*X*' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n",
    "\n",
    "Another thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9b557956-df91-bab3-0e8b-d8ffc054470f",
    "_uuid": "6ef7d7b7747807431aa9827046c76bfa3d34ffe1"
   },
   "source": [
    "#### 'SalePrice' correlation matrix (zoomed heatmap style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc33db9e-9ee3-6cfe-7643-a2aff5a9234d",
    "_execution_state": "idle",
    "_uuid": "a6ee47c540ce9f3f1d2af6efe0b030e76e3a3f7f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f5c23b8a-aad9-809f-0fdf-f758f926f5c9",
    "_uuid": "c10a0a0bd55e55822726c78a59ab2bb6d9763f68"
   },
   "source": [
    "According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n",
    "\n",
    "* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n",
    "* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n",
    "* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n",
    "* 'FullBath'?? Really? \n",
    "* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\n",
    "* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.\n",
    "\n",
    "Let's proceed to the scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3bdb34db-cf47-d8c8-b857-7031828808ed",
    "_uuid": "28e0b3f8ecee68390a40bd653d7370c20615ad5d"
   },
   "source": [
    "#### Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f49b1a3a-2edf-d582-e4ed-3b6188675fbc",
    "_uuid": "cbebb4763aa2a977078a1fba424073b3ea1ecd61"
   },
   "source": [
    "Get ready for what you're about to see. I must confess that the first time I saw these scatter plots I was totally blown away! So much information in so short space... It's just amazing. Once more, thank you @seaborn! You make me 'move like Jagger'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a8db5de-d3f9-9a28-f220-bb05d51c53d0",
    "_execution_state": "idle",
    "_uuid": "cdafd230216fd04cc4ecf635967925da0bce9195",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(df_train[cols], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a61d0a23-e5e4-3190-41a7-03a5110dc967",
    "_uuid": "7dd622e414e1fda79d2d1f163b1d30d6c2de15eb"
   },
   "source": [
    "Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n",
    "\n",
    "One of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n",
    "\n",
    "The plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n",
    "\n",
    "Ok, enough of Rorschach test for now. Let's move forward to what's missing: missing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9ce00498-d5e6-9e35-debc-8d507002d461",
    "_uuid": "726efbb348d1022cabb171f622d7b4e01fe8c778"
   },
   "source": [
    "# 4. Missing data\n",
    "\n",
    "Important questions when thinking about missing data:\n",
    "\n",
    "* How prevalent is the missing data?\n",
    "* Is missing data random or does it have a pattern?\n",
    "\n",
    "The answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ca2f89e7-1c16-c3ae-6fe0-ab4eaf7e52a1",
    "_execution_state": "idle",
    "_uuid": "664e03dc1434fa2c4eb730ea36ab60e37f13cd3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#missing data\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42bf6c61-9836-5c25-ff2f-1219e96f60a2",
    "_uuid": "c7607aaf8378eac11a8a8395dcd4a2346c22f5ea"
   },
   "source": [
    "Let's analyse this to understand how to handle the missing data.\n",
    "\n",
    "We'll consider that when more than 15% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases. According to this, there is a set of variables (e.g. 'PoolQC', 'MiscFeature', 'Alley', etc.) that we should delete. The point is: will we miss this data? I don't think so. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers, so we'll be happy to delete them.\n",
    "\n",
    "In what concerns the remaining cases, we can see that 'Garage*X*' variables have the same number of missing data. I bet missing data refers to the same set of observations (although I will not check it; it's just 5% and we should not spend 20$ in 5$ problems). Since the most important information regarding garages is expressed by 'GarageCars' and considering that we are just talking about 5% of missing data, I'll delete the mentioned 'Garage*X*' variables. The same logic applies to 'Bsmt*X*' variables.\n",
    "\n",
    "Regarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.\n",
    "\n",
    "Finally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation and keep the variable.\n",
    "\n",
    "In summary, to handle missing data, we'll delete all the variables with missing data, except the variable 'Electrical'. In 'Electrical' we'll just delete the observation with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f43f72c5-5457-1f47-e8ef-502db4355086",
    "_execution_state": "idle",
    "_uuid": "726617e295ee6bfe26ccf277323cc68ef52dc61b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dealing with missing data\n",
    "df_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\n",
    "df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\n",
    "df_train.isnull().sum().max() #just checking that there's no missing data missing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf77b1ad-c87b-0be1-7adc-02b0186e2c37",
    "_uuid": "402bab0012e6f60f3fd045788d9c60e02e43e46d"
   },
   "source": [
    "# Out liars!\n",
    "\n",
    "Outliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n",
    "\n",
    "Outliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06852f05-22e9-6ea3-ae37-c08f83ed401f",
    "_uuid": "39ccb28e64249ea88dc9d04c5c1484734a8e9dcc"
   },
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1553bf55-edb3-7c65-f505-227d34d018fb",
    "_uuid": "1c4f41a45aabef83aab703c4e9c0101db1d3c7f7"
   },
   "source": [
    "The primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49a133fb-b713-45bd-ca42-c1ca0eb4d3f6",
    "_execution_state": "idle",
    "_uuid": "09b3bc296d01936b3b6df7f3ea670499e926720e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardizing data\n",
    "saleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\n",
    "low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\n",
    "high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n",
    "print('outer range (low) of the distribution:')\n",
    "print(low_range)\n",
    "print('\\nouter range (high) of the distribution:')\n",
    "print(high_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ae8ecbd-3236-7e32-fa2c-9c12a6ba53d3",
    "_uuid": "68d1465ef6f6da48d318868646741072eec73f0b"
   },
   "source": [
    "How 'SalePrice' looks with her new clothes:\n",
    "\n",
    "* Low range values are similar and not too far from 0.\n",
    "* High range values are far from 0 and the 7.something values are really out of range.\n",
    "\n",
    "For now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae1b038a-9016-1412-bfb8-e2a27022a02f",
    "_uuid": "6b9511771766dbdb72abe83ec3f479c40c08ce6d"
   },
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed741196-bc75-a4b5-dda9-417b6d8aa52b",
    "_uuid": "a4137453b99017b08ed6d433829e04160b6613d2"
   },
   "source": [
    "We already know the following scatter plots by heart. However, when we look to things from a new perspective, there's always something to discover. As Alan Kay said, 'a change in perspective is worth 80 IQ points'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a03b5ee8-0701-10f0-2d4c-06fcaf1fada5",
    "_execution_state": "idle",
    "_uuid": "4d3e3941ca62ce141f96d0fd2f3276cd80e03ed3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bivariate analysis saleprice/grlivarea\n",
    "var = 'GrLivArea'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "48c2ad5c-b861-6b33-a292-9d6b7b58b1cc",
    "_uuid": "8ef88aabbea5235f920f0a12eba9cf9f640b5f6d"
   },
   "source": [
    "What has been revealed:\n",
    "\n",
    "* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n",
    "* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "63a6517c-431f-c3fe-30cf-61034e54a5cb",
    "_execution_state": "idle",
    "_uuid": "ff34e1e620a89d16ecf35508e40e23b1d7a4771c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deleting points\n",
    "df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
    "df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\n",
    "df_train = df_train.drop(df_train[df_train['Id'] == 524].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d4bf215-19d5-877e-1958-f5713009a94a",
    "_execution_state": "idle",
    "_uuid": "a48108966041d652f9fee571bf9ff08a7c73f6ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bivariate analysis saleprice/grlivarea\n",
    "var = 'TotalBsmtSF'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ff4b9944-30d2-834a-de2f-6ee1b54aefc6",
    "_uuid": "83b610acd7caee0f0ac5e82856bbeb17fb54ceee"
   },
   "source": [
    "We can feel tempted to eliminate some observations (e.g. TotalBsmtSF > 3000) but I suppose it's not worth it. We can live with that, so we'll not do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0b5def9-2709-a643-0ade-2689b6b36dae",
    "_uuid": "283f857f9ac2e1510cd238f59d937132a5492da6"
   },
   "source": [
    "# 5. Getting hard core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b997f3f-c28f-6029-64c9-2d2bfc01077c",
    "_uuid": "f8e75e15ed7cba60c64c44b4fa3ec6737414e218"
   },
   "source": [
    "In Ayn Rand's novel, 'Atlas Shrugged', there is an often-repeated question: who is John Galt? A big part of the book is about the quest to discover the answer to this question.\n",
    "\n",
    "I feel Randian now. Who is 'SalePrice'?\n",
    "\n",
    "The answer to this question lies in testing for the assumptions underlying the statistical bases for multivariate analysis. We already did some data cleaning and discovered a lot about 'SalePrice'. Now it's time to go deep and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.\n",
    "\n",
    "According to [Hair et al. (2013)](https://amzn.to/2uC3j9p), four assumptions should be tested:\n",
    "\n",
    "* <b>Normality</b> - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely  on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n",
    "\n",
    "* <b>Homoscedasticity</b> - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' [(Hair et al., 2013)](https://amzn.to/2uC3j9p). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n",
    "\n",
    "* <b>Linearity</b>- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n",
    "\n",
    "* <b>Absence of correlated errors</b> - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n",
    "\n",
    "What do you think Elvis would say about this long explanation? 'A little less conversation, a little more action please'? Probably... By the way, do you know what was Elvis's last great hit?\n",
    "\n",
    "(...)\n",
    "\n",
    "The bathroom floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f1a301d-0c06-548f-540d-124975931e70",
    "_uuid": "2497351eca10510469b27ca8518c5c1510932dc1"
   },
   "source": [
    "### In the search for normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0d4f4ca7-3d4f-99e2-2aa7-1a6a6fc9e62e",
    "_uuid": "b408f24ef2b8fe26e44243fd7b0931a169789cc1"
   },
   "source": [
    "The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n",
    "\n",
    "* <b>Histogram</b> - Kurtosis and skewness.\n",
    "* <b>Normal probability plot</b> - Data distribution should closely follow the diagonal that represents the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "99ca9885-9335-411e-9b35-21ff69d312e6",
    "_execution_state": "idle",
    "_uuid": "ad844362305f49c0e199ffe276dd119563d46a72",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ad84e765-4e47-2f00-dc09-474bf24c1dee",
    "_uuid": "eb8588ce1dd5cfcdd62a943268fa7d252ca7c447"
   },
   "source": [
    "Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n",
    "\n",
    "But everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. When I discovered this, I felt like an Hogwarts' student discovering a new cool spell.\n",
    "\n",
    "*Avada kedavra!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cced5b14-c39d-c847-6dc9-93af3f4b6e6d",
    "_execution_state": "idle",
    "_uuid": "f578838e98e9996b09abbec058200cb18aa38869",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#applying log transformation\n",
    "df_train['SalePrice'] = np.log(df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e17fba2-3ff2-d6f1-841d-bc2a9746bcc2",
    "_execution_state": "idle",
    "_uuid": "de8366b3ad71c7cb398644766a412bee1d05642f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74131161-b766-8b08-5ae3-0b70688a198a",
    "_uuid": "19b172dfedfef5330c53cc34837d47641a3cf135"
   },
   "source": [
    "Done! Let's check what's going on with 'GrLivArea'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bda5d77e-07ea-1d16-644c-a850487cc35d",
    "_execution_state": "idle",
    "_uuid": "aa29a3612ee861c42e5d6022ac3a8ad75266b0ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['GrLivArea'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['GrLivArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a8fa54e-46e2-5c09-0a43-0b200ee59874",
    "_uuid": "f452585a30fe3006d4dc993941e826c793044a4b"
   },
   "source": [
    "Tastes like skewness... *Avada kedavra!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0fe3abe-cc32-6f2e-0e79-1afd9b49c7ad",
    "_execution_state": "idle",
    "_uuid": "aa428f580e39e498dc06747423955fa714d97952",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data transformation\n",
    "df_train['GrLivArea'] = np.log(df_train['GrLivArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0171b317-885d-7092-ade9-d2332b8ea796",
    "_execution_state": "idle",
    "_uuid": "bb004d70d1f260289f864c2ecb5c5dceedc2d6fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train['GrLivArea'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['GrLivArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1686254a-4391-a744-b8ae-653364e216d4",
    "_uuid": "bdd45521276779b330af14f279bf578a9457dac0"
   },
   "source": [
    "Next, please..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4cec452c-ccfd-e65e-2e8b-6c601d3664cd",
    "_execution_state": "idle",
    "_uuid": "b9b1528dcb2a5d1a85c469c69f9df08149fe5ff0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['TotalBsmtSF'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['TotalBsmtSF'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8212aea1-70b8-5cdb-1d54-ebad40f3d73e",
    "_uuid": "317120e4f6469692db40bfb8c9c450cf2dc91938"
   },
   "source": [
    "Ok, now we are dealing with the big boss. What do we have here?\n",
    "\n",
    "* Something that, in general, presents skewness.\n",
    "* A significant number of observations with value zero (houses without basement).\n",
    "* A big problem because the value zero doesn't allow us to do log transformations.\n",
    "\n",
    "To apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n",
    "\n",
    "I'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a025f96a-c537-92d5-9187-355cb4f0a520",
    "_execution_state": "idle",
    "_uuid": "5cb8e327f4fd2e4714ec522bdd64c6ed81781652",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create column for new variable (one is enough because it's a binary categorical feature)\n",
    "#if area>0 it gets 1, for area==0 it gets 0\n",
    "df_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\n",
    "df_train['HasBsmt'] = 0 \n",
    "df_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9933a45e-93f0-b43f-e7ad-2cb1986570d8",
    "_execution_state": "idle",
    "_uuid": "a7b15dd8a8163ab753ca2aa9e16ed38616bd2d03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform data\n",
    "df_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a2dc1c6-162d-4b42-8687-046b1ca47511",
    "_execution_state": "idle",
    "_uuid": "f8b1b53558610f9623617fd71a6440439a47e48e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78c3c8e5-4ddd-e9ba-6ea8-a23b9d2b21a5",
    "_uuid": "f6f0988bbc48a298177ecc7ec70f2706c72fc1db"
   },
   "source": [
    "### In the search for writing 'homoscedasticity' right at the first attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14a0ff01-0d6d-4f3e-c959-0b1039b06439",
    "_uuid": "469987c7961cd885cc4233626426777e2b675466"
   },
   "source": [
    "The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n",
    "\n",
    "Starting by 'SalePrice' and 'GrLivArea'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5f13ea7-70af-6a3d-bff1-84a696972e59",
    "_execution_state": "busy",
    "_uuid": "d6db45dbc22abfcdaf49be23f70e6b8f4ca4c13b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "plt.scatter(df_train['GrLivArea'], df_train['SalePrice']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522a6dc2-291a-807b-8fe9-3aa335dbd79d",
    "_uuid": "aed0d40afac41960d6409e9ea4ff9cd3d484ae86"
   },
   "source": [
    "Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n",
    "\n",
    "Now let's check 'SalePrice' with 'TotalBsmtSF'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5a6cecd-4f33-5afe-2c95-9459da03091f",
    "_execution_state": "idle",
    "_uuid": "45cdf80ae7cf8bf1d5d62ff09f6e9b106047cafb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "plt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "38a2af07-61d9-bc63-e78d-f88fd8bd03dc",
    "_uuid": "612394fe6ca03c21a96019cb72bd64966d05066e"
   },
   "source": [
    "We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56d7b15d-aaa1-0aaf-875b-8d914f4983dd",
    "_uuid": "9957d2bc81d7c1f694314495b923469be3428bbf"
   },
   "source": [
    "# Last but not the least, dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b782af0e-c785-0fbb-de2d-1301b53ba729",
    "_uuid": "4603fc4c5a9b54139d7d0b598b5afbad66ad4b2e"
   },
   "source": [
    "Easy mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06dfd6a2-f613-01e8-b7d0-2672dfd41db8",
    "_execution_state": "idle",
    "_uuid": "0f697a3a1949c3d9b86e9c38b11628d3f8bdc138",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert categorical variable into dummy\n",
    "df_train = pd.get_dummies(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92018f8f-4782-5a0c-9fee-f657b6331ffd",
    "_uuid": "38631001da379277b3ae8cbc78504546a23a810d"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95c93455-85f4-e0a4-3d3a-8365bcf750e0",
    "_uuid": "c05bfefad3b6084b9a7314ab87e5d9ae54af5ba1"
   },
   "source": [
    "That's it! We reached the end of our exercise.\n",
    "\n",
    "Throughout this kernel we put in practice many of the strategies proposed by [Hair et al. (2013)](https://amzn.to/2uC3j9p). We philosophied about the variables, we analysed 'SalePrice' alone and with the most correlated variables, we dealt with missing data and outliers, we tested some of the fundamental statistical assumptions and we even transformed categorial variables into dummy variables. That's a lot of work that Python helped us make easier.\n",
    "\n",
    "But the quest is not over. Remember that our story stopped in the Facebook research. Now it's time to give a call to 'SalePrice' and invite her to dinner. Try to predict her behaviour. Do you think she's a girl that enjoys regularized linear regression approaches? Or do you think she prefers ensemble methods? Or maybe something else?\n",
    "\n",
    "It's up to you to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f29b0f7c-5f02-e28b-fc3b-c161fde831aa",
    "_uuid": "69dd52ff366c69a33a1aabe58e9f6718ba7a73b3"
   },
   "source": [
    "# <b>References</b>\n",
    "* [My blog](http://pmarcelino.com)\n",
    "* [My other kernels](https://www.kaggle.com/pmarcelino/data-analysis-and-feature-extraction-with-python)\n",
    "* [Hair et al., 2013, Multivariate Data Analysis, 7th Edition](https://amzn.to/2JuDmvo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2f6375aa-89c3-d6d1-6acb-288eab8b24f5",
    "_uuid": "06c7675e062996ed0f706776afacc2138bf5ed63"
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "Thanks to [João Rico](https://www.linkedin.com/in/joaomiguelrico/) for reading drafts of this."
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
